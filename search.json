[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "HW0",
    "section": "",
    "text": "Hello! This is my HW0.\n\nLoad Palmer Penguins data set\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins = penguins[penguins[\"Sex\"] != \".\"]\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]\n\n\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\nHave a general idea of the proportion of each species and the island they inhabit.\n\nimport matplotlib.pyplot as plt\n\nspecies = penguins.Species.value_counts()\nspecies.plot(kind='pie',autopct=\"%.2f%%\")\nplt.title('Percentage Distribution of Penguin Species')\n\nText(0.5, 1.0, 'Percentage Distribution of Penguin Species')\n\n\n\n\n\n\n\n\n\nBased on the pie chart, the Palmer Penguins data set predominantly consists of Adelie penguins, accounting for 43.84%, followed by Gentoo penguins at 35.74%, and finally, Chinstrap penguins at 20.42%.\n\nisland= penguins.Island.value_counts()\nisland.plot(kind='pie',autopct=\"%.2f%%\")\nplt.title('Percentage Distribution of Penguin on Each Island')\n\nText(0.5, 1.0, 'Percentage Distribution of Penguin on Each Island')\n\n\n\n\n\n\n\n\n\nBased on the pie chart, the Palmer Penguins data set indicates that the majority of penguins inhabit Biscoe Island, comprising 48.95%, followed by Dream Island at 36.94%, and finally, Torgersen Island at 14.11%.\n\nimport seaborn as sns\n\n# Set style for seaborn\nsns.set(style=\"whitegrid\")\n\n# Create a figure with three subplots in the same line\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n\n# Plot 1: Island = Biscoe\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Biscoe'], ax=axes[0])\naxes[0].set_title(\"Biscoe Island\")\n\n# Plot 2: Island = Dream\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Dream'], ax=axes[1])\naxes[1].set_title(\"Dream Island\")\n\n# Plot 3: Island = Torgersen\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Torgersen'], ax=axes[2])\naxes[2].set_title(\"Torgersen Island\")\n\n# Set common y-label\naxes[0].set_ylabel(\"Count\")\n\nplt.suptitle(\"Distribution of Penguin Species on Different Islands\", y=1.05)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nBased on the bar plot illustrating the distribution of each species on each island, Gentoo penguins are the predominant species on Biscoe Island, Chinstrap penguins dominate Dream Island, and Adelie penguins are the major species on Torgersen Island. However, it’s worth noting that Adelie penguins inhabit all three islands, with Dream Island having the highest count of Adelie, even though Torgersen Island exclusively has Adelie penguins.\n\n\nDive deep into the Culmen Length and Culmen Depth relationships\n\nimport plotly\n\nfrom plotly import express as px\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 size = \"Body Mass (g)\",\n                 size_max = 8,\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5\n                )\n\n#reduce whitespace\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\n# show the plot\nfig.show()\n\n\n\n\nWe now observe a general pattern where Adelie penguins tend to exhibit shorter culmen length but longer culmen depth compared to the other two species. Chinstrap penguins typically fall in the middle for both culmen length and culmen depth when compared to the other two species. On the other hand, Gentoo penguins tend to have greater culmen length and shorter culmen depth than the other two species. In summary, across all species, culmen length tends to be greater than culmen depth.\n\n\nNow, let’s explore the factors that may contribute to the variation in culmen depth and culmen length among different species.\n\n# convert categorical values into numerical values\npenguins_encoded = pd.get_dummies(penguins, columns=[\"Species\", \"Island\", \"Sex\"], drop_first=True)\n\n\npenguins_encoded['Species_Adelie'] = ~penguins_encoded['Species_Chinstrap'] & ~penguins_encoded['Species_Gentoo']\n\n\npenguins_encoded['Island_Biscoe'] =  ~penguins_encoded['Island_Dream'] & ~penguins_encoded['Island_Torgersen']\n\n\npenguins_encoded\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSpecies_Chinstrap\nSpecies_Gentoo\nIsland_Dream\nIsland_Torgersen\nSex_MALE\nSpecies_Adelie\nIsland_Biscoe\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n36.7\n19.3\n193.0\n3450.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n5\n39.3\n20.6\n190.0\n3650.0\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\n47.2\n13.7\n214.0\n4925.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n340\n46.8\n14.3\n215.0\n4850.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n341\n50.4\n15.7\n222.0\n5750.0\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n342\n45.2\n14.8\n212.0\n5200.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n343\n49.9\n16.1\n213.0\n5400.0\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n333 rows × 11 columns\n\n\n\n\n# Compute the correlation matrix\ncorrelation_matrix = penguins_encoded.corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n\n# Set plot title\nplt.title(\"Correlation Heatmap\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nBased on the plot, it appears that flipper length has a significant effect on culmen length, with a correlation of 0.65. Following this, body mass shows a correlation of 0.59. This aligns with the earlier visualization, indicating that Adelie tends to have a shorter length with a negative coefficient of -0.84. In contrast, Gentoo tends to have a longer culmen length with a positive coefficient of 0.49. The same trend applies to Chinstrap, with a positive coefficient similar to Gentoo.\nConcerning culmen depth, Adelie tends to have a longer culmen depth with the greatest positive coefficient of 0.53. Dream Island positively affects culmen depth with a coefficient of 0.46, which makes sense as Adelie and Chinstrap occupy the island. Gentoo, on the other hand, exhibits the greatest negative effect on culmen depth, consistent with the fact that Gentoo inhabits Biscoe Island."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Movie or TV Show Recommendation",
    "section": "",
    "text": "Unleashing the Power of Scrapy: Scraping and Recommending Movies and TV Shows from TMDB\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/HW2/index.html#intro",
    "href": "posts/HW2/index.html#intro",
    "title": "Movie or TV Show Recommendation",
    "section": "Intro",
    "text": "Intro\nWelcome to the world of web scraping and data-driven recommendations! In today’s digital era, where information is abundant and readily accessible, leveraging the power of data extraction tools like Scrapy opens up a realm of possibilities. In this tutorial, we’ll embark on a journey to scrape data from TMDB (The Movie Database), a treasure trove of information about movies and TV shows. But we’re not stopping there; we’ll dive deeper into the web of interconnected data, exploring the crew members’ profiles and unraveling the threads of their acting history.\nUsing Scrapy, a powerful and versatile web crawling framework in Python, we’ll navigate through TMDB’s vast database with ease. Our mission? To gather detailed insights into specific movie and TV show pages, extract information about the cast and crew, and then traverse the web of actor profiles to unearth their acting repertoire. By analyzing the overlaps in their performances, we’ll construct a recommendation engine that suggests similar movies or TV shows based on shared talent.\nWhether you’re a data enthusiast, a budding web developer, or simply curious about the magic behind personalized recommendations, this tutorial will equip you with the tools and knowledge to embark on your scraping and recommendation journey. So, let’s roll up our sleeves, fire up our code editors, and dive headfirst into the fascinating world of Scrapy and TMDB scraping!"
  },
  {
    "objectID": "posts/HW2/index.html#table-of-contents",
    "href": "posts/HW2/index.html#table-of-contents",
    "title": "Movie or TV Show Recommendation",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1. Web Scraper\n1.1 Setup\n    1.1.1 Setting up a Scrapy Project\n    1.1.2 Test Scrapy for TMDB Scraping\n1.2 Implementation\n\n\n2. Visualization and Recommendation"
  },
  {
    "objectID": "posts/HW2/index.html#web-scraper-1",
    "href": "posts/HW2/index.html#web-scraper-1",
    "title": "Movie or TV Show Recommendation",
    "section": "1. Web Scraper",
    "text": "1. Web Scraper\n\n1.1 Setup\n\n1.1.1 Setting up a Scrapy Project\nIn this section, we’ll walk through the process of setting up a Scrapy project to scrape data from TMDB (The Movie Database). By following these steps, you’ll be able to create a structured project environment ready for web scraping.\n\nStep 1: Activate Your Conda Environment\nOpen a terminal or command prompt and activate your Conda environment where you want to set up your Scrapy project. Type the following command:\nconda activate PIC16B-24W\nThis command activates the specified Conda environment named “PIC16B-24W”. Ensure that you have created and configured this environment beforehand.(You can choose whatever environment you have)\n\n\nStep 2: Create a Scrapy Project\nNow, we’ll create a Scrapy project named “TMDB_scraper”. In your terminal, type the following command:\nscrapy startproject TMDB_scraper\nThis command initializes a new Scrapy project named “TMDB_scraper”. Scrapy will generate several files and directories within this project. When you create a Scrapy project using the scrapy startproject command with the name “TMDB_scraper”, the generated project directory (TMDB_scraper) typically contains the following files and directories:\n\nscrapy.cfg: This file is the configuration file for your Scrapy project. It contains settings for Scrapy deployment and other global configurations.\nTMDB_scraper/ (directory inside TMDB_scraper/): This is the Python package directory containing your project’s code.\n\nitems.py: This file defines the data structure (items) that your spider will extract during scraping. You define the fields you want to extract from the website here.\nmiddlewares.py: This file contains the middlewares used by Scrapy, such as user-agent rotation, proxy usage, etc.\npipelines.py: This file contains the pipelines used by Scrapy for processing scraped items. Pipelines are used to perform tasks like cleaning, validation, and storing data.\nsettings.py: This file contains Scrapy project settings, such as user-agent, download delay, etc. You can customize these settings according to your requirements.\nspiders/ (directory): This directory contains your spider scripts.\n\n__init__.py: This file makes the spiders directory a Python package.\n\n\n\nThese are the essential files and directories that are created when you initialize a Scrapy project. Depending on your specific requirements and configurations, your project might have additional files or directories.\n\n\nStep 3: Navigate to Your Project Directory\nNavigate to the newly created project directory “TMDB_scraper” using the cd command:\ncd TMDB_scraper\nThis command changes your current directory to the “TMDB_scraper” directory, where your Scrapy project files are located. From here, you’ll be able to configure and customize your scraping scripts as needed.\nCongratulations! You’ve successfully set up a Scrapy project named “TMDB_scraper” ready for web scraping.\n\n\n\n1.1.2 Test Scrapy for TMDB Scraping\nWe’ll start by configuring settings in the settings.py file to prevent excessive data downloads and avoid being blocked by the website. Then, we’ll explore using the Scrapy shell for interactive testing and debugging.\n\nStep 1: Modify settings.py to Limit Page Count\nOpen the settings.py file located in your Scrapy project directory (TMDB_scraper) using a text editor. Add the following line to limit the number of pages to be scraped:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading too much data during testing. Remember to remove this line later for full-scale scraping.\n\n\nStep 2: Configure User-Agent to Mimic a Web Browser\nTo avoid getting blocked by TMDB, we’ll mimic a web browser’s behavior by modifying the USER_AGENT in the settings.py file. Replace the existing USER_AGENT line with the following:\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'\nThis user-agent string resembles a typical web browser’s user-agent, which makes it less likely for TMDB to block our requests.\nOr you can use other methods:\nhttps://doc.scrapy.org/en/latest/topics/practices.html#avoiding-getting-banned\nhttps://scrapeops.io/python-scrapy-playbook/scrapy-403-unhandled-forbidden-error/\nhttps://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/\nhttps://scrapingrobot.com/blog/most-common-user-agents/\n\n\nStep 3: Test Scraping with Scrapy Shell\nNow, let’s test our scraping capability using Scrapy shell. Open a terminal or command prompt and navigate to your Scrapy project directory (TMDB_scraper). Then, run the following command:\nscrapy shell https://www.themoviedb.org/movie/157336-interstellar/cast\nReplace the URL with the desired TMDB page to scrape. Here I used Interstellar. This command launches the Scrapy shell with the specified user-agent and the provided URL.\nStay tuned for the next part of this tutorial series where we’ll explore writing spider scripts and extracting data from TMDB using Scrapy.\n\n\n\n\n1.2 Website Navigation\nBefore writing the scraper method, we need to have a basic idea of what the website looks like and what information we need to scrap from the website.\nStart by visiting the website you want to scrape. In our case, it’s TMDB (The Movie Database). Browse through different pages, such as movie pages and the Full Cast & Crew page, to get familiar with the layout and structure.\n\nExplore the Website\nStart by visiting the TMDB website and navigating to the movie page of interest. For our example, let’s consider the movie “Interstellar”. You can find its page at:\nInterstellar Movie Page\nThe url is: https://www.themoviedb.org/movie/157336-interstellar\nThis is the url that we want to start with in the __init__, where we define the start url, that can be modified with whatever movie or tv show names after the slash movie.\n\n\nIdentify Relevant Pages\nOnce on the movie page, explore the different sections and links available. In our case, we’re interested in extracting information about the movie’s cast. We’ll navigate to the Full Cast & Crew page by appending “/cast” to the movie page URL:\nInterstellar Cast Page\nThe url is: https://www.themoviedb.org/movie/157336-interstellar/cast\nNote that there is a ‘cast’ was added at the end of the initial url and that is what we are going to do in the parse method.\nFor the other 2 method parse_full_credits and parse_actor_page we need to dig deeper to the page construction. Here I am just going to introduce the overall structer of the TmdbSpider class.\n\n\nPlan Scraping Strategy\nNow that we’ve identified the relevant pages, we can plan our scraping strategy. We’ll create a Scrapy spider with the following methods:\n\nparse: Navigates to the Full Cast & Crew page and calls parse_full_credits.\nparse_full_credits: Extracts information about the cast members and navigates to each actor’s personal page.\nparse_actor_page: Extracts the actor’s acting history from their personal page.\n\nUnderstanding the structure of the website you’re scraping is essential for developing an effective scraping method. By exploring different pages, identifying relevant information, and planning your scraping strategy, you’ll be better prepared to write the scraper method and extract the desired data.\nOnce we’ve outlined the blueprint of our class, the next step is bringing our plan to life. In web scraping, this primarily involves inspecting and extracting data. Once we understand the data structure, our task is to pinpoint its location within the HTML and then select the most appropriate method for extraction.\n\n\nInspect Elements\nMost modern web browsers offer developer tools that allow you to inspect the HTML and CSS of web pages. Right-click on any element of interest (e.g., movie title, actor name) and select “Inspect” or “Inspect Element” from the context menu.\n\n\n\nimage.png\n\n\nYou can see it at the bottom!\n\n\nIdentify Data to Scrape\nOnce you’re in the developer tools, explore the HTML structure to identify the elements containing the data you want to scrape. Look for unique identifiers such as class names, IDs, or tag names that can help you locate the desired information. I will guide you with the details later, don’t worry.\n\n\nPlan Scraping Strategy\nBased on the information you want to extract and its location in the HTML structure, plan your scraping strategy. Determine which Scrapy selectors (e.g., CSS selectors, XPath expressions) you’ll use to target and extract the data.\nYou might want to get familar with these before we proceed:\nRequest objects: https://docs.scrapy.org/en/latest/topics/request-response.html#request-objectsLinks\nResponse objects: https://docs.scrapy.org/en/latest/topics/request-response.html#response-objectsLinks\nSelector objects: https://docs.scrapy.org/en/latest/topics/selectors.html#scrapy.selector.Selector\n\n\n\n1.3 Implementation\nIn this section, we’ll create a Scrapy spider named tmdb_spider.py to scrape data from TMDB (The Movie Database) for a specific movie. We’ll set it up to accept the movie’s subdirectory as a command-line argument for easy customization.\n\nStep 1: Create tmdb_spider.py\nInside the spiders directory of your Scrapy project (TMDB_scraper), create a new Python file named tmdb_spider.py. You can do this manually or by running the following command in your terminal:\ntouch TMDB_scraper/spiders/tmdb_spider.py\nOpen this file in a text editor and add the following lines:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThe __init__ method in the spider class initializes the spider with the provided subdirectory argument. This allows us to customize the URL to scrape based on the movie’s subdirectory.\nTo run the spider and scrape data for a specific movie, use the following command:\nscrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nReplace 671-harry-potter-and-the-philosopher-s-stone with the subdirectory of the movie you want to scrape. This command instructs Scrapy to crawl the specified URL and save the scraped data to a CSV file named movies.csv.\nYou’ve successfully created a Scrapy spider named tmdb_spider.py configured to scrape data from TMDB for a specific movie. By providing the movie’s subdirectory as a command-line argument, you can easily customize the spider to scrape data for different movies.\nHere’s a basic outline of the TmdbSpider class:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        # Navigate to the Full Cast & Crew page\n        pass\n\n    def parse_full_credits(self, response):\n        # Extract information about the cast members\n        # Navigate to each actor's personal page\n        # Call parse_actor_page for each actor\n        pass\n\n    def parse_actor_page(self, response):\n        # Extract the actor's acting history from their personal page\n        pass\n\n\n1.2.1 parse(self, response)\nIn the parse method: - We construct the URL for the Full Cast & Crew page by appending “cast” to the movie page’s URL. - We then yield a scrapy.Request object with the URL of the Full Cast & Crew page and specify parse_full_credits as the callback method.\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        \"\"\"\n        Parse method to navigate to the Full Cast & Crew page and call parse_full_credits.\n\n        Args:\n            response (scrapy.http.Response): The response object containing the web page data.\n\n        Yields:\n            scrapy.Request: A scrapy request object to navigate to the Full Cast & Crew page.\n\n        \"\"\"\n        # Navigate to the Full Cast & Crew page by adding \"cast\" after the start URL\n        full_credits_url = response.url + \"cast\"\n        \n        # After navigation, call parse_full_credits\n        yield scrapy.Request(full_credits_url, callback=self.parse_full_credits)\nLet’s break down each part of the provided code snippet:\n\ndef parse(self, response): This is a method definition in a Scrapy spider class. It’s the default method called by Scrapy to handle responses downloaded for each request made.\nfull_credits_url = response.url + \"cast\": This line of code constructs the URL for the Full Cast & Crew page. It takes the current URL of the response object and appends “cast” to it, which is the relative URL for the Full Cast & Crew page.\nyield scrapy.Request(full_credits_url, callback=self.parse_full_credits): This line of code yields a new Scrapy Request object. Scrapy uses Request and Response objects for crawling web sites.Typically, Request objects are generated in the spiders and pass across the system until they reach the Downloader, which executes the request and returns a Response object which travels back to the spider that issued the request. This initiates a new request to the full_credits_url URL and specifies self.parse_full_credits as the callback function to handle the response.\n\nIn summary, this parse method is responsible for constructing a request to navigate to the Full Cast & Crew page and then calling the parse_full_credits method to handle the response from that page.\n\n\n1.2.2 parse_full_credits(self, response)\nThe overall code looks like this:\ndef parse_full_credits(self, response):\n    \"\"\"\n    Parse method to extract actor URLs from the Full Cast & Crew page and yield requests to parse actor pages.\n\n    Args:\n        response (scrapy.http.Response): The response object containing the Full Cast & Crew page data.\n\n    Yields:\n        scrapy.Request: A scrapy request object to parse each actor's page.\n\n    \"\"\"\n    cast_list = response.css('h3:contains(\"Cast\") + ol li div.info')\n    for actor in cast_list:\n        actor_url = actor.css(\"a::attr(href)\").get()\n        actor_page_url = response.urljoin(actor_url)\n        yield scrapy.Request(actor_page_url, callback=self.parse_actor_page)\n\ndef parse_full_credits(self, response): This is a method definition within a Scrapy spider class. It’s intended to handle the response from the Full Cast & Crew page.\ncast_list = response.css('h3:contains(\"Cast\") + ol li div.info'): This line of code uses CSS selectors to locate the elements containing information about each actor in the cast. It selects the &lt;div&gt; elements with the class “info” that are within list items (&lt;li&gt;) following an &lt;ol&gt; element immediately after an &lt;h3&gt; element containing the text “Cast” since we only want the Cast section instead of Crew. For how to use selector, make sure go through this: https://docs.scrapy.org/en/latest/topics/selectors.html\n\n\n\n\nimage.png\n\n\nNow we have all the casts information, we need to extract each person’s page url by iteration.\n\nfor actor in cast_list:: This loop iterates over each element in the cast_list.\nactor_url = actor.css(\"a::attr(href)\").get(): Within each iteration of the loop, this line of code extracts the URL of the actor’s personal page. It selects the value of the “href” attribute of the &lt;a&gt; element within the current actor’s &lt;div&gt; as you see in the screenshot above.\nactor_page_url = response.urljoin(actor_url): This line constructs the absolute URL for the actor’s personal page by joining the relative URL extracted in the previous step with the base URL of the current response.\nyield scrapy.Request(actor_page_url, callback=self.parse_actor_page): Finally, this line yields a new Scrapy Request object for each actor’s personal page. It specifies self.parse_actor_page as the callback function to handle the response from each actor’s page.\n\nIn summary, this parse_full_credits method is responsible for extracting the URLs of each actor’s personal page from the Full Cast & Crew page and yielding requests to scrape data from those individual pages using the parse_actor_page method.\n\n\n1.2.3 parse_actor_page(self, response)\nIn this tutorial, we’ll cover how to scrape data from an actor’s personal page, focusing on extracting their past or current movies or TV shows.\nThe overall code looks like this:\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parse method to extract acting credits for an actor from their personal page.\n\n    Args:\n        response (scrapy.http.Response): The response object containing the actor's personal page data.\n\n    Yields:\n        dict: A dictionary containing the actor's name and the movie or TV show they acted in.\n\n    \"\"\"\n    # Yield a dictionary for each movie or TV show the actor has worked in an \"Acting\" role\n    actor_name = response.css('div.title h2.title a::text').get()\n    acting_table = response.css('h3:contains(\"Acting\") + table.card.credits')\n    name_list = acting_table.css(\"a.tooltip\")\n\n    for name in name_list:\n        movie_or_TV_name = name.css(\"bdi::text\").get() \n        yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\n\n\nimage.png\n\n\n\nactor_name = response.css('div.title h2.title a::text').get(): This line extracts the actor’s name from the title of their personal page, which is under &lt;div class='title'&gt;, then under &lt;h2 class='title'&gt; then inside the &lt;a&gt;``&lt;/a&gt; here we can get the name of the actor and store it in the actor_name, which also can be done in pervious method, feel free to try it out!\n\n\n\n\nimage.png\n\n\n\nacting_table = response.css('h3:contains(\"Acting\") + table.card.credits'): This line selects the table containing the actor’s acting credits. It looks for the heading “Acting” and then selects the adjacent table with the class “card” and “credits”. Note that there are several tables under the &lt;div class='credits_list'&gt; make sure you only capture the table with Acting as the header.\n\n\n\n\nimage.png\n\n\n\nname_list = acting_table.css(\"a.tooltip\"): This line selects all the name of a movie or TV show in which the actor has acted.\nfor name in name_list:: This loop iterates over each link in the name_list since we only want the text part of the list.\n\n\n\n\nimage.png\n\n\n\nmovie_or_TV_name = name.css(\"bdi::text\").get(): Within each iteration of the loop, this line extracts the text representing the name of the movie or TV show from the link.\nyield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}: Finally, this line yields a dictionary containing the actor’s name and the name of the movie or TV show they acted in.\n\nFinally, we are done with the scraping part. We’ve learned how to extract an actor’s acting roles from their personal page using Scrapy. By understanding the structure of the actor’s page and using appropriate CSS selectors, we were able to extract the desired information effectively.\nNow try to run this in terminal to implement the scraper:\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=157336-interstellar\nFeel free to replace any Movie or TV show you like!\nThis would give you a results.csv scraped. Before we proceed, remember to remove the restriction in the settings.py."
  },
  {
    "objectID": "posts/HW2/index.html#visualization-and-recommendation-1",
    "href": "posts/HW2/index.html#visualization-and-recommendation-1",
    "title": "Movie or TV Show Recommendation",
    "section": "2. Visualization and Recommendation",
    "text": "2. Visualization and Recommendation\n\n# import libraries\nimport pandas as pd\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom plotly import express as px\n\nLet’s import this file into our notebook to see what we got!\n\ndf = pd.read_csv(\"results.csv\")\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2063 entries, 0 to 2062\nData columns (total 2 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   actor             2063 non-null   object\n 1   movie_or_TV_name  2063 non-null   object\ndtypes: object(2)\nmemory usage: 32.4+ KB\n\n\nWe’ve got 2063 movies and TV shows!\n\ndf.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nWilliam Patrick Brown\nInterstellar\n\n\n1\nWilliam Patrick Brown\nMighty Med\n\n\n2\nWilliam Patrick Brown\nFriends with Benefits\n\n\n3\nWilliam Patrick Brown\nChuck\n\n\n4\nMatthew McConaughey\n2024\n\n\n\n\n\n\n\nLet’s determine the overlaps between these movies and TV shows to form the basis of our recommendation. The logic is straightforward: we’ll analyze the movies or TV shows that feature the actors from your favorite movie or show in the highest quantity.\n\nrecmd_movies_or_TV = pd.DataFrame(df.movie_or_TV_name.value_counts().reset_index())\n\ndf.movie_or_TV_name.value_counts() we extract the movie_or_TV_name column from the dataframe and get unique values of this column with its count through .value_counts(), which would give us a Series. In order to make use of this Series, we reset the index through .reset_index() and transfer it into another data frame via pd.DataFrame().\n\n# original column name are movie_or_TV_name and count, we change them to understand it easier into movie names and number of shared actors\nrecmd_movies_or_TV = recmd_movies_or_TV.rename(columns={'movie_or_TV_name':'movie names', 'count':'number of shared actors'})\n\n\nrecmd_movies_or_TV['number of shared actors'].value_counts()\n\nnumber of shared actors\n1     1602\n2      107\n3       18\n4        9\n7        6\n6        6\n9        2\n8        2\n35       1\n10       1\nName: count, dtype: int64\n\n\nWhat we observe here is that 1602 rows of the data correspond to a single actor. This indicates that these actors are the only ones in the Interstellar crew who have performed in another movie or show, without any other members of the Interstellar crew. The same interpretation applies to the subsequent results, and the output is sorted based on the count.\nNext, we may want to split the data to visualize it in two parts, as we are not interested in the solo performances for those 1602 rows, or in movies or TV shows that contain only 2, 3, or 4 members of the full crew. You can define the splitting criteria according to your own preferences; here, I am simply dividing it into recmd_more and recmd_less based on the presence of 4 or more shared actors.\n\nrecmd_more = recmd_movies_or_TV[recmd_movies_or_TV['number of shared actors'] &gt;= 4 ]\n\n\nrecmd_less = recmd_movies_or_TV[recmd_movies_or_TV['number of shared actors'] &lt; 4 ]\n\nWe create a horizontal bar plot using Plotly Express. We specify the DataFrame recmd_more as the data source, set the ‘number of shared actors’ as the x-axis, ‘movie names’ as the y-axis, and provide additional parameters for orientation and text.\nimport plotly.express as px\n\nfig = px.bar(recmd_more, x='number of shared actors', y='movie names', orientation='h',\n             text='number of shared actors', title='Movie Names and Number of Shared Actors &gt;= 5')\nWe can adjust the position of the text annotations on the bars using the update_traces method.\nfig.update_traces(textposition='outside')\nNext, we customize the layout of the plot by specifying axis titles, ordering categories, adjusting tick marks, and setting the height of the plot.\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='total ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=600\n)\nBy following these steps, you can create a customized horizontal bar plot using Plotly Express in Python. Experiment with different parameters and settings to achieve the desired visualization for your data.\nFeel free to further customize the plot according to your specific requirements, such as adjusting colors, fonts, or adding annotations.\nHere is the full code:\n\nfig = px.bar(recmd_more, x='number of shared actors', y='movie names', orientation='h',\n             text='number of shared actors', title='Movie/Show Names and Number of Shared Actors &gt;= 5')\n\nfig.update_traces(textposition='outside')\n\n# Customizing the layout\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='total ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=500\n)\n\n                                                \n\n\nAs we can see from the plot, Interstellar would of course be the top one we would recommend the it appears to be Saturday Night Live, which shares 10 actors. The View, Late Night with Seth Meyers share 9 actors with Interstellar. Well this would be a tool to do the recommand that is super obvious for the reader or the viewer.\n\nplt.figure(figsize=(8, 4))\nsns.histplot(recmd_less['number of shared actors'], bins=range(1, 6), kde=False, color='skyblue')\nplt.xlabel('Number of Shared Actors')\nplt.ylabel('Number of Movies/TV Shows')\nplt.title('Distribution of Movies/TV Shows with Less Than 5 Shared Actors')\nplt.show()\n\n\n\n\n\n\n\n\nSince we cannot plot the thousands rows, we can visualize it through histplot though this provides not that much of the information we need for recommendation.\nNow we are going to see that for each actor, the percentage of sole performance on the overall acting histroy, sole performance here means not with any other actors in the Interstellar crew.\nBefore we begin, let’s define the required columns in our DataFrame:\n\nisDup: A boolean column indicating whether the movie or TV show is unique. If True, it means there is a duplicate movie or TV show in the DataFrame, indicating it’s not a sole performance.\ndup_cnt: The count of occurrences where isDup is True or False, grouped by actor and isDup.\ntotal_cnt: The total count of all acting roles for each actor.\n\nCalculate isDup Column\nWe’ll calculate the isDup column using the duplicated method, which marks duplicate entries as True and unique entries as False.\ndf['isDup'] = df.duplicated(subset=['movie_or_TV_name'], keep=False)\nCalculate dup_cnt and total_cnt Columns\nNext, we’ll calculate the dup_cnt and total_cnt columns using the groupby method along with the transform function to get the count of occurrences for each actor.\ndf['dup_cnt'] = df.groupby([\"actor\", 'isDup'])['movie_or_TV_name'].transform('count')\ndf['total_cnt'] = df.groupby([\"actor\"])['movie_or_TV_name'].transform('count')\n\ndf['isDup'] = df.duplicated(subset=['movie_or_TV_name'], keep=False)\n\ndf['dup_cnt'] = df.groupby([\"actor\", 'isDup'])['movie_or_TV_name'].transform('count')\n\ndf['total_cnt'] = df.groupby([\"actor\"])['movie_or_TV_name'].transform('count')\n\n\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nisDup\ndup_cnt\ntotal_cnt\n\n\n\n\n0\nWilliam Patrick Brown\nInterstellar\nTrue\n2\n4\n\n\n1\nWilliam Patrick Brown\nMighty Med\nFalse\n2\n4\n\n\n2\nWilliam Patrick Brown\nFriends with Benefits\nTrue\n2\n4\n\n\n3\nWilliam Patrick Brown\nChuck\nFalse\n2\n4\n\n\n4\nMatthew McConaughey\n2024\nFalse\n91\n127\n\n\n...\n...\n...\n...\n...\n...\n\n\n2058\nJohn Lithgow\nDealing: Or the Berkeley-to-Boston Forty-Brick...\nFalse\n151\n189\n\n\n2059\nJohn Lithgow\nGreat Performances\nFalse\n151\n189\n\n\n2060\nJohn Lithgow\nTony Awards\nTrue\n38\n189\n\n\n2061\nJohn Lithgow\nToday\nTrue\n38\n189\n\n\n2062\nJohn Lithgow\nHallmark Hall of Fame\nTrue\n38\n189\n\n\n\n\n2063 rows × 5 columns\n\n\n\nNow we contruct the plot: Let’s break down what each line of code does:\npx.bar(df, x='total_cnt', y='actor', orientation='h', color='isDup', text='total_cnt', title='Sole/Shared Performance Count for Each Actor'): - This line creates a horizontal bar plot using Plotly Express (px.bar). - The DataFrame df is used as the data source. - The x-axis represents the total count of performances (total_cnt). - The y-axis represents the actors (actor). - The bars are oriented horizontally (orientation='h'). - The color of the bars is determined by the isDup column. - The text annotations on the bars display the total count of performances. - The title of the plot is set to ‘Sole/Shared Performance Count for Each Actor’.\nfig.update_traces(textposition='outside'): - This line updates the position of the text annotations on the bars to be outside the bars. - By default, the text annotations are placed inside the bars, but setting textposition='outside' moves them outside for better visibility.\nyaxis_categoryorder='max ascending': This line specifies the order of categories (actors) on the y-axis based on the maximum value of their associated data points. Categories with higher maximum values will appear towards the top of the plot, while categories with lower maximum values will appear towards the bottom. You can alter the values to experiment.\nyaxis=dict(tickmode='linear'): This line ensures that the tick marks on the y-axis are displayed in a linear fashion. This is the default behavior, so it’s not necessary to explicitly specify it unless you want to override any previous settings.\n\nfig = px.bar(df, x='total_cnt', y='actor', orientation='h', color='isDup',\n             text='total_cnt', title='Sole/Shared Performance Count for Each Actor')\n\nfig.update_traces(textposition='outside')\n\n# Customizing the layout\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='max ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=600\n)\n\n\n\n\nBy hovering the plot, we can see that Michael Caine, John Lithgow, and Matt Damon have the greatest number of shared movies or TV shows with other crews in the Interstellar even though Michael Caine, John Lithgow, and Ellen Burstyn have the greatest number of actings. Therefore, we could not only recommand by the number of overlap among movies or shows, but also by the ‘popularity’ of the actor."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "Global Warming: Data Wrangling and Visualization",
    "section": "",
    "text": "Hello!"
  },
  {
    "objectID": "posts/HW1/index.html#intro",
    "href": "posts/HW1/index.html#intro",
    "title": "Global Warming: Data Wrangling and Visualization",
    "section": "Intro",
    "text": "Intro\nIn this blog, we will develop interactive tools that offer an engaging exploration of the impact of global warming on temperatures worldwide. Moreover, I’ll guide you through the process, assuming you possess fundamental Python skills. The primary tools employed include Matplotlib and Seaborn, with a significant focus on Plotly for creating interactive graphics. Additionally, we’ll utilize Pandas and NumPy for efficient dataframe manipulation and SQLite3 for establishing the database.\nThe data set that we are going to use are:\n\ntemperatures(NOAA-GHCN data): temps.csv from the class\nstations: station-metadata.csv fromt the class\ncontries: https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\n\nLet’s get start it!!"
  },
  {
    "objectID": "posts/HW1/index.html#import-libraries-1",
    "href": "posts/HW1/index.html#import-libraries-1",
    "title": "Global Warming: Data Wrangling and Visualization",
    "section": "1. Import Libraries",
    "text": "1. Import Libraries\n\n# Set the default Plotly renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n\nimport pandas as pd\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport sqlite3\n\nfrom sklearn.linear_model import LinearRegression\nfrom plotly import express as px\nimport calendar\n\nimport plotly.subplots as sp\nimport plotly.graph_objs as go"
  },
  {
    "objectID": "posts/HW1/index.html#create-data-base-1",
    "href": "posts/HW1/index.html#create-data-base-1",
    "title": "Global Warming: Data Wrangling and Visualization",
    "section": "2. Create Data Base",
    "text": "2. Create Data Base\nThe Data Base called HW1.db would contain 3 tables: temperatures, stations, and contries.\nWe will do this by : - First connect to the data base - Then prepare the data frames before write it into the data base. - Lastly, check the table information and close the connection.\n\n# Connect to the data base HW1.db\n\nconn = sqlite3.connect(\"HW1.db\")\n\n\nTable1: temperatures\n\ntemps = pd.read_csv(\"temps.csv\")\n\n\ndef prepare_temp_df(df):\n    \"\"\"\n    Preprocesses a DataFrame containing temperature data.\n\n    Parameters:\n    - df (pd.DataFrame): Input DataFrame containing temperature information.\n\n    Returns:\n    - pd.DataFrame: Processed DataFrame with columns: 'ID', 'Year', 'Month', 'Temp'.\n      'Month' is extracted from the original DataFrame, and 'Temp' is normalized to degrees Celsius.\n\n    Steps:\n    1. Set the multi-index with keys 'ID' and 'Year'.\n    2. Stack the DataFrame to transform columns into rows.\n    3. Reset the index to create a new default integer index.\n    4. Rename columns to 'Month' and 'Temp'.\n    5. Extract the numeric month from the 'Month' column.\n    6. Normalize the 'Temp' column by dividing by 100.\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns={\"level_2\": \"Month\", 0: \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"] / 100\n    return df\n\n\ntemperature_df = prepare_temp_df(temps)\n\n\n# Write the DataFrame 'temperature_df' to an SQLite database table named 'temperatures'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\n\ntemperature_df.to_sql(\"temperatures\", conn, if_exists = \"replace\", index = False)\n\n13992662\n\n\n\n\nTable2: stations\n\nstations = pd.read_csv(\"station-metadata.csv\")\n\n\n# Write the DataFrame 'stations' to an SQLite database table named 'stations'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\n\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n\n\nTable3: countries\n\n# Read the CSV data from the URL 'countries_url' into a DataFrame 'countries'\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\n\n# Write the DataFrame 'countries' to an SQLite database table named 'countries'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\n\n279\n\n\n\n# Create a cursor object from the SQLite connection\ncursor = conn.cursor()\n\n# Execute a SQL query to retrieve the CREATE TABLE statements for all tables in the database\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\n# Fetch and print the CREATE TABLE statements for each table\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nHere we checked the names of tables, columns of tables, and the storage type of each column.\n\n# Close the connection\nconn.close()"
  },
  {
    "objectID": "posts/HW1/index.html#query-function-query_climate_database-1",
    "href": "posts/HW1/index.html#query-function-query_climate_database-1",
    "title": "Global Warming: Data Wrangling and Visualization",
    "section": "3. Query Function: query_climate_database",
    "text": "3. Query Function: query_climate_database\nThis Query Function located in climate_database.py would extract the information we need from those 3 tables and manipulate it with restrictions.\nWhat this query_climate_database really do is you give it the db file you are gonna use, the country that you want to investigate, the year range of the data you want, and the specific month of temperatures you want.\n\ndb_file, the file name for the database\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\n\nfrom climate_database import query_climate_database\nimport inspect\n\n# Print the source code of the 'query_climate_database' function\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    with sqlite3.connect(db_file) as conn:\n\n        cmd = \\\n        f\"\"\"\n        SELECT S.name, S.latitude, S.longitude, C.name as Country, T.year, T.month, T.temp\n        FROM countries C\n        JOIN stations S \n        ON SUBSTR(S.id, 1, 2) = C.\"FIPS 10-4\"\n        JOIN temperatures T\n        ON S.id = T.id\n        WHERE C.name = '{country}'\n        AND (T.year &gt;= {year_begin} AND T.year &lt;= {year_end})\n        AND T.month = {month}\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    return df\n\n\n\nWe try out the function we imported with parameters country = “India”, year_begin = 1980, year_end = 2020,month = 1. What the function would give us is a data frame with 7 columns: name, latitude, longitude, Name, Year, Month, Temp, so that we would know each station’s temperature of Jan. in India from 1980 to 2020.\nThe result has a shape of 3152*7.\n\n# test case\nquery_climate_database(db_file = \"HW1.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nName\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/HW1/index.html#a-geographic-scatter-function-for-yearly-temperature-increases-1",
    "href": "posts/HW1/index.html#a-geographic-scatter-function-for-yearly-temperature-increases-1",
    "title": "Global Warming: Data Wrangling and Visualization",
    "section": "4. A Geographic Scatter Function for Yearly Temperature Increases",
    "text": "4. A Geographic Scatter Function for Yearly Temperature Increases\n\nHow does the average yearly change in temperature vary within a given country?\nGiven this question, we would like to write a function that would give us the answer vividly.\nThis function temperature_coefficient_plot should accept six explicit arguments, and an undetermined number of keyword arguments.\n\ndb_file, country, year_begin, year_end, and month should be as in the previous part.\nmin_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month should be plotted; the others should be filtered out. df.transform() plus filtering is a good way to achieve this task.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\n\n# convert the numbers of month into names so that we could use in the title\nmonth_names = calendar.month_name\nmonth_number_to_name = {index: month for index, month in enumerate(month_names) if month}\n\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, db_file='HW1.db', **kwargs):\n    \"\"\"\n    Generate a scatter plot on a map showing the estimated yearly increase in temperature for weather stations in a specified country and month.\n\n    Parameters:\n    - country (str): Country name.\n    - year_begin (int): Starting year for data retrieval.\n    - year_end (int): Ending year for data retrieval.\n    - month (int): Numeric representation of the month for data retrieval(1-12).\n    - min_obs (int): Minimum number of years of observation required for a station to be included.\n    - db_file (str, optional): Database file path. Default is 'HW1.db'.\n    - **kwargs: Additional keyword arguments to be passed to Plotly Express scatter_mapbox.\n\n    Returns:\n    - plotly.graph_objs.Figure: Scatter plot on a map.\n\n    Note:\n    - Requires the 'query_climate_database' function from the 'climate_database' module.\n\n    Example:\n    ```python\n    fig = temperature_coefficient_plot('USA', 2000, 2020, 7, 10)\n    fig.show()\n    ```\n    \"\"\"\n    # load the database\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    \n    # filtering \n    df['num_years'] = df.groupby(['NAME', 'Month'])['Year'].transform('nunique')\n    df = df[df['num_years'] &gt;= min_obs].drop(columns='num_years')\n    \n    # calculate the average yearly change in temperature for each station\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"] \n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n    \n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    \n    # merge the station info with yearly temp\n    station_info = df.groupby('NAME', as_index=False).agg({'LATITUDE': 'first', 'LONGITUDE': 'first'})\n    df_merge = pd.merge(coefs, station_info, on='NAME')\n    df_merge = df_merge.rename(columns={0: 'Estimated Yearly Increase(°C)'})\n    df_merge['Estimated Yearly Increase(°C)'] = df_merge['Estimated Yearly Increase(°C)'].round(4)\n    \n    month_name = month_number_to_name[month]\n    # plot\n    fig = px.scatter_mapbox(df_merge,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color='Estimated Yearly Increase(°C)',\n                            title=f\"Estimates of Yearly Increase in Temperature in {month_name} for Stations in {country}, Years {year_begin}-{year_end}\",\n                            color_continuous_midpoint=0,\n                            **kwargs\n                           )\n\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\n# if you want to make the graph larger:\n# fig.update_layout(margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 30})\n\nfig.show()\n\n\n\n\nAs we can see from the plot Ludhiana, India had a major increase in Jan. temperature from 1980-2020 with a rate of 0.1318. Some other stations near the coast also experienced a increase in Jan. temperature from 1980-2020. The stations inside the country mainly have a decrease in Jan. temperature from 1980-2020.\n\ncolor_map = px.colors.diverging.RdBu_r # choose a colormap\n\nfig = temperature_coefficient_plot('Russia', 1980, 2010, 6, \n                                   min_obs = 12,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\n# if you want to make the graph larger:\n# fig.update_layout(margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 30})\n\nfig.show()\n\n\n\n\nFrom the plot, it’s evident that Amguema, Russia experienced a significant increase in June temperature from 1980 to 2020, with a rate of 0.286. Most cities are located in the southern regions due to warmer climates, while city distribution in Russia is more dispersed in the north due to colder temperatures. It’s notable that the right-hand side of the plot shows mostly increasing temperatures, with western cities experiencing a slight decrease, albeit minimal as indicated by the lighter color close to white. Conversely, eastern cities exhibit more pronounced changes in temperature, indicated by darker colors. We can potentially observe the effects of global warming near the North Pole, as the upper region of Russia, which is close to the Arctic, shows an overall increase in temperature."
  },
  {
    "objectID": "posts/HW1/index.html#temperature-changes-in-difference-climate-zone",
    "href": "posts/HW1/index.html#temperature-changes-in-difference-climate-zone",
    "title": "Global Warming: Data Wrangling and Visualization",
    "section": "5. Temperature Changes in Difference Climate Zone",
    "text": "5. Temperature Changes in Difference Climate Zone\n\nBackground\nThere are many different ways to classify climate zones, here is one of them:\nThere are 4 major climate zones.\n\nTropical zone from 0°– 23.5°/ -23.5° - 0°(between the tropics)\nIn the regions between the equator and the tropics (equatorial region), the solar radiation reaches the ground nearly vertically at noontime during almost the entire year. Thereby, it is very warm in these regions. Through high temperatures, more water evaporates and the air is often moist. The resulting frequent and dense cloud cover reduces the effect of solar radiation on ground temperature.\n\n\nSubtropics from 23.5°– 40°/ -40° - -23.5°\nThe subtropics receive the highest radiation in summer, since the Sun’s angle at noon is almost vertical to the Earth, whilst the cloud cover is relatively thin. These regions receive less moisture (see trade winds), what increases the effect of radiation. Therefore, most of the deserts in the world are situated in this zone. In winter, the radiation in these regions decreases significantly, and it can temporarily be very cool and moist.\n\n\nTemperate zone from 40°–60°/ -60° - -40°\nIn the temperate zone, the solar radiation arrives with a smaller angle, and the average temperatures here are much cooler than in the subtropics. The seasons and daylength differ significantly in the course of a year. The climate is characterised by less frequent extremes, a more regular distribution of the precipitation over the year and a longer vegetation period - therefore the name “temperate”.\n\n\nCold zone from 60°–90°/ -90° - -60°\nThe polar areas between 60° latitude and the poles receive less heat through solar radiation, since the Sun has a very flat angle toward the ground. Because of the changes of the Earth axis angle to the Sun, the daylength varies most in this zone. In the summer, polar days occur. Vegetation is only possible during a few months per year and even then is often sparse. The conditions for life in these regions are very hard.\n\n\n\nHow has global warming impacted the four climate zones throughout the years and across different seasons?\nThis query allyear_climate_region_database() would give you Name, NAME, Latitude, Year, Month, Temp if you input the db_file, la_lower, la_upper - db_file - la_lower and la_upper, the latitude range you want to investgate\n\nfrom climate_database import allyear_climate_region_database\nimport inspect\n\n# Print the source code of the 'allyear_climate_region_database' function\nprint(inspect.getsource(allyear_climate_region_database))\n\ndef allyear_climate_region_database(db_file, la_lower, la_upper):\n    with sqlite3.connect(db_file) as conn:\n\n        cmd = \\\n        f\"\"\"\n        SELECT C.name, S.name, S.latitude, T.year, T.month, T.temp\n        FROM countries C\n        JOIN stations S \n        ON SUBSTR(S.id, 1, 2) = C.\"FIPS 10-4\"\n        JOIN temperatures T\n        ON S.id = T.id\n        WHERE (S.latitude &gt;= {la_lower} AND S.latitude &lt;= {la_upper})\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    return df\n\n\n\nThe two_season_plot(a, b,season_a, season_b) and four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d) would draw a 2*1 plot or 4*1 plot, coresponds to each season. X axis would be the Year, Y axis would be the Temperatures throughout years.\nThere would 3 lines, the red line is the Max temperature of each month throughout years, the blue line is the Min temperature of each month throughout years, and the black line is the Avg temperature of the year.\n\ndef two_season_plot(a, b, season_a, season_b):\n    \"\"\"\n    Generate a side-by-side plot comparing temperature trends for two different seasons over the years.\n\n    Parameters:\n    - a (pd.DataFrame): DataFrame for the first season's temperature data.\n    - b (pd.DataFrame): DataFrame for the second season's temperature data.\n    - season_a (str): Name or identifier for the first season.\n    - season_b (str): Name or identifier for the second season.\n\n    Returns:\n    - plotly.graph_objs.Figure: Side-by-side line plots with linear regression lines for both seasons.\n\n    Example:\n    ```python\n    fig = two_season_plot(df_season_spring, df_season_fall, 'Spring', 'Fall')\n    fig.show()\n    ```\n    \"\"\"\n    # Find the common x-axis range for both DataFrames\n    x_range = [min(min(a['Year']), min(b['Year'])), max(max(a['Year']), max(b['Year']))]\n\n    # Calculate y_range\n    y_range = [min(min(a['MinTemp']), min(b['MinTemp'])), max(max(a['MaxTemp']), max(b['MaxTemp']))]\n\n    # Create subplots with 1 row and 2 columns\n    fig = sp.make_subplots(rows=2, cols=1, subplot_titles=[f'{season_a} Temperature Over Years', f'{season_b} Temperature Over Years'], row_heights=[3, 3])\n\n    # Add line plot for DataFrame 'a' to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)')), row=1, col=1)\n\n    # Calculate linear regression for DataFrame 'a'\n    a_slope, a_intercept = np.polyfit(a['Year'], a['AvgTemp'], 1)\n    a_regression_line = a_slope * a['Year'] + a_intercept\n\n    # Add linear regression line to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a_regression_line, mode='lines', name=f'Regression A: {a_slope:.2f}x + {a_intercept:.2f}', line=dict(color='lightblue')), row=1, col=1)\n\n    # Add line plot for DataFrame 'b' to the second subplot\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['AvgTemp'], mode='lines',name='Avg Temperature', line=dict(color='black'), showlegend=False), row=2, col=1)\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['MinTemp'], mode='lines',name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)'), showlegend=False), row=2, col=1)\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['MaxTemp'], mode='lines',name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)'), showlegend=False), row=2, col=1)\n\n    # Calculate linear regression for DataFrame 'b'\n    b_slope, b_intercept = np.polyfit(b['Year'], b['AvgTemp'], 1)\n    b_regression_line = b_slope * b['Year'] + b_intercept\n\n    # Add linear regression line to the second subplot\n    fig.add_trace(go.Scatter(x=b['Year'], y=b_regression_line, mode='lines', name=f'Regression B: {b_slope:.2f}x + {b_intercept:.2f}', line=dict(color='lightcoral')), row=2, col=1)\n\n    # Set the common x-axis range for both subplots\n    fig.update_xaxes(range=x_range, row=1, col=1)\n    fig.update_xaxes(range=x_range, row=2, col=1)\n\n    # Set the common y-axis range for both subplots\n    fig.update_yaxes(range=y_range, row=1, col=1)\n    fig.update_yaxes(range=y_range, row=2, col=1)\n    \n    fig.update_xaxes(title_text='Year', row=2, col=1)\n    fig.update_yaxes(title_text='Temperature', row=1, col=1)\n    fig.update_yaxes(title_text='Temperature', row=2, col=1)\n\n    # Update layout if needed\n    fig.update_layout(showlegend=True)  # Optional: set showlegend=False if you don't want legends for each subplot\n\n    return fig\n\n\ndef four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d):\n    \"\"\"\n    Generate a four-subplot plot comparing temperature trends for four different seasons over the years.\n\n    Parameters:\n    - a, b, c, d (pd.DataFrame): DataFrames for temperature data of four different seasons.\n    - season_a, season_b, season_c, season_d (str): Names or identifiers for the four seasons.\n\n    Returns:\n    - plotly.graph_objs.Figure: Four-subplot line plots with linear regression lines for each season.\n\n    Example:\n    ```python\n    fig = four_season_plot(df_season_spring, df_season_summer, df_season_fall, df_season_winter, 'Spring', 'Summer', 'Fall', 'Winter')\n    fig.show()\n    ```\n    \"\"\"\n    # Find the common x-axis range for all DataFrames\n    x_range = [min(min(a['Year']), min(b['Year']), min(c['Year']), min(d['Year'])),\n               max(max(a['Year']), max(b['Year']), max(c['Year']), max(d['Year']))]\n\n    # Calculate y_range\n    y_range = [min(min(a['MinTemp']), min(b['MinTemp']), min(c['MinTemp']), min(d['MinTemp'])),\n               max(max(a['MaxTemp']), max(b['MaxTemp']), max(c['MaxTemp']), max(d['MaxTemp']))]\n\n    # Create subplots with 4 rows and 1 column\n    fig = sp.make_subplots(rows=4, cols=1, subplot_titles=[f'{season_a} Temperature Over Years', f'{season_b} Temperature Over Years', f'{season_c} Temperature Over Years', f'{season_d} Temperature Over Years'], vertical_spacing=0.1)\n\n    # Add line plot for DataFrame 'a' to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)')), row=1, col=1)\n\n    # Calculate linear regression for DataFrame 'a'\n    a_slope, a_intercept = np.polyfit(a['Year'], a['AvgTemp'], 1)\n    a_regression_line = a_slope * a['Year'] + a_intercept\n\n    # Add linear regression line to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a_regression_line, mode='lines', name=f'Regression A: {a_slope:.2f}x + {a_intercept:.2f}', line=dict(color='lightblue')), row=1, col=1)\n\n    i = 2\n    for data in (b, c, d):\n        # Add line plot for DataFrame 'b' to the second subplot\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black'), showlegend=False), row=i, col=1)\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)'), showlegend=False), row=i, col=1)\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)'), showlegend=False), row=i, col=1)\n\n        # Calculate linear regression for DataFrame 'b'\n        slope, intercept = np.polyfit(data['Year'], data['AvgTemp'], 1)\n        regression_line = slope * data['Year'] + intercept\n\n        # Add linear regression line to the second subplot\n        fig.add_trace(go.Scatter(x=data['Year'], y=regression_line, mode='lines', name=f\"Regression {i}: {slope:.2f}x + {intercept:.2f}\", line=dict(color='lightcoral')), row=i, col=1)\n\n        i += 1\n\n    for r in range(1, 5):\n        # Set the common x-axis range for all subplots\n        fig.update_xaxes(range=x_range, row=r, col=1)\n\n        # Set the common y-axis range for all subplots\n        fig.update_yaxes(range=y_range, row=r, col=1)\n\n        fig.update_yaxes(title_text='Temperature', row=r, col=1)\n\n    # Update layout if needed\n    fig.update_xaxes(title_text='Year', row=4, col=1)\n    fig.update_layout(showlegend=True)  # Optional: set showlegend=False if you don't want legends for each subplot\n\n    return fig\n\nThe season_plot will notify you the latitude you inputted is locate in which climate zone and the yearly temprature of that zone in different seasons.\n\ndef season_plot(la_lower, la_upper, db_file='HW1.db'):\n    \"\"\"\n    Generate temperature plots for different seasons based on latitude range.\n\n    Parameters:\n    - la_lower, la_upper (float): Latitude range for temperature data.\n    - db_file (str): Database file containing temperature data. Default is 'HW1.db'.\n\n    Returns:\n    - plotly.graph_objs.Figure: Temperature plots based on latitude range and seasons.\n\n    Example:\n    ```python\n    fig = season_plot(-40, -23.5, 'climate_data.db')\n    fig.show()\n    ```\n    \"\"\"\n    df = allyear_climate_region_database(db_file, la_lower, la_upper)\n\n    # Northern Tropical Zone (0-23.5)\n    if 0 &lt;= la_lower &lt;= 23.5 and 0 &lt;= la_upper &lt;= 23.5:\n        print('This latitude range is in the Northern Tropical Zone and it has only 2 seasons!')\n        filter1 = df[df['Month'].isin([10, 1, 2, 3])]  # Dry season\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([4, 5, 6, 7, 8, 9])]  # Wet season\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        season_a = 'Dry Season'\n        season_b = 'Wet Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Southern Tropical Zone (-23.5 to 0)\n    if -23.5 &lt;= la_lower &lt;= 0 and -23.5 &lt;= la_upper &lt;= 0:\n        print('This latitude range is in the Southern Tropical Zone and it has only 2 seasons!')\n        a = avg2\n        b = avg1\n        season_a = 'Dry Season'\n        season_b = 'Wet Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Northern Subtropical Zone (23.5-40)\n    if 23.5 &lt;= la_lower &lt;= 40 and 23.5 &lt;= la_upper &lt;= 40:\n        print('This latitude range is in the Northern Subtropical Zone and it has only 2 seasons!')\n        filter1 = df[df['Month'].isin([10, 1, 2, 3])]  # Hot summer\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([4, 5, 6, 7, 8, 9])]  # Wet season\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        season_a = 'Summer Season'\n        season_b = 'Mild Winter Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Southern Subtropical Zone (-40 to -23.5)\n    if -40 &lt;= la_lower &lt;= -23.5 and -40 &lt;= la_upper &lt;= -23.5:\n        print('This latitude range is in the Southern Subtropical Zone and it has only 2 seasons!')\n        a = avg2\n        b = avg1\n        season_a = 'Summer Season'\n        season_b = 'Mild Winter Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Northern Temperate Zone (40-60)\n    if 40 &lt;= la_lower &lt;= 60 and 40 &lt;= la_upper &lt;= 60:\n        print('This latitude range is in the Northern Temperate Zone and it has 4 seasons!')\n        filter1 = df[df['Month'].isin([3, 4, 5])]  # Spring\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([6, 7, 8])]  # Summer\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter3 = df[df['Month'].isin([9, 10, 11])]  # Autumn\n        avg3 = filter3.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg3.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter4 = df[df['Month'].isin([12, 1, 2])]  # Winter\n        avg4 = filter4.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg4.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        c = avg3\n        d = avg4\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    # Southern Temperate Zone (-60 to -40)\n    if -60 &lt;= la_lower &lt;= -40 and -60 &lt;= la_upper &lt;= -40:\n        print('This latitude range is in the Southern Temperate Zone and it has 4 seasons!')\n        a = avg3\n        b = avg4\n        c = avg1\n        d = avg2\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    # Northern Cold Zone (60-90)\n    if 60 &lt;= la_lower &lt;= 90 and 60 &lt;= la_upper &lt;= 90:\n        print('This latitude range is in the Northern Cold Zone and it has 4 seasons!')\n        filter1 = df[df['Month'].isin([3, 4, 5])]  # Spring\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([6, 7, 8])]  # Summer\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter3 = df[df['Month'].isin([9, 10, 11])]  # Autumn\n        avg3 = filter3.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg3.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter4 = df[df['Month'].isin([12, 1, 2])]  # Winter\n        avg4 = filter4.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg4.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        c = avg3\n        d = avg4\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    warning = 'Your latitude should be inside the range of each zone!'\n    return warning\n\n\n# let's try out the Northern Cold Zone\nresult = season_plot(60, 90)\n\nThis latitude range is in the Northern Cold Zone and it has 4 seasons!\n\n\n\nresult.update_layout(height=600, width=1000)\nresult.show()\n\n                                                \n\n\nHere you could do the experiment by yourself with different latitude. I plug in 60 and 90, which is in Northern Cold Zone and it has 4 seasons. The overall temperature is low compare to other places and the linear line of average temperature is nearly flat, which indicates a not much of difference among the temperatures over years. Other than that, Spring and Fall share the same distribution, Summer being the overall hottest, and Winter being the overall coldest.\n\n\nDoes the Global Warming effect differently in these 4 climate zones?\nTo answer this question, for each zone, we can plot the yearly average temperature for each zone.\n\n# Retrieving climate data for different latitude ranges to analyze specific climate zones\n\n# Tropical Zone: Latitude between -23.5 and 23.5 degrees\ntropical = allyear_climate_region_database('HW1.db', -23.5, 23.5)\n\n# Subtropical Zone: Latitude between 23.5 and 40 degrees (sub1) and -40 to -23.5 degrees (sub2)\nsub1 = allyear_climate_region_database('HW1.db', 23.5, 40)\nsub2 = allyear_climate_region_database('HW1.db', -40, -23.5)\nsub = pd.concat([sub1, sub2], ignore_index=True)\n\n# Temperate Zone: Latitude between 40 and 60 degrees (temperate1) and -60 to -40 degrees (temperate2)\ntemperate1 = allyear_climate_region_database('HW1.db', 40, 60)\ntemperate2 = allyear_climate_region_database('HW1.db', -60, -40)\ntemperate = pd.concat([temperate1, temperate2], ignore_index=True)\n\n# Cold Zone: Latitude between 60 and 90 degrees\ncold = allyear_climate_region_database('HW1.db', 60, 90)\n\n# Calculate average yearly temperature for each climate zone\ntro_avg = tropical.groupby('Year')['Temp'].mean()\nsub_avg = sub.groupby('Year')['Temp'].mean()\ntemperate_avg = temperate.groupby('Year')['Temp'].mean()\ncold_avg = cold.groupby('Year')['Temp'].mean()\n\n\nfig = go.Figure()\n\n# Add trace for tropical dataset\nfig.add_trace(go.Scatter(x=tro_avg.index, y=tro_avg.values, mode='markers', name='Tropical', line=dict(color='blue')))\n\n# Add trace for sub dataset\nfig.add_trace(go.Scatter(x=sub_avg.index, y=sub_avg.values, mode='markers', name='Subtropical', line=dict(color='green')))\n\n# Add trace for temperate dataset\nfig.add_trace(go.Scatter(x=temperate_avg.index, y=temperate_avg.values, mode='markers', name='Temperate', line=dict(color='orange')))\n\n# Add trace for cold dataset\nfig.add_trace(go.Scatter(x=cold_avg.index, y=cold_avg.values, mode='markers', name='Cold', line=dict(color='red')))\n\n# Update layout for better readability\nfig.update_layout(title='Yearly Average Temperature',\n                  xaxis_title='Year',\n                  yaxis_title='Temperature',\n                  legend=dict(title='Dataset'))\n\n# Show the plot\nfig.show()\n\n                                                \n\n\nThe overall Yearly Average Temperature for each climate zone did not change a lot throughout years but with a slight increase for each scatter line especially for the cold and tropical zone. However, from the average temperature, we could barely see the global warming effect. There are more infos that we need to investigate!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jamie’s Blog",
    "section": "",
    "text": "Movie or TV Show Recommendation\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nJiaxin Luo\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Warming: Data Wrangling and Visualization\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nJan 28, 2023\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nJamie\n\n\n\n\n\n\nNo matching items"
  }
]