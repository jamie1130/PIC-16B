[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "Penguin World",
    "section": "",
    "text": "Data Visualization\nIn this post, I’ll show you how to make a nice visualization of the famous “Palmer Penguins” dataset by using the Pandas, Matplotlib, and Seaborn packages in Python.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nLoad The Palmer Penguins Data Set\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nPandas is a powerful open-source library in Python used for data manipulation and analysis. It provides easy-to-use data structures and functions to work with structured data. Pandas is commonly used in data preprocessing tasks before feeding data into machine learning models.\n\n# Drop rows with missing values in specific columns\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\n\n# Extract the first word of the \"Species\" column\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\n# Filter out rows where \"Sex\" is not specified\npenguins = penguins[penguins[\"Sex\"] != \".\"]\n\n# Selecting specific columns\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]\n\n\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\nHave a general idea of the proportion of each species and the island they inhabit.\nMatplotlib is a widely used Python library for creating static, interactive, and animated visualizations in Python. It provides a MATLAB-like interface and can generate plots, histograms, power spectra, bar charts, error charts, scatterplots, etc.\n\nimport matplotlib.pyplot as plt\n\n# Count the occurrences of each species\nspecies = penguins.Species.value_counts()\n\n# Plotting the pie chart\nspecies.plot(kind='pie', autopct=\"%.2f%%\")\nplt.title('Percentage Distribution of Penguin Species')\n\n# Display the pie chart\nplt.show()\n\nText(0.5, 1.0, 'Percentage Distribution of Penguin Species')\n\n\n\n\n\n\n\n\n\nBased on the pie chart, the Palmer Penguins data set predominantly consists of Adelie penguins, accounting for 43.84%, followed by Gentoo penguins at 35.74%, and finally, Chinstrap penguins at 20.42%.\n\n# Count the occurrences of penguins on each island\nisland = penguins.Island.value_counts()\n\n# Plotting the pie chart\nisland.plot(kind='pie', autopct=\"%.2f%%\")\nplt.title('Percentage Distribution of Penguins on Each Island')\n\n# Display the pie chart\nplt.show()\n\nText(0.5, 1.0, 'Percentage Distribution of Penguin on Each Island')\n\n\n\n\n\n\n\n\n\nBased on the pie chart, the Palmer Penguins data set indicates that the majority of penguins inhabit Biscoe Island, comprising 48.95%, followed by Dream Island at 36.94%, and finally, Torgersen Island at 14.11%.\n\nimport seaborn as sns\n\n# Set style for seaborn\nsns.set(style=\"whitegrid\")\n\n# Create a figure with three subplots in the same line\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n\n# Plot 1: Island = Biscoe\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Biscoe'], ax=axes[0])\naxes[0].set_title(\"Biscoe Island\")\n\n# Plot 2: Island = Dream\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Dream'], ax=axes[1])\naxes[1].set_title(\"Dream Island\")\n\n# Plot 3: Island = Torgersen\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Torgersen'], ax=axes[2])\naxes[2].set_title(\"Torgersen Island\")\n\n# Set common y-label\naxes[0].set_ylabel(\"Count\")\n\nplt.suptitle(\"Distribution of Penguin Species on Different Islands\", y=1.05)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nBased on the bar plot illustrating the distribution of each species on each island, Gentoo penguins are the predominant species on Biscoe Island, Chinstrap penguins dominate Dream Island, and Adelie penguins are the major species on Torgersen Island. However, it’s worth noting that Adelie penguins inhabit all three islands, with Dream Island having the highest count of Adelie, even though Torgersen Island exclusively has Adelie penguins.\n\n\nDive deep into the Culmen Length and Culmen Depth relationships\nPlotly Express is a Python library that provides an easy-to-use interface for creating interactive visualizations. It is built on top of Plotly, a JavaScript library for creating interactive plots. Plotly Express offers a wide range of chart types and customization options.\n\npx.scatter(): This function creates a scatter plot using Plotly Express. We specify the DataFrame penguins and map the columns “Culmen Length (mm)” to the x-axis and “Culmen Depth (mm)” to the y-axis.\ncolor=\"Species\": This parameter colors the points based on the species of the penguins.\nhover_name=\"Species\": This parameter determines what appears when hovering over each point. Here, we display the species name.\nhover_data=[\"Island\", \"Sex\"]: This parameter adds additional information to be displayed when hovering over each point. We include the island and sex of the penguins.\nsize=\"Body Mass (g)\": This parameter sizes the points based on the body mass of the penguins.\nsize_max=8: This parameter sets the maximum size of the points.\nwidth=500, height=300: These parameters set the width and height of the plot.\nopacity=0.5: This parameter sets the opacity of the points to 0.5, making them slightly transparent.\n\n\nimport plotly\n\nfrom plotly import express as px\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 size = \"Body Mass (g)\",\n                 size_max = 8,\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5\n                )\n\n#reduce whitespace\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\n# show the plot\nfig.show()\n\n\n\n\nWe now observe a general pattern where Adelie penguins tend to exhibit shorter culmen length but longer culmen depth compared to the other two species. Chinstrap penguins typically fall in the middle for both culmen length and culmen depth when compared to the other two species. On the other hand, Gentoo penguins tend to have greater culmen length and shorter culmen depth than the other two species. In summary, across all species, culmen length tends to be greater than culmen depth.\n\n\nNow, let’s explore the factors that may contribute to the variation in culmen depth and culmen length among different species.\nFirst let’s do some data preprocessing:\nThe line of code penguins_encoded = pd.get_dummies(penguins, columns=[\"Species\", \"Island\", \"Sex\"], drop_first=True) is used to convert categorical values into numerical values through a process called one-hot encoding.\nExplanation:\n\nCategorical Variables: Categorical variables are variables that can take on a limited, and usually fixed, number of possible values. In the DataFrame penguins, the columns “Species”, “Island”, and “Sex” are categorical variables because they represent categories rather than numerical quantities.\nOne-Hot Encoding: One-hot encoding is a technique used to convert categorical variables into a numerical format that can be provided as input to machine learning algorithms. In this encoding scheme, each category is represented as a binary vector, where each element of the vector corresponds to one category. The element representing the category of a particular observation is set to 1, while all other elements are set to 0.\npd.get_dummies() Function: The pd.get_dummies() function provided by the Pandas library is used to perform one-hot encoding. It creates a new DataFrame where each categorical variable specified in the columns parameter is converted into one-hot encoded columns.\ndrop_first=True: The drop_first parameter is set to True to avoid multicollinearity in the dataset. When drop_first=True, the first category for each variable is dropped after encoding, leaving only n-1 binary columns for n categories. This prevents redundancy in the data, as the dropped category can be inferred from the other binary columns. It also helps to avoid issues such as the “dummy variable trap” in regression analysis.\nOutput: After executing this line of code, penguins_encoded will contain the original columns from penguins, along with new binary columns representing the one-hot encoded categorical variables. Each binary column corresponds to a unique category within the original categorical variable. The dataset is now in a format suitable for many machine learning algorithms that require numerical input.\n\n\n# convert categorical values into numerical values\npenguins_encoded = pd.get_dummies(penguins, columns=[\"Species\", \"Island\", \"Sex\"], drop_first=True)\n\nSince we want to find out each variable’s relation with the Culmen Length and Culmen Depth, the following code will add 2 extra columns for Species_Adelie and Island_Biscoe since they would not be included in the data frame due to get_dummies would neglact these 2 columns and replace it with other columns being False and False.\n\npenguins_encoded['Species_Adelie'] = ~penguins_encoded['Species_Chinstrap'] & ~penguins_encoded['Species_Gentoo']\n\n\npenguins_encoded['Island_Biscoe'] =  ~penguins_encoded['Island_Dream'] & ~penguins_encoded['Island_Torgersen']\n\n\npenguins_encoded\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSpecies_Chinstrap\nSpecies_Gentoo\nIsland_Dream\nIsland_Torgersen\nSex_MALE\nSpecies_Adelie\nIsland_Biscoe\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n36.7\n19.3\n193.0\n3450.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n5\n39.3\n20.6\n190.0\n3650.0\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\n47.2\n13.7\n214.0\n4925.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n340\n46.8\n14.3\n215.0\n4850.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n341\n50.4\n15.7\n222.0\n5750.0\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n342\n45.2\n14.8\n212.0\n5200.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n343\n49.9\n16.1\n213.0\n5400.0\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n333 rows × 11 columns\n\n\n\n\n# Compute the correlation matrix\ncorrelation_matrix = penguins_encoded.corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n\n# Set plot title\nplt.title(\"Correlation Heatmap\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nBased on the plot, it appears that flipper length has a significant effect on culmen length, with a correlation of 0.65. Following this, body mass shows a correlation of 0.59. This aligns with the earlier visualization, indicating that Adelie tends to have a shorter length with a negative coefficient of -0.84. In contrast, Gentoo tends to have a longer culmen length with a positive coefficient of 0.49. The same trend applies to Chinstrap, with a positive coefficient similar to Gentoo.\nConcerning culmen depth, Adelie tends to have a longer culmen depth with the greatest positive coefficient of 0.53. Dream Island positively affects culmen depth with a coefficient of 0.46, which makes sense as Adelie and Chinstrap occupy the island. Gentoo, on the other hand, exhibits the greatest negative effect on culmen depth, consistent with the fact that Gentoo inhabits Biscoe Island."
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Message Hub",
    "section": "",
    "text": "Building a Simple Message Bank Web Application with Flask: A Step-by-Step Tutorial"
  },
  {
    "objectID": "posts/HW3/index.html#table-of-contents",
    "href": "posts/HW3/index.html#table-of-contents",
    "title": "Message Hub",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1. Introduction\n\n\n2. Document composition\n2.1 templates\n    2.1.1 base.html\n    2.1.2 main.html\n    2.1.3 submit.html\n    2.1.4 view.html\n2.2 app.py\n2.3 static (style.css)\n\n\n1. Introduction\nWelcome to this tutorial where you’ll learn how to create a simple web application using Flask. In this blog post, I’ll guide you through the process of building a message bank web app from scratch and describe each step along the way. By the end of this tutorial, you’ll have a basic understanding of Flask fundamentals, database skills, and how to apply basic CSS to enhance the appearance of your web app.\nYou are free to build on any of the examples, as well as any other resources you are able to find.\nYou can find the code for this tutorial here: https://github.com/jamie1130/PIC-16B\nLet’s take a peek at what this website can do!\nThis is the first thing you would see when you open the page, the Main page: \nThen you can either view other’s messages by click Messages at the bottom or compose one by yourself. Let’s try to write one:\n\n\n\nimage-2.png\n\n\nMessage sent: \nAnd it will be stored in the message bank and you can view it when you want:\n\n\n\nimage-4.png\n\n\nAt the third row you can see your message!\n\n\n2. Document composition\n\nSome preparation:\nFirst we are going to deal with the webpage structure using Jinjia template and the logic behind it using Flask.\nA few things you might want to know before we proceed:\nJinja is a modern and designer-friendly templating engine for Python programming language. It is widely used in web development with frameworks like Flask and Django.\nA Jinja template is a text file containing a mixture of static text and template tags, which are special syntax blocks or expressions recognized by the Jinja engine. These template tags allow you to inject dynamic content, perform control flow operations, and execute logic directly within your HTML or other text-based documents.\nHere are some key features of Jinja templates:\n\nTemplate Inheritance: Jinja supports template inheritance, allowing you to define a base template with common layout and structure, and then extend or override specific sections in child templates.\nVariables: You can use variables within your templates to insert dynamic content. These variables are typically passed from the Python code to the template during rendering.\nControl Structures: Jinja provides control structures like loops and conditionals, allowing you to perform logic and iterate over data directly within your templates.\nFilters: Filters allow you to modify the output of variables in your templates. For example, you can apply filters to format dates, convert strings to uppercase, or perform other transformations.\nMacros: Macros are reusable blocks of code that can be defined once and called multiple times within a template. They are similar to functions in programming languages.\nComments: Jinja templates support comments, allowing you to add explanatory notes or reminders within your template files.\n\nOverall, Jinja templates provide a powerful and flexible way to generate dynamic content for web applications, making it easier to separate presentation logic from application logic.\nFlask is a lightweight and flexible web framework for Python. It is designed to make getting started with web development quick and easy, while still being powerful enough to build complex web applications.\nKey features of Flask include:\n\nMinimalistic: Flask is minimalist by design. It provides only the essential components needed for web development, allowing developers to add functionality as needed through extensions.\nEasy to Use: Flask is known for its simplicity and ease of use. Its API is intuitive and straightforward, making it a great choice for beginners and experienced developers alike.\nExtensible: Flask is highly extensible. It provides a modular architecture that allows developers to easily add third-party extensions to add additional functionality to their applications.\nWerkzeug and Jinja2 Integration: Flask is built on top of the Werkzeug WSGI toolkit and the Jinja2 template engine. Werkzeug provides low-level utilities for handling HTTP requests and responses, while Jinja2 provides a powerful and flexible templating engine for generating HTML and other text-based documents.\nDevelopment Server: Flask comes with a built-in development server, allowing developers to test their applications locally during development.\nRESTful Request Dispatching: Flask supports RESTful request dispatching out of the box, making it easy to build RESTful APIs.\nBuilt-in Development and Debugging Tools: Flask provides built-in development and debugging tools, including a built-in debugger and support for interactive debugging in the Python shell.\n\nOverall, Flask is a popular choice for web development in Python due to its simplicity, flexibility, and extensibility. It is widely used for building web applications, APIs, and microservices.\nHere you can find the tutorial of Jinja template: https://jinja.palletsprojects.com/en/3.1.x/templates/Links to an external site.\nHere you can find the tutorial of Flask: https://flask.palletsprojects.com/en/2.2.x/quickstart/Links to an external site.\nOk now you have a basic idea of what Jinja and Flask are, you need to make sure that you have installed Flask in your environment. Jinja is one of the dependencies of Flask, and Flask installs it automatically when you install Flask using pip. Flask relies on Jinja for its templating engine, so it includes Jinja as part of its package.\nIn terminal:\npip install Flask\nThen run the following:\nexport FLASK_ENV=development\nSetting FLASK_ENV to development tells Flask to run the application in development mode. In development mode, Flask enables additional features and behaviors that are helpful during the development process.\nAfter this, create a file and name whatever you want and that is the place we are going to operate and everything happened inside this file.\n\n\n2.1 templates\nCreate a file inside it and named it templates.\ntemplates directory: This directory contains HTML templates that Jinja uses to render dynamic content. Templates typically include HTML markup along with Jinja template tags and expressions for inserting dynamic content, variables, control structures, and more. When you call render_template() in app.py, Flask looks for templates in the templates directory by default.\nSince our webpage would have 3 pages and each cooresponds to a html: - main page (main.html): the page that we see first - submit page (submit.html): the page that we can compose our messages and submit it - view page (view.html): the messages in the bank\nNow we are going to see it one-by-one.\n\n2.1.1 base.html\nbase.html typically serves as the base or parent template in a Flask application’s template hierarchy. It contains the common structure, layout, and elements that are shared across multiple pages of a website or web application.\n&lt;!DOCTYPE html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\" /&gt;\n&lt;title&gt;{% block title %}{% endblock %} - Message Hub&lt;/title&gt;\n&lt;div class=\"container\"&gt;\n  &lt;div class=\"title\"&gt;{% block header %}{% endblock %}&lt;/div&gt;\n  &lt;div class=\"content\"&gt;\n    {% block content %}{% endblock %}\n  &lt;/div&gt;\n  &lt;div class=\"nav-bar\"&gt;\n    &lt;a href=\"{{ url_for('main') }}\" class=\"operation\"&gt;\n      &lt;img class=\"icon\" src=\"../static/home.svg\" /&gt;\n      &lt;span&gt;Main page&lt;/span&gt;\n    &lt;/a&gt;\n    &lt;a href=\"{{ url_for('view') }}\" class=\"operation\"&gt;\n      &lt;img class=\"icon\" src=\"../static/message.svg\" /&gt;\n      &lt;span&gt;Messages&lt;/span&gt;\n    &lt;/a&gt;\n    &lt;a href=\"{{ url_for('submit') }}\" class=\"operation\"&gt;\n      &lt;img class=\"icon\" src=\"../static/create.svg\" /&gt;\n      &lt;span&gt;Compose&lt;/span&gt;\n    &lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\nThe code block defines the basic structure of the website, which includes a title of Message Hub at the top, then there is a container that contains title, content, and a nav-bar that includes there buttons: Main page, Message, and Compose, where each button has a icon that is in the static folder, we will mention it later.\n\n&lt;!DOCTYPE html&gt;: This is a document type declaration that specifies the HTML version being used. It tells the web browser which version of HTML the document is written in.\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\" /&gt;: This line includes an external CSS stylesheet named style.css into the HTML document. The url_for() function is a Flask helper function that generates a URL for a given endpoint. Here, it generates the URL for the static file style.css located in the static directory of the Flask application.\n&lt;title&gt;{% block title %}{% endblock %} - Message Hub&lt;/title&gt;: This line sets the title of the HTML document. The {% block title %}{% endblock %} part is a Jinja template block that serves as a placeholder for child templates to provide a title specific to each page. The - Message Hub text is a static part of the title that is common across all pages.\n&lt;div class=\"container\"&gt;: This line starts a container div element with the CSS class container. It’s used to wrap the content of the page and apply styling or layout rules.\n&lt;div class=\"title\"&gt;{% block header %}{% endblock %}&lt;/div&gt;: This line defines a div element with the CSS class title. The {% block header %}{% endblock %} part is a Jinja template block that serves as a placeholder for child templates to provide a header specific to each page.\n&lt;div class=\"nav-bar\"&gt;...&lt;/div&gt;: This line defines a div element with the CSS class nav-bar, which typically contains navigation links. Inside this div, there are anchor (&lt;a&gt;) elements representing navigation links to different pages of the website. The href attributes of these anchor elements use the url_for() function to generate URLs for the corresponding endpoints in the Flask application.\n\nOverall, this base.html template provides a common layout structure for HTML pages in the Flask application, with placeholders for dynamic content such as page titles, headers, and main content. Child templates can extend this base template and fill in the content blocks with specific content relevant to each page.\n\n\n2.1.2 main.html\nThis would be the first page that you see when open the webpage. It would be like this: \nThe code looks like\n{% extends 'base.html' %}\n\n{% block title %}Overview{% endblock %}\n{% block header %}Message Hub{% endblock %}\n\n{% block content %}\n&lt;div class=\"welcome-messages\"&gt;\n  &lt;div&gt;Welcome to Message Hub!&lt;/div&gt;\n  &lt;div&gt;At here, you can either comspanose a message by yourself or view messages in the hub. Hope you will enjoy it!&lt;/div&gt;\n&lt;/div&gt;\n{% endblock %}\nLet’s break down each part:\n\n{% extends 'base.html' %}: This line specifies that the current template extends the base template base.html. It means that the content of the current template will replace the corresponding blocks in the base template.\n{% block title %}Overview{% endblock %}: This block overrides the title block defined in the base template. It sets the title of the HTML document to “Overview”. The content between {% block title %} and {% endblock %} will replace the corresponding block in the base template.\n{% block header %}Message Hub{% endblock %}: This block overrides the header block defined in the base template. It sets the header of the page to “Message Hub”. Similar to the title block, the content between {% block header %} and {% endblock %} will replace the corresponding block in the base template.\n{% block content %}...{% endblock %}: This block overrides the content block defined in the base template. It provides specific content for the main content area of the page. In this case, it contains a &lt;div&gt; element with the class welcome-messages and two paragraphs of text welcoming users to the Message Hub.\n\nBy using the extends keyword and defining blocks within the child template, you can create modular and reusable templates in Flask applications.\n\n\n2.1.3 submit.html\nThe submit page would contain 3 components: 1. A text box for submitting a message. 2. A text box for submitting the name of the user. 3. A “submit” button.\nThe page looks like this:\n\n\n\nimage.png\n\n\nThe code looks like this:\n{% extends 'base.html' %}\n\n{% block header %}\n  {% block title %}New Message{% endblock %}\n{% endblock %}\n\n{% block content %}\n  &lt;form action=\"/submit\" method=\"post\" class=\"form\"&gt;\n    &lt;div class='form-item form-item-name'&gt;\n      &lt;label for=\"name\"&gt;Your Name or Handle:&lt;/label&gt;\n      &lt;input type=\"text\" id=\"name\" name=\"name\" required&gt;\n    &lt;/div&gt;\n    &lt;div class='form-item form-item-message'&gt;\n      &lt;label for=\"message\"&gt;Your Message:&lt;/label&gt;\n      &lt;textarea id=\"message\" name=\"message\" required&gt;&lt;/textarea&gt;\n    &lt;/div&gt;\n    &lt;button type=\"submit\"&gt;Submit Message&lt;/button&gt;\n  &lt;/form&gt;\n\n  {% if thanks %}\n    &lt;div class=\"success\"&gt;\n      Message Sent\n    &lt;/div&gt;\n  {% endif %}\n\n{% endblock %}\nLet’s break down each part:\n\n{% extends 'base.html' %}: This line specifies that the current template extends the base template base.html. It means that the content of the current template will replace the corresponding blocks in the base template.\n{% block header %}...{% endblock %}: This block overrides the header block defined in the base template. Inside this block, there is another block called title. This nested block defines the title of the HTML document to be “New Message”. The content between {% block header %} and {% endblock %} will replace the corresponding block in the base template.\n{% block content %}...{% endblock %}: This block overrides the content block defined in the base template. It provides specific content for the main content area of the page. In this case, it contains an HTML form with input fields for the user’s name (or handle) and message, along with a submit button. Below the form, there is an {% if thanks %} condition, which checks if the thanks variable is set to true. If it is, a success message “Message Sent” is displayed and we are going to see this variable soon in the app.py.\n\n\n\n2.1.4 view.html\nThis one would be easier since it only contains the messages we retrived from the data base.\nThe page would looks like this: \nThe code looks like this:\n{% extends 'base.html' %}\n\n{% block header %}\n  {% block title %}Sinp of Messages{% endblock %}\n{% endblock %}\n\n{% block content %}\n&lt;div class=\"message-container\"&gt;\n  {% for tuple in message_tuples%}\n    &lt;div class=\"message\"&gt;\n      &lt;div class=\"message-owner\"&gt;{{tuple[0]}}&lt;/div&gt;\n      &lt;div class=\"message-content\"&gt;\"{{tuple[1]}}\"&lt;/div&gt;\n    &lt;/div&gt;\n  {% endfor %}\n&lt;/div&gt;\n{% endblock %}\nLet’s break down each part:\n\n{% extends 'base.html' %}: This line specifies that the current template extends the base template base.html. It means that the content of the current template will replace the corresponding blocks in the base template, just like before.\n{% block header %}...{% endblock %}: This block overrides the header block defined in the base template. Inside this block, there is another block called title. This nested block defines the title of the HTML document to be “Sinp of Messages”. The content between {% block header %} and {% endblock %} will replace the corresponding block in the base template.\n{% block content %}...{% endblock %}: This block overrides the content block defined in the base template. It provides specific content for the main content area of the page. In this case, it contains a &lt;div&gt; element with the class message-container, which serves as a container for displaying messages.\nInside the message-container div, there is a for loop that iterates over a list of message_tuples. For each tuple in the list, it generates a &lt;div&gt; element with the class message, containing two inner &lt;div&gt; elements:\n\n&lt;div class=\"message-owner\"&gt;{{tuple[0]}}&lt;/div&gt;: This div displays the owner of the message. It accesses the first element of the tuple using { tuple[0] } Jinja syntax.\n&lt;div class=\"message-content\"&gt;\"{{tuple[1]}}\"&lt;/div&gt;: This div displays the content of the message. It accesses the second element of the tuple using { tuple[1] } Jinja syntax. The content is enclosed in double quotes.\n\n\nWe will see why we get the data this way once we know how the data were stored.\n\n\n\n2.2 app.py\nThis is the main Python script where you define your Flask application. In app.py, you define routes, configure the application, and handle requests. Jinja is used in conjunction with render_template() function calls in app.py to render HTML templates dynamically.\nFirst let’s import libraries we are going to use:\nfrom flask import Flask, g, render_template, request\nfrom flask import redirect, url_for\n\nimport sqlite3\nimport pandas as pd\nThen creates a Flask application instance by:\napp = Flask(__name__)\nHere’s what it does:\n\nCreates an Application Instance: The Flask() constructor creates a new Flask application instance. This instance represents your web application and allows you to configure routes, define view functions, handle HTTP requests, and more.\n__name__ Argument: The __name__ argument is a special Python variable that represents the name of the current module. When you use __name__ as an argument in the Flask() constructor, Flask uses it to determine the root path of the application’s resources, such as templates and static files. It’s important to note that __name__ is different depending on whether the Python script is being run as the main program or imported as a module.\nAssigns the Application Instance to app: The resulting Flask application instance is assigned to the variable app, which you can then use throughout your Flask application to define routes, configure settings, and run the application.\n\n@app.route('/')\ndef main():\n    return render_template('main.html')\nIn Flask, @app.route('/') is a decorator that is used to define a route for a specific URL.\nHere’s how it works:\n\n@app.route('/'): This line decorates the following function, main(), indicating that the function should be executed when a request is made to the root URL '/'. The @app.route() decorator takes the URL pattern as its argument. In this case, '/' represents the root URL of the Flask application.\ndef main():: This is the definition of the main() function. This function is executed when a request is made to the root URL '/'.\nreturn render_template('main.html'): Inside the main() function, render_template() is called with the argument 'main.html'. This function renders the specified HTML template, main.html, and returns the resulting HTML content as the response to the client’s request.\nrender_template() is a function provided by Flask that renders HTML templates. It looks for the specified template file in the templates directory of the Flask application and processes any Jinja template tags or blocks contained within the template file.\n\nIn summary, the code @app.route('/') decorates the main() function, specifying that it should be executed when a request is made to the root URL '/'. When this route is accessed, the main() function returns the contents of the main.html template to the client’s web browser.\n@app.route(\"/submit/\", methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    else:\n        insert_message(request)\n        return render_template('submit.html', thanks = True)\nThis code is a Flask route definition for handling requests to the URL “/submit/” with both GET and POST methods.\nHere’s how it works:\n\n@app.route(\"/submit/\", methods=['POST', 'GET']): This decorator defines a route for the URL “/submit/” and specifies that it should handle both GET and POST requests. The methods argument is a list containing the HTTP methods that this route should accept.\ndef submit():: This is the definition of the submit() function, which will be executed when a request is made to the “/submit/” URL.\nif request.method == 'GET':: This conditional statement checks if the HTTP request method is GET. If it is, it means that the user is accessing the “/submit/” page via a GET request (e.g., by typing the URL into their browser’s address bar or following a link). In this case, the function returns the rendered template ‘submit.html’ using render_template(). This template likely contains a form that users can fill out to submit a message.\nelse:: If the request method is not GET, it means that it is a POST request (e.g., the user submitted the form). In this case, the function calls insert_message(request) to handle inserting the submitted message into the database. After inserting the message, it returns the rendered template ‘submit.html’ again, this time passing in an additional variable thanks = True. This variable is used to display a message on the page indicating that the message was successfully submitted.\n\nIn summary, this route handler allows users to access the “/submit/” page via both GET and POST requests. If accessed via GET, it renders the ‘submit.html’ template containing a form. If accessed via POST (i.e., when the form is submitted), it inserts the submitted message into the database and renders the ‘submit.html’ template again, this time displaying a message indicating that the message was successfully submitted.\n@app.route(\"/view/\")\ndef view(): \n    rdm_mesg = random_messages(5)\n    length = 5\n    message_tuples = []\n    for i in range(length):\n        message_tuples.append(tuple(rdm_mesg.iloc[i,:]))\n    return render_template('view.html', message_tuples = message_tuples)\nThis code defines a Flask route for handling requests to the URL “/view/”.\nHere’s how it works:\n\n@app.route(\"/view/\"): This decorator defines a route for the URL “/view/”. When a request is made to this URL, the associated function, view(), will be executed.\ndef view():: This is the definition of the view() function, which handles requests to the “/view/” URL.\nrdm_mesg = random_messages(5): This line calls a function random_messages() to retrieve a random selection of 5 messages. The exact implementation of random_messages() is not shown, but it presumably retrieves random messages from some data source.\nlength = 5: This line sets the variable length to 5, indicating the number of messages to retrieve.\nmessage_tuples = []: This line initializes an empty list called message_tuples. This list will store tuples, where each tuple represents a message.\nfor i in range(length):: This line starts a loop that iterates length times. It iterates through the range of numbers from 0 to length - 1.\nmessage_tuples.append(tuple(rdm_mesg.iloc[i,:])): Inside the loop, this line retrieves a row from the rdm_mesg DataFrame (presumably containing the random messages) using rdm_mesg.iloc[i,:], converts it to a tuple, and appends it to the message_tuples list.\nreturn render_template('view.html', message_tuples = message_tuples): Finally, this line renders the ‘view.html’ template using render_template(). It passes the message_tuples list as a variable named message_tuples to the template. This allows the template to access and display the messages retrieved in the view function.\n\nIn summary, when a request is made to the “/view/” URL, the view() function retrieves a random selection of messages, converts them to tuples, and passes them to the ‘view.html’ template for rendering. The template can then display these messages to the user.\nAs you might noticed, there are functions we need to write that appeared in front:\ndef get_message_db():\n    try:\n        # gets datbase from `g` object\n        return g.message_db\n    except:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cmd = \"\"\"\n        CREATE TABLE IF NOT EXISTS messages(\n        handle TEXT,\n        message TEXT\n        );\n        \"\"\"\n        cursor.execute(cmd)\n        cursor.close()\n        return g.message_db\nThis Python function get_message_db() serves the purpose of managing the database connection for the Flask application.\nHere’s how it works:\n\nTry-Except Block: The function first attempts to retrieve the database connection from the g attribute of the Flask application context. The g object is a special object provided by Flask for storing global variables during the lifetime of a request. If the database connection is already present in g, it is returned immediately.\nIf the database connection is not found in g, the except block is executed.\nDatabase Connection: Inside the except block, a new SQLite database connection is established using sqlite3.connect(). The database file is specified as \"messages_db.sqlite\". This creates a new SQLite database file if it doesn’t already exist.\nTable Creation: After establishing the database connection, the function checks whether a table named “messages” exists in the database. If the table doesn’t exist, it creates it using the SQL command specified in the cmd variable. This command creates a table with two columns: handle and message, both of type TEXT.\nCursor Operation: The function creates a cursor using g.message_db.cursor() to execute SQL commands. It then executes the CREATE TABLE command to create the “messages” table if it doesn’t exist.\nClosing Cursor: After executing the SQL command, the cursor is closed using cursor.close() to free up resources.\nReturning Database Connection: Finally, the function returns the database connection stored in g.message_db. This ensures that subsequent calls to get_message_db() within the same Flask request context reuse the same database connection.\n\nIn summary, get_message_db() ensures that a database connection is established, a “messages” table is created if it doesn’t exist, and returns the database connection for use in other parts of the Flask application.\ndef insert_message(request):\n\n    # Extract the message and the handle from request\n    handle = request.form['name']\n    message = request.form['message']\n\n    # Connect to the message database\n    db = get_message_db()\n\n    # Insert message into the messages table\n    cursor = db.cursor()\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    \n    # Commit changes to the database\n    db.commit()\n    \n    # Close the database connection\n    db.close()\n    \n    # Return the message and the handle\n    return message, handle\nThis Python function insert_message(request) is responsible for handling the insertion of a user message into the database of messages.\nHere’s a breakdown of how it works:\n\nParameter Explanation: The docstring also explains that the function takes a request object as a parameter. This request object contains form data submitted by the user.\nExtracting Message and Handle: The function extracts the message and handle from the request object using request.form['name'] and request.form['message'], respectively. These values correspond to the form fields named “name” and “message” submitted by the user.\nDatabase Connection: The function connects to the message database using the get_message_db() function. This function ensures that a database connection is established and returns the connection.\nInserting Message: The function creates a cursor using db.cursor() to execute SQL commands. It then executes an SQL INSERT statement to insert the message and handle into the “messages” table of the database. The VALUES (?, ?) part of the statement indicates that the values are provided as parameters, which helps prevent SQL injection attacks.\nCommitting Changes: After inserting the message into the database, the function commits the changes using db.commit(). This ensures that the changes are saved permanently in the database.\nClosing Database Connection: Finally, the function closes the database connection using db.close(). Closing the connection frees up resources and prevents potential issues with concurrent access to the database.\nReturning Message and Handle: The function returns a tuple containing the message and handle. This allows the caller to access these values if needed.\n\nIn summary, insert_message(request) handles the insertion of user messages into the database by extracting the message and handle from the request, inserting them into the database, committing the changes, closing the database connection, and returning the message and handle.\ndef random_messages(n):\n    db = get_message_db()\n    cmd = f\"\"\" SELECT * FROM messages ORDER BY RANDOM() LIMIT {n}; \"\"\"\n    rdm_mesg = pd.read_sql_query(cmd, db)\n    db.close()\n    return rdm_mesg\nThis Python function random_messages(n) retrieves a random selection of messages from the database of messages.\nHere’s a breakdown of how it works:\n\nDatabase Connection: The function first connects to the message database using the get_message_db() function. This function ensures that a database connection is established and returns the connection.\nSQL Command: The function constructs an SQL command to select a random sample of messages from the “messages” table. The ORDER BY RANDOM() clause ensures that the rows are returned in random order, and the LIMIT {n} clause limits the number of rows returned to n. The f-string syntax is used to include the value of n in the SQL command.\nExecuting SQL Command: The function executes the SQL command using pd.read_sql_query(), a function provided by the pandas library. This function executes the SQL query against the database connection (db) and returns the results as a pandas DataFrame (rdm_mesg).\nClosing Database Connection: After retrieving the random messages, the function closes the database connection using db.close(). Closing the connection frees up resources and prevents potential issues with concurrent access to the database.\nReturning Random Messages: Finally, the function returns the pandas DataFrame rdm_mesg, which contains the randomly selected messages.\n\nIn summary, random_messages(n) retrieves a random selection of messages from the database by executing an SQL query, reading the results into a pandas DataFrame, and returning the DataFrame containing the random messages.\n\n\n2.3 static (style.css)\nstatic directory: This directory contains static assets such as CSS, JavaScript, images, and other files that are served directly to clients without modification. While Jinja itself doesn’t directly interact with the static directory, Flask applications often use Jinja to generate URLs for static assets using the url_for() function. This allows you to reference static assets in your HTML templates in a way that’s flexible and consistent with your application’s routing.\nhtml {\n  font-family: sans-serif;\n  background: #fff;\n}\n\nbody {\n  max-width: 400px;\n  margin: auto;\n}\n\n.container {\n  width: 368px;\n  height: 780px;\n  padding: 39px 16px;\n  position: relative;\n  background-image: url(./phone_background.png);\n  background-size: cover;\n  /* background-repeat: no-repeat; */\n  /* background-size: 400px 800pwx; */\n}\n\n.title {\n  width: 368px;\n  height: 54px;\n  position: absolute;\n  left: 16px;\n  top: 36px;\n  font-size: 18px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  color: #fff;\n  background-color: #444444;\n}\n\n.content {\n  padding: 20px;\n  margin: 54px 0;\n  height: 580px;\n  overflow-y: scroll;\n}\n\n.nav-bar {\n  position: absolute;\n  left: 16px;\n  bottom: 90px;\n  width: 368px;\n  height: 54px;\n  background-color: #444444;\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n}\n\n.operation {\n  width: calc(100% / 3);\n  height: 100%;\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  justify-content: center;\n}\n\n.operation:hover {\n  background-color: #333;\n  transition: all 0.3;\n}\n\n.icon {\n  color: #fff;\n  width: 24px;\n  height: 24px;\n}\n\na {\n  color: #fff;\n  text-decoration: none;\n}\n\n.welcome-messages {\n  display: flex;\n  flex-direction: column;\n  gap: 18px;\n  font-size: 16px;\n}\n\n.welcome-messages &gt; div:first-child {\n  font-size: 24px;\n}\n\n.welcome-messages &gt; div:last-child {\n  line-height: 1.4;\n}\n\n.form {\n  display: flex;\n  flex-direction: column;\n  gap: 24px;\n}\n.form-item {\n  display: flex;\n  flex-direction: column;\n  gap: 12px;\n}\n\ninput {\n  outline: none;\n  height: 24px;\n}\n\ntextarea {\n  outline: none;\n  height: 120px;\n  resize: none;\n}\n\nbutton {\n  height: 24px;\n}\n\n.success {\n  margin-top: 22px;\n  color: green;\n}\n\n.message-container {\n  display: flex;\n  flex-direction: column;\n  gap: 9px;\n}\n\n.message {\n  display: flex;\n  flex-direction: column;\n  gap: 9px;\n  padding-bottom: 18px;\n  border-bottom: #eeeeee 1px solid;\n}\n.message:hover {\n  border-bottom: #555555 1px solid;\n}\n\n.message-owner {\n  font-size: 24px;\n  color: #444444;\n}\nThis is a CSS stylesheet defining various styles for different elements of a web page. Here’s a breakdown of the styles defined:\n\nhtml: Sets the default font family to sans-serif and sets the background color to white.\nbody: Sets the maximum width of the body to 400px and centers it horizontally using auto margins.\n.container: Styles a container element with a fixed width and height, positioned relative to its containing block. It sets a background image and ensures it covers the entire container.\n.title: Styles a title element with a fixed width and height, positioned absolutely within its containing block. It centers the text vertically and horizontally, sets the font size, and applies colors.\n.content: Styles a content area with padding, margin, height, and overflow properties. It ensures that content exceeding the height is scrollable.\n.nav-bar: Styles a navigation bar with a fixed position at the bottom of its containing block. It sets the background color and arranges its child elements horizontally with space-between alignment.\n.operation: Styles individual navigation bar items, including width, height, and alignment properties. It changes the background color on hover.\n.icon: Styles icons within navigation bar items, setting the color, width, and height.\na: Styles anchor links, setting the color and removing the underline.\n.welcome-messages: Styles a section containing welcome messages, arranging its child elements vertically with a gap between them.\n.form: Styles a form element, arranging its child elements vertically with a gap between them.\n.form-item: Styles form items, arranging their child elements vertically with a gap between them.\ninput and textarea: Styles form input and textarea elements, removing their outline and setting their height and other properties.\nbutton: Styles buttons, setting their height.\n.success: Styles a success message, setting its margin and color.\n.message-container: Styles a container for messages, arranging its child elements vertically with a gap between them.\n.message: Styles individual message elements, arranging their child elements vertically with a gap between them and adding a border.\n.message:hover: Styles individual message elements on hover, changing the border color.\n.message-owner: Styles message owner elements, setting the font size and color.\n\nOverall, these styles define the appearance and layout of various elements on a web page, providing consistency and visual appeal.\nAlright, we’ve completed all the tasks! Now you can get creative with your own message bank."
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Movie or TV Show Recommendation",
    "section": "",
    "text": "Unleashing the Power of Scrapy: Scraping and Recommending Movies and TV Shows from TMDB\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/HW2/index.html#intro",
    "href": "posts/HW2/index.html#intro",
    "title": "Movie or TV Show Recommendation",
    "section": "Intro",
    "text": "Intro\nWelcome to the world of web scraping and data-driven recommendations! In today’s digital era, where information is abundant and readily accessible, leveraging the power of data extraction tools like Scrapy opens up a realm of possibilities. In this tutorial, we’ll embark on a journey to scrape data from TMDB (The Movie Database), a treasure trove of information about movies and TV shows. But we’re not stopping there; we’ll dive deeper into the web of interconnected data, exploring the crew members’ profiles and unraveling the threads of their acting history.\nUsing Scrapy, a powerful and versatile web crawling framework in Python, we’ll navigate through TMDB’s vast database with ease. Our mission? To gather detailed insights into specific movie and TV show pages, extract information about the cast and crew, and then traverse the web of actor profiles to unearth their acting repertoire. By analyzing the overlaps in their performances, we’ll construct a recommendation engine that suggests similar movies or TV shows based on shared talent.\nWhether you’re a data enthusiast, a budding web developer, or simply curious about the magic behind personalized recommendations, this tutorial will equip you with the tools and knowledge to embark on your scraping and recommendation journey. So, let’s roll up our sleeves, fire up our code editors, and dive headfirst into the fascinating world of Scrapy and TMDB scraping!"
  },
  {
    "objectID": "posts/HW2/index.html#table-of-contents",
    "href": "posts/HW2/index.html#table-of-contents",
    "title": "Movie or TV Show Recommendation",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1. Web Scraper\n1.1 Setup\n    1.1.1 Setting up a Scrapy Project\n    1.1.2 Test Scrapy for TMDB Scraping\n1.2 Implementation\n\n\n2. Visualization and Recommendation"
  },
  {
    "objectID": "posts/HW2/index.html#web-scraper-1",
    "href": "posts/HW2/index.html#web-scraper-1",
    "title": "Movie or TV Show Recommendation",
    "section": "1. Web Scraper",
    "text": "1. Web Scraper\n\n1.1 Setup\n\n1.1.1 Setting up a Scrapy Project\nIn this section, we’ll walk through the process of setting up a Scrapy project to scrape data from TMDB (The Movie Database). By following these steps, you’ll be able to create a structured project environment ready for web scraping.\n\nStep 1: Activate Your Conda Environment\nOpen a terminal or command prompt and activate your Conda environment where you want to set up your Scrapy project. Type the following command:\nconda activate PIC16B-24W\nThis command activates the specified Conda environment named “PIC16B-24W”. Ensure that you have created and configured this environment beforehand.(You can choose whatever environment you have)\n\n\nStep 2: Create a Scrapy Project\nNow, we’ll create a Scrapy project named “TMDB_scraper”. In your terminal, type the following command:\nscrapy startproject TMDB_scraper\nThis command initializes a new Scrapy project named “TMDB_scraper”. Scrapy will generate several files and directories within this project. When you create a Scrapy project using the scrapy startproject command with the name “TMDB_scraper”, the generated project directory (TMDB_scraper) typically contains the following files and directories:\n\nscrapy.cfg: This file is the configuration file for your Scrapy project. It contains settings for Scrapy deployment and other global configurations.\nTMDB_scraper/ (directory inside TMDB_scraper/): This is the Python package directory containing your project’s code.\n\nitems.py: This file defines the data structure (items) that your spider will extract during scraping. You define the fields you want to extract from the website here.\nmiddlewares.py: This file contains the middlewares used by Scrapy, such as user-agent rotation, proxy usage, etc.\npipelines.py: This file contains the pipelines used by Scrapy for processing scraped items. Pipelines are used to perform tasks like cleaning, validation, and storing data.\nsettings.py: This file contains Scrapy project settings, such as user-agent, download delay, etc. You can customize these settings according to your requirements.\nspiders/ (directory): This directory contains your spider scripts.\n\n__init__.py: This file makes the spiders directory a Python package.\n\n\n\nThese are the essential files and directories that are created when you initialize a Scrapy project. Depending on your specific requirements and configurations, your project might have additional files or directories.\n\n\nStep 3: Navigate to Your Project Directory\nNavigate to the newly created project directory “TMDB_scraper” using the cd command:\ncd TMDB_scraper\nThis command changes your current directory to the “TMDB_scraper” directory, where your Scrapy project files are located. From here, you’ll be able to configure and customize your scraping scripts as needed.\nCongratulations! You’ve successfully set up a Scrapy project named “TMDB_scraper” ready for web scraping.\n\n\n\n1.1.2 Test Scrapy for TMDB Scraping\nWe’ll start by configuring settings in the settings.py file to prevent excessive data downloads and avoid being blocked by the website. Then, we’ll explore using the Scrapy shell for interactive testing and debugging.\n\nStep 1: Modify settings.py to Limit Page Count\nOpen the settings.py file located in your Scrapy project directory (TMDB_scraper) using a text editor. Add the following line to limit the number of pages to be scraped:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading too much data during testing. Remember to remove this line later for full-scale scraping.\n\n\nStep 2: Configure User-Agent to Mimic a Web Browser\nTo avoid getting blocked by TMDB, we’ll mimic a web browser’s behavior by modifying the USER_AGENT in the settings.py file. Replace the existing USER_AGENT line with the following:\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'\nThis user-agent string resembles a typical web browser’s user-agent, which makes it less likely for TMDB to block our requests.\nOr you can use other methods:\nhttps://doc.scrapy.org/en/latest/topics/practices.html#avoiding-getting-banned\nhttps://scrapeops.io/python-scrapy-playbook/scrapy-403-unhandled-forbidden-error/\nhttps://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/\nhttps://scrapingrobot.com/blog/most-common-user-agents/\n\n\nStep 3: Test Scraping with Scrapy Shell\nNow, let’s test our scraping capability using Scrapy shell. Open a terminal or command prompt and navigate to your Scrapy project directory (TMDB_scraper). Then, run the following command:\nscrapy shell https://www.themoviedb.org/movie/157336-interstellar/cast\nReplace the URL with the desired TMDB page to scrape. Here I used Interstellar. This command launches the Scrapy shell with the specified user-agent and the provided URL.\nStay tuned for the next part of this tutorial series where we’ll explore writing spider scripts and extracting data from TMDB using Scrapy.\n\n\n\n\n1.2 Website Navigation\nBefore writing the scraper method, we need to have a basic idea of what the website looks like and what information we need to scrap from the website.\nStart by visiting the website you want to scrape. In our case, it’s TMDB (The Movie Database). Browse through different pages, such as movie pages and the Full Cast & Crew page, to get familiar with the layout and structure.\n\nExplore the Website\nStart by visiting the TMDB website and navigating to the movie page of interest. For our example, let’s consider the movie “Interstellar”. You can find its page at:\nInterstellar Movie Page\nThe url is: https://www.themoviedb.org/movie/157336-interstellar\nThis is the url that we want to start with in the __init__, where we define the start url, that can be modified with whatever movie or tv show names after the slash movie.\n\n\nIdentify Relevant Pages\nOnce on the movie page, explore the different sections and links available. In our case, we’re interested in extracting information about the movie’s cast. We’ll navigate to the Full Cast & Crew page by appending “/cast” to the movie page URL:\nInterstellar Cast Page\nThe url is: https://www.themoviedb.org/movie/157336-interstellar/cast\nNote that there is a ‘cast’ was added at the end of the initial url and that is what we are going to do in the parse method.\nFor the other 2 method parse_full_credits and parse_actor_page we need to dig deeper to the page construction. Here I am just going to introduce the overall structer of the TmdbSpider class.\n\n\nPlan Scraping Strategy\nNow that we’ve identified the relevant pages, we can plan our scraping strategy. We’ll create a Scrapy spider with the following methods:\n\nparse: Navigates to the Full Cast & Crew page and calls parse_full_credits.\nparse_full_credits: Extracts information about the cast members and navigates to each actor’s personal page.\nparse_actor_page: Extracts the actor’s acting history from their personal page.\n\nUnderstanding the structure of the website you’re scraping is essential for developing an effective scraping method. By exploring different pages, identifying relevant information, and planning your scraping strategy, you’ll be better prepared to write the scraper method and extract the desired data.\nOnce we’ve outlined the blueprint of our class, the next step is bringing our plan to life. In web scraping, this primarily involves inspecting and extracting data. Once we understand the data structure, our task is to pinpoint its location within the HTML and then select the most appropriate method for extraction.\n\n\nInspect Elements\nMost modern web browsers offer developer tools that allow you to inspect the HTML and CSS of web pages. Right-click on any element of interest (e.g., movie title, actor name) and select “Inspect” or “Inspect Element” from the context menu.\n\n\n\nimage.png\n\n\nYou can see it at the bottom!\n\n\nIdentify Data to Scrape\nOnce you’re in the developer tools, explore the HTML structure to identify the elements containing the data you want to scrape. Look for unique identifiers such as class names, IDs, or tag names that can help you locate the desired information. I will guide you with the details later, don’t worry.\n\n\nPlan Scraping Strategy\nBased on the information you want to extract and its location in the HTML structure, plan your scraping strategy. Determine which Scrapy selectors (e.g., CSS selectors, XPath expressions) you’ll use to target and extract the data.\nYou might want to get familar with these before we proceed:\nRequest objects: https://docs.scrapy.org/en/latest/topics/request-response.html#request-objectsLinks\nResponse objects: https://docs.scrapy.org/en/latest/topics/request-response.html#response-objectsLinks\nSelector objects: https://docs.scrapy.org/en/latest/topics/selectors.html#scrapy.selector.Selector\n\n\n\n1.3 Implementation\nIn this section, we’ll create a Scrapy spider named tmdb_spider.py to scrape data from TMDB (The Movie Database) for a specific movie. We’ll set it up to accept the movie’s subdirectory as a command-line argument for easy customization.\n\nStep 1: Create tmdb_spider.py\nInside the spiders directory of your Scrapy project (TMDB_scraper), create a new Python file named tmdb_spider.py. You can do this manually or by running the following command in your terminal:\ntouch TMDB_scraper/spiders/tmdb_spider.py\nOpen this file in a text editor and add the following lines:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThe __init__ method in the spider class initializes the spider with the provided subdirectory argument. This allows us to customize the URL to scrape based on the movie’s subdirectory.\nTo run the spider and scrape data for a specific movie, use the following command:\nscrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nReplace 671-harry-potter-and-the-philosopher-s-stone with the subdirectory of the movie you want to scrape. This command instructs Scrapy to crawl the specified URL and save the scraped data to a CSV file named movies.csv.\nYou’ve successfully created a Scrapy spider named tmdb_spider.py configured to scrape data from TMDB for a specific movie. By providing the movie’s subdirectory as a command-line argument, you can easily customize the spider to scrape data for different movies.\nHere’s a basic outline of the TmdbSpider class:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        # Navigate to the Full Cast & Crew page\n        pass\n\n    def parse_full_credits(self, response):\n        # Extract information about the cast members\n        # Navigate to each actor's personal page\n        # Call parse_actor_page for each actor\n        pass\n\n    def parse_actor_page(self, response):\n        # Extract the actor's acting history from their personal page\n        pass\n\n\n1.2.1 parse(self, response)\nIn the parse method: - We construct the URL for the Full Cast & Crew page by appending “cast” to the movie page’s URL. - We then yield a scrapy.Request object with the URL of the Full Cast & Crew page and specify parse_full_credits as the callback method.\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        \"\"\"\n        Parse method to navigate to the Full Cast & Crew page and call parse_full_credits.\n\n        Args:\n            response (scrapy.http.Response): The response object containing the web page data.\n\n        Yields:\n            scrapy.Request: A scrapy request object to navigate to the Full Cast & Crew page.\n\n        \"\"\"\n        # Navigate to the Full Cast & Crew page by adding \"cast\" after the start URL\n        full_credits_url = response.url + \"cast\"\n        \n        # After navigation, call parse_full_credits\n        yield scrapy.Request(full_credits_url, callback=self.parse_full_credits)\nLet’s break down each part of the provided code snippet:\n\ndef parse(self, response): This is a method definition in a Scrapy spider class. It’s the default method called by Scrapy to handle responses downloaded for each request made.\nfull_credits_url = response.url + \"cast\": This line of code constructs the URL for the Full Cast & Crew page. It takes the current URL of the response object and appends “cast” to it, which is the relative URL for the Full Cast & Crew page.\nyield scrapy.Request(full_credits_url, callback=self.parse_full_credits): This line of code yields a new Scrapy Request object. Scrapy uses Request and Response objects for crawling web sites.Typically, Request objects are generated in the spiders and pass across the system until they reach the Downloader, which executes the request and returns a Response object which travels back to the spider that issued the request. This initiates a new request to the full_credits_url URL and specifies self.parse_full_credits as the callback function to handle the response.\n\nIn summary, this parse method is responsible for constructing a request to navigate to the Full Cast & Crew page and then calling the parse_full_credits method to handle the response from that page.\n\n\n1.2.2 parse_full_credits(self, response)\nThe overall code looks like this:\ndef parse_full_credits(self, response):\n    \"\"\"\n    Parse method to extract actor URLs from the Full Cast & Crew page and yield requests to parse actor pages.\n\n    Args:\n        response (scrapy.http.Response): The response object containing the Full Cast & Crew page data.\n\n    Yields:\n        scrapy.Request: A scrapy request object to parse each actor's page.\n\n    \"\"\"\n    cast_list = response.css('h3:contains(\"Cast\") + ol li div.info')\n    for actor in cast_list:\n        actor_url = actor.css(\"a::attr(href)\").get()\n        actor_page_url = response.urljoin(actor_url)\n        yield scrapy.Request(actor_page_url, callback=self.parse_actor_page)\n\ndef parse_full_credits(self, response): This is a method definition within a Scrapy spider class. It’s intended to handle the response from the Full Cast & Crew page.\ncast_list = response.css('h3:contains(\"Cast\") + ol li div.info'): This line of code uses CSS selectors to locate the elements containing information about each actor in the cast. It selects the &lt;div&gt; elements with the class “info” that are within list items (&lt;li&gt;) following an &lt;ol&gt; element immediately after an &lt;h3&gt; element containing the text “Cast” since we only want the Cast section instead of Crew. For how to use selector, make sure go through this: https://docs.scrapy.org/en/latest/topics/selectors.html\n\n\n\n\nimage.png\n\n\nNow we have all the casts information, we need to extract each person’s page url by iteration.\n\nfor actor in cast_list:: This loop iterates over each element in the cast_list.\nactor_url = actor.css(\"a::attr(href)\").get(): Within each iteration of the loop, this line of code extracts the URL of the actor’s personal page. It selects the value of the “href” attribute of the &lt;a&gt; element within the current actor’s &lt;div&gt; as you see in the screenshot above.\nactor_page_url = response.urljoin(actor_url): This line constructs the absolute URL for the actor’s personal page by joining the relative URL extracted in the previous step with the base URL of the current response.\nyield scrapy.Request(actor_page_url, callback=self.parse_actor_page): Finally, this line yields a new Scrapy Request object for each actor’s personal page. It specifies self.parse_actor_page as the callback function to handle the response from each actor’s page.\n\nIn summary, this parse_full_credits method is responsible for extracting the URLs of each actor’s personal page from the Full Cast & Crew page and yielding requests to scrape data from those individual pages using the parse_actor_page method.\n\n\n1.2.3 parse_actor_page(self, response)\nIn this tutorial, we’ll cover how to scrape data from an actor’s personal page, focusing on extracting their past or current movies or TV shows.\nThe overall code looks like this:\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parse method to extract acting credits for an actor from their personal page.\n\n    Args:\n        response (scrapy.http.Response): The response object containing the actor's personal page data.\n\n    Yields:\n        dict: A dictionary containing the actor's name and the movie or TV show they acted in.\n\n    \"\"\"\n    # Yield a dictionary for each movie or TV show the actor has worked in an \"Acting\" role\n    actor_name = response.css('div.title h2.title a::text').get()\n    acting_table = response.css('h3:contains(\"Acting\") + table.card.credits')\n    name_list = acting_table.css(\"a.tooltip\")\n\n    for name in name_list:\n        movie_or_TV_name = name.css(\"bdi::text\").get() \n        yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\n\n\nimage.png\n\n\n\nactor_name = response.css('div.title h2.title a::text').get(): This line extracts the actor’s name from the title of their personal page, which is under &lt;div class='title'&gt;, then under &lt;h2 class='title'&gt; then inside the &lt;a&gt;``&lt;/a&gt; here we can get the name of the actor and store it in the actor_name, which also can be done in pervious method, feel free to try it out!\n\n\n\n\nimage.png\n\n\n\nacting_table = response.css('h3:contains(\"Acting\") + table.card.credits'): This line selects the table containing the actor’s acting credits. It looks for the heading “Acting” and then selects the adjacent table with the class “card” and “credits”. Note that there are several tables under the &lt;div class='credits_list'&gt; make sure you only capture the table with Acting as the header.\n\n\n\n\nimage.png\n\n\n\nname_list = acting_table.css(\"a.tooltip\"): This line selects all the name of a movie or TV show in which the actor has acted.\nfor name in name_list:: This loop iterates over each link in the name_list since we only want the text part of the list.\n\n\n\n\nimage.png\n\n\n\nmovie_or_TV_name = name.css(\"bdi::text\").get(): Within each iteration of the loop, this line extracts the text representing the name of the movie or TV show from the link.\nyield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}: Finally, this line yields a dictionary containing the actor’s name and the name of the movie or TV show they acted in.\n\nFinally, we are done with the scraping part. We’ve learned how to extract an actor’s acting roles from their personal page using Scrapy. By understanding the structure of the actor’s page and using appropriate CSS selectors, we were able to extract the desired information effectively.\nNow try to run this in terminal to implement the scraper:\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=157336-interstellar\nFeel free to replace any Movie or TV show you like!\nThis would give you a results.csv scraped. Before we proceed, remember to remove the restriction in the settings.py."
  },
  {
    "objectID": "posts/HW2/index.html#visualization-and-recommendation-1",
    "href": "posts/HW2/index.html#visualization-and-recommendation-1",
    "title": "Movie or TV Show Recommendation",
    "section": "2. Visualization and Recommendation",
    "text": "2. Visualization and Recommendation\n\n# import libraries\nimport pandas as pd\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom plotly import express as px\n\nLet’s import this file into our notebook to see what we got!\n\ndf = pd.read_csv(\"results.csv\")\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2063 entries, 0 to 2062\nData columns (total 2 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   actor             2063 non-null   object\n 1   movie_or_TV_name  2063 non-null   object\ndtypes: object(2)\nmemory usage: 32.4+ KB\n\n\nWe’ve got 2063 movies and TV shows!\n\ndf.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nWilliam Patrick Brown\nInterstellar\n\n\n1\nWilliam Patrick Brown\nMighty Med\n\n\n2\nWilliam Patrick Brown\nFriends with Benefits\n\n\n3\nWilliam Patrick Brown\nChuck\n\n\n4\nMatthew McConaughey\n2024\n\n\n\n\n\n\n\nLet’s determine the overlaps between these movies and TV shows to form the basis of our recommendation. The logic is straightforward: we’ll analyze the movies or TV shows that feature the actors from your favorite movie or show in the highest quantity.\n\nrecmd_movies_or_TV = pd.DataFrame(df.movie_or_TV_name.value_counts().reset_index())\n\ndf.movie_or_TV_name.value_counts() we extract the movie_or_TV_name column from the dataframe and get unique values of this column with its count through .value_counts(), which would give us a Series. In order to make use of this Series, we reset the index through .reset_index() and transfer it into another data frame via pd.DataFrame().\n\n# original column name are movie_or_TV_name and count, we change them to understand it easier into movie names and number of shared actors\nrecmd_movies_or_TV = recmd_movies_or_TV.rename(columns={'movie_or_TV_name':'movie names', 'count':'number of shared actors'})\n\n\nrecmd_movies_or_TV['number of shared actors'].value_counts()\n\nnumber of shared actors\n1     1602\n2      107\n3       18\n4        9\n7        6\n6        6\n9        2\n8        2\n35       1\n10       1\nName: count, dtype: int64\n\n\nWhat we observe here is that 1602 rows of the data correspond to a single actor. This indicates that these actors are the only ones in the Interstellar crew who have performed in another movie or show, without any other members of the Interstellar crew. The same interpretation applies to the subsequent results, and the output is sorted based on the count.\nNext, we may want to split the data to visualize it in two parts, as we are not interested in the solo performances for those 1602 rows, or in movies or TV shows that contain only 2, 3, or 4 members of the full crew. You can define the splitting criteria according to your own preferences; here, I am simply dividing it into recmd_more and recmd_less based on the presence of 4 or more shared actors.\n\nrecmd_more = recmd_movies_or_TV[recmd_movies_or_TV['number of shared actors'] &gt;= 4 ]\n\n\nrecmd_less = recmd_movies_or_TV[recmd_movies_or_TV['number of shared actors'] &lt; 4 ]\n\nWe create a horizontal bar plot using Plotly Express. We specify the DataFrame recmd_more as the data source, set the ‘number of shared actors’ as the x-axis, ‘movie names’ as the y-axis, and provide additional parameters for orientation and text.\nimport plotly.express as px\n\nfig = px.bar(recmd_more, x='number of shared actors', y='movie names', orientation='h',\n             text='number of shared actors', title='Movie Names and Number of Shared Actors &gt;= 5')\nWe can adjust the position of the text annotations on the bars using the update_traces method.\nfig.update_traces(textposition='outside')\nNext, we customize the layout of the plot by specifying axis titles, ordering categories, adjusting tick marks, and setting the height of the plot.\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='total ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=600\n)\nBy following these steps, you can create a customized horizontal bar plot using Plotly Express in Python. Experiment with different parameters and settings to achieve the desired visualization for your data.\nFeel free to further customize the plot according to your specific requirements, such as adjusting colors, fonts, or adding annotations.\nHere is the full code:\n\nfig = px.bar(recmd_more, x='number of shared actors', y='movie names', orientation='h',\n             text='number of shared actors', title='Movie/Show Names and Number of Shared Actors &gt;= 5')\n\nfig.update_traces(textposition='outside')\n\n# Customizing the layout\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='total ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=500\n)\n\n                                                \n\n\nAs we can see from the plot, Interstellar would of course be the top one we would recommend the it appears to be Saturday Night Live, which shares 10 actors. The View, Late Night with Seth Meyers share 9 actors with Interstellar. Well this would be a tool to do the recommand that is super obvious for the reader or the viewer.\n\nplt.figure(figsize=(8, 4))\nsns.histplot(recmd_less['number of shared actors'], bins=range(1, 6), kde=False, color='skyblue')\nplt.xlabel('Number of Shared Actors')\nplt.ylabel('Number of Movies/TV Shows')\nplt.title('Distribution of Movies/TV Shows with Less Than 5 Shared Actors')\nplt.show()\n\n\n\n\n\n\n\n\nSince we cannot plot the thousands rows, we can visualize it through histplot though this provides not that much of the information we need for recommendation.\nNow we are going to see that for each actor, the percentage of sole performance on the overall acting histroy, sole performance here means not with any other actors in the Interstellar crew.\nBefore we begin, let’s define the required columns in our DataFrame:\n\nisDup: A boolean column indicating whether the movie or TV show is unique. If True, it means there is a duplicate movie or TV show in the DataFrame, indicating it’s not a sole performance.\ndup_cnt: The count of occurrences where isDup is True or False, grouped by actor and isDup.\ntotal_cnt: The total count of all acting roles for each actor.\n\nCalculate isDup Column\nWe’ll calculate the isDup column using the duplicated method, which marks duplicate entries as True and unique entries as False.\ndf['isDup'] = df.duplicated(subset=['movie_or_TV_name'], keep=False)\nCalculate dup_cnt and total_cnt Columns\nNext, we’ll calculate the dup_cnt and total_cnt columns using the groupby method along with the transform function to get the count of occurrences for each actor.\ndf['dup_cnt'] = df.groupby([\"actor\", 'isDup'])['movie_or_TV_name'].transform('count')\ndf['total_cnt'] = df.groupby([\"actor\"])['movie_or_TV_name'].transform('count')\n\ndf['isDup'] = df.duplicated(subset=['movie_or_TV_name'], keep=False)\n\ndf['dup_cnt'] = df.groupby([\"actor\", 'isDup'])['movie_or_TV_name'].transform('count')\n\ndf['total_cnt'] = df.groupby([\"actor\"])['movie_or_TV_name'].transform('count')\n\n\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nisDup\ndup_cnt\ntotal_cnt\n\n\n\n\n0\nWilliam Patrick Brown\nInterstellar\nTrue\n2\n4\n\n\n1\nWilliam Patrick Brown\nMighty Med\nFalse\n2\n4\n\n\n2\nWilliam Patrick Brown\nFriends with Benefits\nTrue\n2\n4\n\n\n3\nWilliam Patrick Brown\nChuck\nFalse\n2\n4\n\n\n4\nMatthew McConaughey\n2024\nFalse\n91\n127\n\n\n...\n...\n...\n...\n...\n...\n\n\n2058\nJohn Lithgow\nDealing: Or the Berkeley-to-Boston Forty-Brick...\nFalse\n151\n189\n\n\n2059\nJohn Lithgow\nGreat Performances\nFalse\n151\n189\n\n\n2060\nJohn Lithgow\nTony Awards\nTrue\n38\n189\n\n\n2061\nJohn Lithgow\nToday\nTrue\n38\n189\n\n\n2062\nJohn Lithgow\nHallmark Hall of Fame\nTrue\n38\n189\n\n\n\n\n2063 rows × 5 columns\n\n\n\nNow we contruct the plot: Let’s break down what each line of code does:\npx.bar(df, x='total_cnt', y='actor', orientation='h', color='isDup', text='total_cnt', title='Sole/Shared Performance Count for Each Actor'): - This line creates a horizontal bar plot using Plotly Express (px.bar). - The DataFrame df is used as the data source. - The x-axis represents the total count of performances (total_cnt). - The y-axis represents the actors (actor). - The bars are oriented horizontally (orientation='h'). - The color of the bars is determined by the isDup column. - The text annotations on the bars display the total count of performances. - The title of the plot is set to ‘Sole/Shared Performance Count for Each Actor’.\nfig.update_traces(textposition='outside'): - This line updates the position of the text annotations on the bars to be outside the bars. - By default, the text annotations are placed inside the bars, but setting textposition='outside' moves them outside for better visibility.\nyaxis_categoryorder='max ascending': This line specifies the order of categories (actors) on the y-axis based on the maximum value of their associated data points. Categories with higher maximum values will appear towards the top of the plot, while categories with lower maximum values will appear towards the bottom. You can alter the values to experiment.\nyaxis=dict(tickmode='linear'): This line ensures that the tick marks on the y-axis are displayed in a linear fashion. This is the default behavior, so it’s not necessary to explicitly specify it unless you want to override any previous settings.\n\nfig = px.bar(df, x='total_cnt', y='actor', orientation='h', color='isDup',\n             text='total_cnt', title='Sole/Shared Performance Count for Each Actor')\n\nfig.update_traces(textposition='outside')\n\n# Customizing the layout\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='max ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=600\n)\n\n\n\n\nBy hovering the plot, we can see that Michael Caine, John Lithgow, and Matt Damon have the greatest number of shared movies or TV shows with other crews in the Interstellar even though Michael Caine, John Lithgow, and Ellen Burstyn have the greatest number of actings. Therefore, we could not only recommand by the number of overlap among movies or shows, but also by the ‘popularity’ of the actor."
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "Accelerate The Heat Diffusion!",
    "section": "",
    "text": "Conduct a simulation of two-dimensional heat diffusion in various ways\n# Set the default Plotly renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\""
  },
  {
    "objectID": "posts/HW4/index.html#table-of-contents",
    "href": "posts/HW4/index.html#table-of-contents",
    "title": "Accelerate The Heat Diffusion!",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1. Introduction\n\n\n2. Initial conditions\n\n\n3. Methods\n3.1 Matrix Multiplication\n3.2 Sparse matrix in JAX\n3.3 Direct operation with `numpy`\n3.4 Direction operation with `jax`"
  },
  {
    "objectID": "posts/HW4/index.html#introduction-1",
    "href": "posts/HW4/index.html#introduction-1",
    "title": "Accelerate The Heat Diffusion!",
    "section": "1. Introduction",
    "text": "1. Introduction\nTwo-dimensional heat diffusion refers to the process by which heat spreads across a surface or within a two-dimensional region over time. It’s governed by the heat equation, which describes how the temperature at any point in the region changes over time due to the flow of heat.\nThe heat equation in two dimensions can be expressed as:\n\\[\n\\frac{\\partial u}{\\partial t} = \\alpha \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right)\n\\]\nWhere: - \\(u\\) is the temperature distribution across the two-dimensional region, which typically depends on the spatial coordinates \\((x, y)\\) and time \\(t\\). - \\(\\alpha\\) is the thermal diffusivity, which represents how readily heat spreads through the material. - \\(\\frac{{\\partial u}}{{\\partial t}}\\) represents the rate of change of temperature with respect to time. - \\(\\frac{{\\partial^2 u}}{{\\partial x^2}}\\) and \\(\\frac{{\\partial^2 u}}{{\\partial y^2}}\\) represent the second spatial derivatives of temperature with respect to the \\(x\\) and \\(y\\) coordinates, respectively. They describe how temperature changes spatially along the \\(x\\) and \\(y\\) directions.\nThe heat equation essentially states that the change in temperature at any point is proportional to the rate of change of temperature over time and the spatial curvature of the temperature distribution.\nTo simulate two-dimensional heat diffusion numerically, we discretize the region into a grid of points. At each point on the grid, we calculate the change in temperature over time based on the temperature differences with neighboring points, following the heat equation. This can be done using finite difference methods or other numerical techniques.\n\n\n\nimage.png\n\n\nThe simulation proceeds in discrete time steps, with the temperature at each point being updated based on the temperatures of neighboring points and the thermal diffusivity. Over time, the heat spreads from hotter regions to cooler regions, gradually evening out the temperature distribution until it reaches equilibrium.\nVisualizing the simulation results often involves plotting heatmaps or contour plots, showing how the temperature evolves across the two-dimensional region over time.\nLet’s get start it!"
  },
  {
    "objectID": "posts/HW4/index.html#initial-conditions-1",
    "href": "posts/HW4/index.html#initial-conditions-1",
    "title": "Accelerate The Heat Diffusion!",
    "section": "2. Initial conditions",
    "text": "2. Initial conditions\n\n# import libraries (you should be familiar with these 2!)\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nFor this tutorial, we will use:\n\n# set the grid to have 101 rows and 101 columns.\nN = 101\n\n# stability constant or the time step size used in numerical simulations we use is 0.2\nepsilon = 0.2\n\n# construct initial condition: 1 unit of heat at midpoint\n\n# initialize a two-dimensional NumPy array `u0` of size `N` by `N`, filled with zeros. This array represents the initial temperature distribution across the grid.\nu0 = np.zeros((N, N))\n\n# sets the value at the center of the grid to 1.0, representing one unit of heat. \n# The expression `int(N/2)` calculates the index of the center of the grid in both dimensions, and the value 1.0 is assigned to that cell\nu0[int(N/2), int(N/2)] = 1.0\n\n# This function displays the array as an image, where each element of the array corresponds to a pixel in the image, and the value of the element determines the color of the pixel. \n# Since `u0` is a two-dimensional array, `imshow()` will display a heatmap where lighter colors represent higher temperatures and darker colors represent lower temperatures.\nplt.imshow(u0)"
  },
  {
    "objectID": "posts/HW4/index.html#methods-1",
    "href": "posts/HW4/index.html#methods-1",
    "title": "Accelerate The Heat Diffusion!",
    "section": "3. Methods",
    "text": "3. Methods\n\n3.1 Matrix Multiplication\n\n# you might have seen this in the climate change tutorial before\nimport inspect\nfrom heat_equation import advance_time_matvecmul\nfrom heat_equation import get_A\n\n# Print the source code of the 'advance_time_matvecmul' function\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = int(np.sqrt(A.shape[0]))\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nadvance_time_matvecmul advances a simulation of the heat equation by one timestep using matrix-vector multiplication.\nLet’s break down the function and its components:\n\nadvance_time_matvecmul takes three parameters:\n\nA: The 2D finite difference matrix representing the discretized differential operator of the heat equation. It has dimensions \\(N^2\\) times \\(N^2\\).\nu: The current state of the temperature grid at timestep k. It’s a 2D array with dimensions N times N.\nepsilon: The stability constant or time step size used in the simulation.\n\nN = int(np.sqrt(A.shape[0])): calculates the size of the grid based on the shape of the finite difference matrix A. Since A is square with dimensions \\(N^2\\) times \\(N^2\\), the square root of the number of rows (or columns) gives us N, the size of the grid in one dimension.\nu = u + epsilon * (A @ u.flatten()).reshape((N, N)): advances the temperature grid u by one timestep. It does this by performing matrix-vector multiplication between the finite difference matrix A and the flattened temperature grid u. The result is then reshaped to a 2D array with dimensions N times N. This operation represents the application of the discrete heat equation to update the temperature distribution.\nreturn u: returns the updated temperature grid after advancing it by one timestep.\n\nIn summary, this function takes the current state of the temperature grid, applies the heat equation using matrix-vector multiplication, and returns the updated temperature grid for the next timestep.\n\n# Print the source code of the 'get_A' function\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"\n    Constructs the finite difference matrix A for a 2D Laplace operator.\n\n    Parameters:\n    - N: int\n        The size of the grid in one dimension.\n\n    Returns:\n    array_like\n        The finite difference matrix A representing the discretized Laplace operator.\n\n    This function constructs the finite difference matrix A for a 2D Laplace operator based on the size of the grid N.\n    The Laplace operator is discretized using a five-point stencil finite difference scheme.\n    The resulting matrix A is a square matrix of size N^2 x N^2.\n\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n\nget_A(N) generates the finite difference matrix A for the 2D discretization of the Laplace operator.\nLet’s break down the function and its components: 1. get_A that takes a single parameter N, which represents the size of the grid in one dimension.\n\nn = N * N: calculates the total number of grid points in the 2D grid by squaring the size of the grid in one dimension.\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]: initializes a list diagonals containing arrays representing the diagonals of the finite difference matrix A.\n\nThe first diagonal is set to -4 for each element (corresponding to the main diagonal).\nThe second and third diagonals are set to 1 for each element, except for the last element in each row, where the value is 0. This is because these diagonals represent the elements directly adjacent to the main diagonal, excluding the edges.\nThe fourth and fifth diagonals are also set to 1, representing the elements that are N positions away from the main diagonal, corresponding to the elements above and below the main diagonal.\n\ndiagonals[1][(N-1)::N] = 0 and diagonals[2][(N-1)::N] = 0: set the elements of the second and third diagonals to 0 at the positions corresponding to the last element in each row. This is done to ensure that the finite difference matrix A respects the boundary conditions of the grid.\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N): constructs the finite difference matrix A by summing the diagonals specified in the diagonals list.\n\nnp.diag(diagonals[0]) constructs a diagonal matrix using the main diagonal elements.\nnp.diag(diagonals[1], 1) constructs a diagonal matrix using the elements from the second diagonal shifted one position to the right.\nnp.diag(diagonals[2], -1) constructs a diagonal matrix using the elements from the third diagonal shifted one position to the left.\nnp.diag(diagonals[3], N) constructs a diagonal matrix using the elements from the fourth diagonal shifted N positions down.\nnp.diag(diagonals[4], -N) constructs a diagonal matrix using the elements from the fifth diagonal shifted N positions up.\nThe matrices are then summed together to obtain the final finite difference matrix A.\n\nreturn A: returns the constructed finite difference matrix A.\n\nIn summary, this function generates the finite difference matrix A for the 2D discretization of the Laplace operator based on the size of the grid N, taking into account the boundary conditions. This matrix is then used in advance_time_matvecmul.\n\nfrom timeit import default_timer\n\ndef heat_plot(f, u0, epsilon, A=None):\n    \"\"\"\n    Visualizes the progression of a simulation over multiple time steps.\n\n    Parameters:\n    - f: function\n        The method used to advance the simulation by one time step. \n        If A is provided, f should take three parameters (A, u, epsilon). \n        Otherwise, f should take two parameters (u, epsilon).\n    - u0: array_like\n        The initial state of the simulation.\n    - epsilon: float\n        The stability constant or time step size.\n    - A: array_like, optional\n        The finite difference matrix representing the discretized differential operator of the simulation.\n\n    Returns:\n    None\n\n    This function iterates over the specified number of time steps, advances the simulation using the provided method f,\n    and visualizes the progression of the simulation at milestone iterations. It also prints the total computation time.\n\n    \"\"\"\n    intermediate_solutions = []\n\n    start = default_timer()\n    u = u0.copy() \n\n    for i in range(1,2701):\n        if A is not None:\n            u = f(A,u,epsilon)\n        else:\n            u = f(u, epsilon)\n        if i % 300 == 0:\n            intermediate_solutions.append(u.copy())\n\n    end = default_timer()\n    computation_time = end - start\n    print(f\"Total computation time: {computation_time} seconds\")\n    \n    fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n    for i in range(3):\n        for j in range(3):\n            idx = i * 3 + j\n            axs[i, j].imshow(intermediate_solutions[idx], interpolation='nearest')\n            axs[i, j].set_title(f\"Iteration {idx * visualization_interval}\")\n        \n    plt.show() \n\nJust for convenience, I created a generic function for visualizing the progression of a simulation over multiple time steps.\nLet’s break it down: 1. heat_plot that takes four parameters: - f: A function representing the method used to advance the simulation by one time step. It can either take two parameters (u, epsilon) or three parameters (A, u, epsilon) depending on whether A is provided or not. - u0: The initial state of the simulation. - epsilon: A parameter representing the stability constant or time step size. - A: (Optional) The finite difference matrix representing the discretized differential operator of the simulation.\n\nintermediate_solutions = []: initializes an empty list to store intermediate states of the simulation for visualization purposes since we should not cont the time for generating plot so we should store it for ploting use with a frequency of 300.\nstart = default_timer(): records the start time of the computation.\nu = u0.copy(): creates a copy of the initial state u0 to ensure that the original state is not modified during the simulation.\nfor i in range(1,2701):: starts a loop iterating over the range from 1 to 2700 (inclusive), representing the number of time steps in the simulation.\nif A is not None:: checks if the finite difference matrix A is provided. If it is not None, it indicates that the function f takes three parameters (A, u, epsilon). Otherwise, it takes two parameters (u, epsilon). For first 2 method, we defined A and for last 2 we are not gonna define it.\nu = f(A,u,epsilon) or u = f(u, epsilon): Depending on whether A is provided or not, this line advances the simulation by one time step using the function f. If A is provided, f is called with three parameters (A, u, epsilon), otherwise with two parameters (u, epsilon).\nif i % 300 == 0:: checks if the current iteration is a multiple of 300. If it is, it indicates a milestone iteration for visualization, and the current state u is appended to the intermediate_solutions list.\nend = default_timer(): records the end time of the computation.\ncomputation_time = end - start: calculates the total computation time by subtracting the start time from the end time.\nprint(f\"Total computation time: {computation_time} seconds\"): prints the total computation time to the console.\nVisualization: This section creates a 3x3 grid of subplots using plt.subplots(3, 3, figsize=(12, 12)). It iterates over the intermediate_solutions list and plots each intermediate state on a subplot. Each subplot is titled with the corresponding iteration number.\n\nLet me show you the work!\n\nvisualization_interval = 300\n\n\nA = get_A(N)\n\n\nheat_plot(advance_time_matvecmul, u0, epsilon, A)\n\nTotal computation time: 20.457517958944663 seconds\n\n\n\n\n\n\n\n\n\nBeautiful plot but the running time is 20.457517958944663 seconds, which is pretty long. The reason why this took this much of time is that most of operations are wasted for computing zeros.\nLet’s use the data structure that exploits a lot of zeros in the matrix A: sparse matrix data structures. The JAX package holds an experimental sparse matrix support. We can use the batched coordinate (BCOO) format to only use O(N^2) space for the matrix, and only take O(N^2) time for each update.\n\n\n3.2 Sparse matrix in JAX\n\nJax\nJAX is a machine learning package putting autodiff, XLA (accelerated linear algebra), and just-in-time compilation together, created by Google.\nIn Python, we already have a couple of widely-used machine learning packages, PyTorch and TensorFlow. Why do we start with something else?\nShort answer: because it can be considerably faster. Over the years, when I was using another just-in-time compilation language for research, it was giving me significant amount of flexibility while maintaining its speed.\nPython is a slow language at its core. Its development is highly focused on flexibility. However, due to its flexibility, a lot of highly performant packages were built, often interfacing with other languages. What just-in-time (JIT) compilation adds to it is that you can write highly performant code without you directly dealing with the lower-level language (writing C or C++), even when those functionalities are not readily written in another package (numpy, scipy, tensorflow, or pytorch).\nFor example, a small neural network model (GoogleNet) on CIFAR10 can be trained in JAX 3x faster than in PyTorch with a similar setup. JAX enables this speedup by compiling functions and numerical programs for accelerators (GPU/TPU) just in time, finding the optimal utilization of the hardware.\nFrameworks with dynamic computation graphs like PyTorch cannot achieve the same efficiency, since they cannot anticipate the next operations before the user calls them. For example, in an Inception block of GoogleNet, multiple convolutional layers (which we will learn soon) are applied in parallel on the same input. JAX can optimize the execution of this layer by compiling the whole forward pass for the available accelerator and fusing operations where possible, reducing memory access and speeding up execution.\nIn contrast, when calling the first convolutional layer in PyTorch, the framework does not know that multiple convolutions on the same feature map will follow. It sends each operation one by one to the GPU, and can only adapt the execution after seeing the next Python calls. Hence, JAX can make more efficient use of the GPU than, for instance, PyTorch.\n\n\nThe downside\nEverything comes with a price. In order to efficiently compile programs just-in-time in JAX, the functions need to be written with certain constraints.\n\nFunctions are not allowed to have side effects.\n\n\nThey can’t affect any variable outside their namespaces – “pure functions” in functional programming nomenclature.\nCannot mutate input arrays.\nRandom number generation procedures are generally written in a way that mutates global states – JAX has its own way to write random number generation.\nEven having print() inside a function is a side effect!\n\nThis kind of functional programming approach is not something unique to JAX in data analytics; it has previously been used in places like Apache Spark, for large-scale parallel data analytics.\n\nJAX compiles functions based on the expected shapes of all arrays/tensors in the function.\n\n\nIt becomes an issue if the shapes or the control flow within the function depends on the values of arrays.\ny = x[x&gt;3]?\n\nStill, in a lot of numerical computations, it is straightforward to write functions within these constraints.\nMany other great JAX tutorials are there, including:\n\nJAX 101 with many subtutorials on individual parts of JAX\nJAX - The Sharp Bits discusses the constraints of JAX and how to overcome them\nJax for the Impatient for a quick intro to JAX with focus on deep learning\n\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nimport jax\n\nsparse module from the jax.experimental package: JAX is a library for numerical computing and automatic differentiation, and it provides experimental support for sparse matrix operations. The sparse module contains functions and data structures for working with sparse matrices in JAX, such as converting between dense and sparse representations, performing sparse matrix-vector multiplication, and solving linear systems with sparse matrices.\njnp: NumPy-compatible array manipulation library provided by JAX and aliases it as jnp. JAX’s NumPy (jnp) offers functions and data structures similar to NumPy for manipulating arrays, performing mathematical operations, and working with numerical data. However, JAX’s NumPy is designed to work seamlessly with JAX’s automatic differentiation capabilities, allowing users to compute gradients of functions involving array operations efficiently.\n\nfrom heat_equation import get_sparse_A\n\n# Print the source code of the 'get_A_sparse' function\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    Constructs a sparse representation of the finite difference matrix A for a 2D Laplace operator.\n\n    Parameters:\n    - N: int\n        The size of the grid in one dimension.\n\n    Returns:\n    sparse.BCOO\n        A sparse representation of the finite difference matrix A.\n\n    This function constructs the finite difference matrix A for a 2D Laplace operator based on the size of the grid N,\n    and converts it into a sparse representation using the BCOO format.\n    The Laplace operator is discretized using a five-point stencil finite difference scheme.\n    The resulting sparse matrix A_sp_matrix is a sparse matrix of size N^2 x N^2.\n\n    \"\"\"\n    A = get_A(N)\n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\n\n\nThe main high-level sparse object currently available in JAX is the BCOO, or batched coordinate sparse array, which offers a compressed storage format compatible with JAX transformations.\nFind more info about BCOO: https://jax.readthedocs.io/en/latest/jax.experimental.sparse.html\n\n# JIT-ed version of advance_time_matvecmul for better performance\nadvance_time_matvecmul_jit = jax.jit(advance_time_matvecmul)\n\nHere’s what it does:\n\nJIT Compilation: JAX’s JIT compilation feature allows for efficient execution of functions by compiling them into optimized machine code just before they are executed. This can lead to significant performance improvements, especially for functions that are called frequently or are computationally intensive.\nCompilation on Demand: When you create a JIT-compiled version of a function, JAX analyzes the function’s computation graph and generates optimized code tailored to the specific inputs it receives. This compilation process occurs on demand, the first time the function is called with a particular set of input shapes and data types.\nUsage of Compiled Function: After compilation, the resulting JIT-compiled function (advance_time_matvecmul_jit in this case) behaves like a regular Python function. However, it executes more efficiently due to the optimized machine code generated during compilation.\nBenefits: Using JIT compilation can lead to faster execution times, reduced memory usage, and improved performance, especially for functions involved in numerical computations or machine learning models.\n\n\nA = get_sparse_A(N)\nheat_plot(advance_time_matvecmul_jit, u0, epsilon, A)\n\nTotal computation time: 0.5534884999506176 seconds\n\n\n\n\n\n\n\n\n\n0.5534884999506176 seconds in Method 2 compares to 20.457517958944663 seconds in Method 1, the speed increase enomoursly by almost 40 times!\n\n\n\n3.3 Direct operation with numpy\nWe’ll implement a function called advance_time_numpy to advance the solution of the heat equation by one timestep using direct operations with NumPy. Here’s the implementation of the function:\n\nfrom heat_equation import advance_time_numpy\n\n# Print the source code of the 'advance_time_jax' function\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    # Compute the Laplacian using central differences\n    laplacian = (\n        np.roll(u, 1, axis=0) +  # Top\n        np.roll(u, -1, axis=0) +  # Bottom\n        np.roll(u, 1, axis=1) +   # Left\n        np.roll(u, -1, axis=1) -   # Right\n        4 * u  # Center\n    )\n\n    # Update the grid state using the heat equation\n    u = u + epsilon * laplacian\n\n    return u\n\n\n\nnp.roll is used to shift the elements of the array u along the specified axes (0 for rows and 1 for columns). By summing the shifted arrays, we effectively compute the Laplacian using central differences without the need for padding the array with zeros.\nYou can find more info here: https://numpy.org/doc/stable/reference/generated/numpy.roll.html\nThis function advances the solution of the heat equation by one timestep using NumPy vectorized operations. It computes the Laplacian of the temperature grid using central differences with np.roll, and updates the grid state using the heat equation.\n\nheat_plot(advance_time_numpy, u0, epsilon)\n\nTotal computation time: 0.14709462481550872 seconds\n\n\n\n\n\n\n\n\n\nNow we have:\n20.457517958944663 seconds in Method 1\n0.5534884999506176 seconds in Method 2\n0.14709462481550872 seconds in Method 3\nWe are getting faster and faster!\n\n\n3.4 Direction operation with jax\nNow, let’s use jax to do the similar using just-in-time compilation. This function advances the simulation of the heat equation by one timestep using JAX. It computes the Laplacian of the temperature grid using central differences, and updates the grid state according to the heat equation. The Laplacian is computed with boundary conditions handled by padding the temperature grid.\n\nfrom heat_equation import advance_time_jax\n\n# Print the source code of the 'advance_time_jax' function\nprint(inspect.getsource(advance_time_jax))\n\n@jax.jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the solution by one timestep using JAX.\n\n    Args:\n    u : array_like\n        N x N grid state at timestep k.\n    epsilon : float\n        Stability constant.\n\n    Returns:\n    array_like\n        N x N grid state at timestep k+1.\n    \"\"\"\n\n    laplacian = (\n        jnp.roll(u, 1, axis=0) +  # Top\n        jnp.roll(u, -1, axis=0) +  # Bottom\n        jnp.roll(u, 1, axis=1) +   # Left\n        jnp.roll(u, -1, axis=1) -   # Right\n        4 * u  # Center\n    )\n\n    # Update the grid state using the heat equation\n    u = u + epsilon * laplacian\n\n    return u\n\n\n\n\nheat_plot(advance_time_jax, u0, epsilon)\n\nTotal computation time: 0.051046375185251236 seconds\n\n\n\n\n\n\n\n\n\nWell, this is so much faster by using jit!\nNow we have:\n20.457517958944663 seconds in Method 1\n0.5534884999506176 seconds in Method 2\n0.14709462481550872 seconds in Method 3\n0.051046375185251236 seconds in Method 4\nCompare the implementation and performances of the four methods. Last one is the fastest and easy to write."
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "Global Warming",
    "section": "",
    "text": "Data Wrangling and Visualization"
  },
  {
    "objectID": "posts/HW1/index.html#intro",
    "href": "posts/HW1/index.html#intro",
    "title": "Global Warming",
    "section": "Intro",
    "text": "Intro\nIn this blog, we will develop interactive tools that offer an engaging exploration of the impact of global warming on temperatures worldwide. Moreover, I’ll guide you through the process, assuming you possess fundamental Python skills. The primary tools employed include Matplotlib and Seaborn, with a significant focus on Plotly for creating interactive graphics. Additionally, we’ll utilize Pandas and NumPy for efficient dataframe manipulation and SQLite3 for establishing the database.\nThe data set that we are going to use are:\n\ntemperatures(NOAA-GHCN data): temps.csv from the class\nstations: station-metadata.csv fromt the class\ncontries: https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\n\nLet’s get start it!!"
  },
  {
    "objectID": "posts/HW1/index.html#import-libraries-1",
    "href": "posts/HW1/index.html#import-libraries-1",
    "title": "Global Warming",
    "section": "1. Import Libraries",
    "text": "1. Import Libraries\n\n# Set the default Plotly renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n\nimport pandas as pd\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport sqlite3\n\nfrom sklearn.linear_model import LinearRegression\nfrom plotly import express as px\nimport calendar\n\nimport plotly.subplots as sp\nimport plotly.graph_objs as go"
  },
  {
    "objectID": "posts/HW1/index.html#create-data-base-1",
    "href": "posts/HW1/index.html#create-data-base-1",
    "title": "Global Warming",
    "section": "2. Create Data Base",
    "text": "2. Create Data Base\nThe Data Base called HW1.db would contain 3 tables: temperatures, stations, and contries.\nWe will do this by : - First connect to the data base - Then prepare the data frames before write it into the data base. - Lastly, check the table information and close the connection.\n\n# Connect to the data base HW1.db\n\nconn = sqlite3.connect(\"HW1.db\")\n\n\nTable1: temperatures\n\ntemps = pd.read_csv(\"temps.csv\")\n\n\ndef prepare_temp_df(df):\n    \"\"\"\n    Preprocesses a DataFrame containing temperature data.\n\n    Parameters:\n    - df (pd.DataFrame): Input DataFrame containing temperature information.\n\n    Returns:\n    - pd.DataFrame: Processed DataFrame with columns: 'ID', 'Year', 'Month', 'Temp'.\n      'Month' is extracted from the original DataFrame, and 'Temp' is normalized to degrees Celsius.\n\n    Steps:\n    1. Set the multi-index with keys 'ID' and 'Year'.\n    2. Stack the DataFrame to transform columns into rows.\n    3. Reset the index to create a new default integer index.\n    4. Rename columns to 'Month' and 'Temp'.\n    5. Extract the numeric month from the 'Month' column.\n    6. Normalize the 'Temp' column by dividing by 100.\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns={\"level_2\": \"Month\", 0: \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"] / 100\n    return df\n\n\ntemperature_df = prepare_temp_df(temps)\n\n\n# Write the DataFrame 'temperature_df' to an SQLite database table named 'temperatures'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\n\ntemperature_df.to_sql(\"temperatures\", conn, if_exists = \"replace\", index = False)\n\n13992662\n\n\n\n\nTable2: stations\n\nstations = pd.read_csv(\"station-metadata.csv\")\n\n\n# Write the DataFrame 'stations' to an SQLite database table named 'stations'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\n\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n\n\nTable3: countries\n\n# Read the CSV data from the URL 'countries_url' into a DataFrame 'countries'\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\n\n# Write the DataFrame 'countries' to an SQLite database table named 'countries'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\n\n279\n\n\n\n# Create a cursor object from the SQLite connection\ncursor = conn.cursor()\n\n# Execute a SQL query to retrieve the CREATE TABLE statements for all tables in the database\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\n# Fetch and print the CREATE TABLE statements for each table\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nHere we checked the names of tables, columns of tables, and the storage type of each column.\n\n# Close the connection\nconn.close()"
  },
  {
    "objectID": "posts/HW1/index.html#query-function-query_climate_database-1",
    "href": "posts/HW1/index.html#query-function-query_climate_database-1",
    "title": "Global Warming",
    "section": "3. Query Function: query_climate_database",
    "text": "3. Query Function: query_climate_database\nThis Query Function located in climate_database.py would extract the information we need from those 3 tables and manipulate it with restrictions.\nWhat this query_climate_database really do is you give it the db file you are gonna use, the country that you want to investigate, the year range of the data you want, and the specific month of temperatures you want.\n\ndb_file, the file name for the database\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\n\nfrom climate_database import query_climate_database\nimport inspect\n\n# Print the source code of the 'query_climate_database' function\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    with sqlite3.connect(db_file) as conn:\n\n        cmd = \\\n        f\"\"\"\n        SELECT S.name, S.latitude, S.longitude, C.name as Country, T.year, T.month, T.temp\n        FROM countries C\n        JOIN stations S \n        ON SUBSTR(S.id, 1, 2) = C.\"FIPS 10-4\"\n        JOIN temperatures T\n        ON S.id = T.id\n        WHERE C.name = '{country}'\n        AND (T.year &gt;= {year_begin} AND T.year &lt;= {year_end})\n        AND T.month = {month}\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    return df\n\n\n\nWe try out the function we imported with parameters country = “India”, year_begin = 1980, year_end = 2020,month = 1. What the function would give us is a data frame with 7 columns: name, latitude, longitude, Name, Year, Month, Temp, so that we would know each station’s temperature of Jan. in India from 1980 to 2020.\nThe result has a shape of 3152*7.\n\n# test case\nquery_climate_database(db_file = \"HW1.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nName\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/HW1/index.html#a-geographic-scatter-function-for-yearly-temperature-increases-1",
    "href": "posts/HW1/index.html#a-geographic-scatter-function-for-yearly-temperature-increases-1",
    "title": "Global Warming",
    "section": "4. A Geographic Scatter Function for Yearly Temperature Increases",
    "text": "4. A Geographic Scatter Function for Yearly Temperature Increases\n\nHow does the average yearly change in temperature vary within a given country?\nGiven this question, we would like to write a function that would give us the answer vividly.\nThis function temperature_coefficient_plot should accept six explicit arguments, and an undetermined number of keyword arguments.\n\ndb_file, country, year_begin, year_end, and month should be as in the previous part.\nmin_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month should be plotted; the others should be filtered out. df.transform() plus filtering is a good way to achieve this task.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\n\n# convert the numbers of month into names so that we could use in the title\nmonth_names = calendar.month_name\nmonth_number_to_name = {index: month for index, month in enumerate(month_names) if month}\n\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, db_file='HW1.db', **kwargs):\n    \"\"\"\n    Generate a scatter plot on a map showing the estimated yearly increase in temperature for weather stations in a specified country and month.\n\n    Parameters:\n    - country (str): Country name.\n    - year_begin (int): Starting year for data retrieval.\n    - year_end (int): Ending year for data retrieval.\n    - month (int): Numeric representation of the month for data retrieval(1-12).\n    - min_obs (int): Minimum number of years of observation required for a station to be included.\n    - db_file (str, optional): Database file path. Default is 'HW1.db'.\n    - **kwargs: Additional keyword arguments to be passed to Plotly Express scatter_mapbox.\n\n    Returns:\n    - plotly.graph_objs.Figure: Scatter plot on a map.\n\n    Note:\n    - Requires the 'query_climate_database' function from the 'climate_database' module.\n\n    Example:\n    ```python\n    fig = temperature_coefficient_plot('USA', 2000, 2020, 7, 10)\n    fig.show()\n    ```\n    \"\"\"\n    # load the database\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    \n    # filtering \n    df['num_years'] = df.groupby(['NAME', 'Month'])['Year'].transform('nunique')\n    df = df[df['num_years'] &gt;= min_obs].drop(columns='num_years')\n    \n    # calculate the average yearly change in temperature for each station\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"] \n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n    \n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    \n    # merge the station info with yearly temp\n    station_info = df.groupby('NAME', as_index=False).agg({'LATITUDE': 'first', 'LONGITUDE': 'first'})\n    df_merge = pd.merge(coefs, station_info, on='NAME')\n    df_merge = df_merge.rename(columns={0: 'Estimated Yearly Increase(°C)'})\n    df_merge['Estimated Yearly Increase(°C)'] = df_merge['Estimated Yearly Increase(°C)'].round(4)\n    \n    month_name = month_number_to_name[month]\n    # plot\n    fig = px.scatter_mapbox(df_merge,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color='Estimated Yearly Increase(°C)',\n                            title=f\"Estimates of Yearly Increase in Temperature in {month_name} for Stations in {country}, Years {year_begin}-{year_end}\",\n                            color_continuous_midpoint=0,\n                            **kwargs\n                           )\n\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\n# if you want to make the graph larger:\n# fig.update_layout(margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 30})\n\nfig.show()\n\n\n\n\nAs we can see from the plot Ludhiana, India had a major increase in Jan. temperature from 1980-2020 with a rate of 0.1318. Some other stations near the coast also experienced a increase in Jan. temperature from 1980-2020. The stations inside the country mainly have a decrease in Jan. temperature from 1980-2020.\n\ncolor_map = px.colors.diverging.RdBu_r # choose a colormap\n\nfig = temperature_coefficient_plot('Russia', 1980, 2010, 6, \n                                   min_obs = 12,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\n# if you want to make the graph larger:\n# fig.update_layout(margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 30})\n\nfig.show()\n\n\n\n\nFrom the plot, it’s evident that Amguema, Russia experienced a significant increase in June temperature from 1980 to 2020, with a rate of 0.286. Most cities are located in the southern regions due to warmer climates, while city distribution in Russia is more dispersed in the north due to colder temperatures. It’s notable that the right-hand side of the plot shows mostly increasing temperatures, with western cities experiencing a slight decrease, albeit minimal as indicated by the lighter color close to white. Conversely, eastern cities exhibit more pronounced changes in temperature, indicated by darker colors. We can potentially observe the effects of global warming near the North Pole, as the upper region of Russia, which is close to the Arctic, shows an overall increase in temperature."
  },
  {
    "objectID": "posts/HW1/index.html#temperature-changes-in-difference-climate-zone",
    "href": "posts/HW1/index.html#temperature-changes-in-difference-climate-zone",
    "title": "Global Warming",
    "section": "5. Temperature Changes in Difference Climate Zone",
    "text": "5. Temperature Changes in Difference Climate Zone\n\nBackground\nThere are many different ways to classify climate zones, here is one of them:\nThere are 4 major climate zones.\n\nTropical zone from 0°– 23.5°/ -23.5° - 0°(between the tropics)\nIn the regions between the equator and the tropics (equatorial region), the solar radiation reaches the ground nearly vertically at noontime during almost the entire year. Thereby, it is very warm in these regions. Through high temperatures, more water evaporates and the air is often moist. The resulting frequent and dense cloud cover reduces the effect of solar radiation on ground temperature.\n\n\nSubtropics from 23.5°– 40°/ -40° - -23.5°\nThe subtropics receive the highest radiation in summer, since the Sun’s angle at noon is almost vertical to the Earth, whilst the cloud cover is relatively thin. These regions receive less moisture (see trade winds), what increases the effect of radiation. Therefore, most of the deserts in the world are situated in this zone. In winter, the radiation in these regions decreases significantly, and it can temporarily be very cool and moist.\n\n\nTemperate zone from 40°–60°/ -60° - -40°\nIn the temperate zone, the solar radiation arrives with a smaller angle, and the average temperatures here are much cooler than in the subtropics. The seasons and daylength differ significantly in the course of a year. The climate is characterised by less frequent extremes, a more regular distribution of the precipitation over the year and a longer vegetation period - therefore the name “temperate”.\n\n\nCold zone from 60°–90°/ -90° - -60°\nThe polar areas between 60° latitude and the poles receive less heat through solar radiation, since the Sun has a very flat angle toward the ground. Because of the changes of the Earth axis angle to the Sun, the daylength varies most in this zone. In the summer, polar days occur. Vegetation is only possible during a few months per year and even then is often sparse. The conditions for life in these regions are very hard.\n\n\n\nHow has global warming impacted the four climate zones throughout the years and across different seasons?\nThis query allyear_climate_region_database() would give you Name, NAME, Latitude, Year, Month, Temp if you input the db_file, la_lower, la_upper - db_file - la_lower and la_upper, the latitude range you want to investgate\n\nfrom climate_database import allyear_climate_region_database\nimport inspect\n\n# Print the source code of the 'allyear_climate_region_database' function\nprint(inspect.getsource(allyear_climate_region_database))\n\ndef allyear_climate_region_database(db_file, la_lower, la_upper):\n    with sqlite3.connect(db_file) as conn:\n\n        cmd = \\\n        f\"\"\"\n        SELECT C.name, S.name, S.latitude, T.year, T.month, T.temp\n        FROM countries C\n        JOIN stations S \n        ON SUBSTR(S.id, 1, 2) = C.\"FIPS 10-4\"\n        JOIN temperatures T\n        ON S.id = T.id\n        WHERE (S.latitude &gt;= {la_lower} AND S.latitude &lt;= {la_upper})\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    return df\n\n\n\nThe two_season_plot(a, b,season_a, season_b) and four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d) would draw a 2*1 plot or 4*1 plot, coresponds to each season. X axis would be the Year, Y axis would be the Temperatures throughout years.\nThere would 3 lines, the red line is the Max temperature of each month throughout years, the blue line is the Min temperature of each month throughout years, and the black line is the Avg temperature of the year.\n\ndef two_season_plot(a, b, season_a, season_b):\n    \"\"\"\n    Generate a side-by-side plot comparing temperature trends for two different seasons over the years.\n\n    Parameters:\n    - a (pd.DataFrame): DataFrame for the first season's temperature data.\n    - b (pd.DataFrame): DataFrame for the second season's temperature data.\n    - season_a (str): Name or identifier for the first season.\n    - season_b (str): Name or identifier for the second season.\n\n    Returns:\n    - plotly.graph_objs.Figure: Side-by-side line plots with linear regression lines for both seasons.\n\n    Example:\n    ```python\n    fig = two_season_plot(df_season_spring, df_season_fall, 'Spring', 'Fall')\n    fig.show()\n    ```\n    \"\"\"\n    # Find the common x-axis range for both DataFrames\n    x_range = [min(min(a['Year']), min(b['Year'])), max(max(a['Year']), max(b['Year']))]\n\n    # Calculate y_range\n    y_range = [min(min(a['MinTemp']), min(b['MinTemp'])), max(max(a['MaxTemp']), max(b['MaxTemp']))]\n\n    # Create subplots with 1 row and 2 columns\n    fig = sp.make_subplots(rows=2, cols=1, subplot_titles=[f'{season_a} Temperature Over Years', f'{season_b} Temperature Over Years'], row_heights=[3, 3])\n\n    # Add line plot for DataFrame 'a' to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)')), row=1, col=1)\n\n    # Calculate linear regression for DataFrame 'a'\n    a_slope, a_intercept = np.polyfit(a['Year'], a['AvgTemp'], 1)\n    a_regression_line = a_slope * a['Year'] + a_intercept\n\n    # Add linear regression line to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a_regression_line, mode='lines', name=f'Regression A: {a_slope:.2f}x + {a_intercept:.2f}', line=dict(color='lightblue')), row=1, col=1)\n\n    # Add line plot for DataFrame 'b' to the second subplot\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['AvgTemp'], mode='lines',name='Avg Temperature', line=dict(color='black'), showlegend=False), row=2, col=1)\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['MinTemp'], mode='lines',name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)'), showlegend=False), row=2, col=1)\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['MaxTemp'], mode='lines',name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)'), showlegend=False), row=2, col=1)\n\n    # Calculate linear regression for DataFrame 'b'\n    b_slope, b_intercept = np.polyfit(b['Year'], b['AvgTemp'], 1)\n    b_regression_line = b_slope * b['Year'] + b_intercept\n\n    # Add linear regression line to the second subplot\n    fig.add_trace(go.Scatter(x=b['Year'], y=b_regression_line, mode='lines', name=f'Regression B: {b_slope:.2f}x + {b_intercept:.2f}', line=dict(color='lightcoral')), row=2, col=1)\n\n    # Set the common x-axis range for both subplots\n    fig.update_xaxes(range=x_range, row=1, col=1)\n    fig.update_xaxes(range=x_range, row=2, col=1)\n\n    # Set the common y-axis range for both subplots\n    fig.update_yaxes(range=y_range, row=1, col=1)\n    fig.update_yaxes(range=y_range, row=2, col=1)\n    \n    fig.update_xaxes(title_text='Year', row=2, col=1)\n    fig.update_yaxes(title_text='Temperature', row=1, col=1)\n    fig.update_yaxes(title_text='Temperature', row=2, col=1)\n\n    # Update layout if needed\n    fig.update_layout(showlegend=True)  # Optional: set showlegend=False if you don't want legends for each subplot\n\n    return fig\n\n\ndef four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d):\n    \"\"\"\n    Generate a four-subplot plot comparing temperature trends for four different seasons over the years.\n\n    Parameters:\n    - a, b, c, d (pd.DataFrame): DataFrames for temperature data of four different seasons.\n    - season_a, season_b, season_c, season_d (str): Names or identifiers for the four seasons.\n\n    Returns:\n    - plotly.graph_objs.Figure: Four-subplot line plots with linear regression lines for each season.\n\n    Example:\n    ```python\n    fig = four_season_plot(df_season_spring, df_season_summer, df_season_fall, df_season_winter, 'Spring', 'Summer', 'Fall', 'Winter')\n    fig.show()\n    ```\n    \"\"\"\n    # Find the common x-axis range for all DataFrames\n    x_range = [min(min(a['Year']), min(b['Year']), min(c['Year']), min(d['Year'])),\n               max(max(a['Year']), max(b['Year']), max(c['Year']), max(d['Year']))]\n\n    # Calculate y_range\n    y_range = [min(min(a['MinTemp']), min(b['MinTemp']), min(c['MinTemp']), min(d['MinTemp'])),\n               max(max(a['MaxTemp']), max(b['MaxTemp']), max(c['MaxTemp']), max(d['MaxTemp']))]\n\n    # Create subplots with 4 rows and 1 column\n    fig = sp.make_subplots(rows=4, cols=1, subplot_titles=[f'{season_a} Temperature Over Years', f'{season_b} Temperature Over Years', f'{season_c} Temperature Over Years', f'{season_d} Temperature Over Years'], vertical_spacing=0.1)\n\n    # Add line plot for DataFrame 'a' to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)')), row=1, col=1)\n\n    # Calculate linear regression for DataFrame 'a'\n    a_slope, a_intercept = np.polyfit(a['Year'], a['AvgTemp'], 1)\n    a_regression_line = a_slope * a['Year'] + a_intercept\n\n    # Add linear regression line to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a_regression_line, mode='lines', name=f'Regression A: {a_slope:.2f}x + {a_intercept:.2f}', line=dict(color='lightblue')), row=1, col=1)\n\n    i = 2\n    for data in (b, c, d):\n        # Add line plot for DataFrame 'b' to the second subplot\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black'), showlegend=False), row=i, col=1)\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)'), showlegend=False), row=i, col=1)\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)'), showlegend=False), row=i, col=1)\n\n        # Calculate linear regression for DataFrame 'b'\n        slope, intercept = np.polyfit(data['Year'], data['AvgTemp'], 1)\n        regression_line = slope * data['Year'] + intercept\n\n        # Add linear regression line to the second subplot\n        fig.add_trace(go.Scatter(x=data['Year'], y=regression_line, mode='lines', name=f\"Regression {i}: {slope:.2f}x + {intercept:.2f}\", line=dict(color='lightcoral')), row=i, col=1)\n\n        i += 1\n\n    for r in range(1, 5):\n        # Set the common x-axis range for all subplots\n        fig.update_xaxes(range=x_range, row=r, col=1)\n\n        # Set the common y-axis range for all subplots\n        fig.update_yaxes(range=y_range, row=r, col=1)\n\n        fig.update_yaxes(title_text='Temperature', row=r, col=1)\n\n    # Update layout if needed\n    fig.update_xaxes(title_text='Year', row=4, col=1)\n    fig.update_layout(showlegend=True)  # Optional: set showlegend=False if you don't want legends for each subplot\n\n    return fig\n\nThe season_plot will notify you the latitude you inputted is locate in which climate zone and the yearly temprature of that zone in different seasons.\n\ndef season_plot(la_lower, la_upper, db_file='HW1.db'):\n    \"\"\"\n    Generate temperature plots for different seasons based on latitude range.\n\n    Parameters:\n    - la_lower, la_upper (float): Latitude range for temperature data.\n    - db_file (str): Database file containing temperature data. Default is 'HW1.db'.\n\n    Returns:\n    - plotly.graph_objs.Figure: Temperature plots based on latitude range and seasons.\n\n    Example:\n    ```python\n    fig = season_plot(-40, -23.5, 'climate_data.db')\n    fig.show()\n    ```\n    \"\"\"\n    df = allyear_climate_region_database(db_file, la_lower, la_upper)\n\n    # Northern Tropical Zone (0-23.5)\n    if 0 &lt;= la_lower &lt;= 23.5 and 0 &lt;= la_upper &lt;= 23.5:\n        print('This latitude range is in the Northern Tropical Zone and it has only 2 seasons!')\n        filter1 = df[df['Month'].isin([10, 1, 2, 3])]  # Dry season\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([4, 5, 6, 7, 8, 9])]  # Wet season\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        season_a = 'Dry Season'\n        season_b = 'Wet Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Southern Tropical Zone (-23.5 to 0)\n    if -23.5 &lt;= la_lower &lt;= 0 and -23.5 &lt;= la_upper &lt;= 0:\n        print('This latitude range is in the Southern Tropical Zone and it has only 2 seasons!')\n        a = avg2\n        b = avg1\n        season_a = 'Dry Season'\n        season_b = 'Wet Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Northern Subtropical Zone (23.5-40)\n    if 23.5 &lt;= la_lower &lt;= 40 and 23.5 &lt;= la_upper &lt;= 40:\n        print('This latitude range is in the Northern Subtropical Zone and it has only 2 seasons!')\n        filter1 = df[df['Month'].isin([10, 1, 2, 3])]  # Hot summer\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([4, 5, 6, 7, 8, 9])]  # Wet season\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        season_a = 'Summer Season'\n        season_b = 'Mild Winter Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Southern Subtropical Zone (-40 to -23.5)\n    if -40 &lt;= la_lower &lt;= -23.5 and -40 &lt;= la_upper &lt;= -23.5:\n        print('This latitude range is in the Southern Subtropical Zone and it has only 2 seasons!')\n        a = avg2\n        b = avg1\n        season_a = 'Summer Season'\n        season_b = 'Mild Winter Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Northern Temperate Zone (40-60)\n    if 40 &lt;= la_lower &lt;= 60 and 40 &lt;= la_upper &lt;= 60:\n        print('This latitude range is in the Northern Temperate Zone and it has 4 seasons!')\n        filter1 = df[df['Month'].isin([3, 4, 5])]  # Spring\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([6, 7, 8])]  # Summer\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter3 = df[df['Month'].isin([9, 10, 11])]  # Autumn\n        avg3 = filter3.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg3.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter4 = df[df['Month'].isin([12, 1, 2])]  # Winter\n        avg4 = filter4.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg4.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        c = avg3\n        d = avg4\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    # Southern Temperate Zone (-60 to -40)\n    if -60 &lt;= la_lower &lt;= -40 and -60 &lt;= la_upper &lt;= -40:\n        print('This latitude range is in the Southern Temperate Zone and it has 4 seasons!')\n        a = avg3\n        b = avg4\n        c = avg1\n        d = avg2\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    # Northern Cold Zone (60-90)\n    if 60 &lt;= la_lower &lt;= 90 and 60 &lt;= la_upper &lt;= 90:\n        print('This latitude range is in the Northern Cold Zone and it has 4 seasons!')\n        filter1 = df[df['Month'].isin([3, 4, 5])]  # Spring\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([6, 7, 8])]  # Summer\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter3 = df[df['Month'].isin([9, 10, 11])]  # Autumn\n        avg3 = filter3.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg3.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter4 = df[df['Month'].isin([12, 1, 2])]  # Winter\n        avg4 = filter4.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg4.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        c = avg3\n        d = avg4\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    warning = 'Your latitude should be inside the range of each zone!'\n    return warning\n\n\n# let's try out the Northern Cold Zone\nresult = season_plot(60, 90)\n\nThis latitude range is in the Northern Cold Zone and it has 4 seasons!\n\n\n\nresult.update_layout(height=600, width=1000)\nresult.show()\n\n                                                \n\n\nHere you could do the experiment by yourself with different latitude. I plug in 60 and 90, which is in Northern Cold Zone and it has 4 seasons. The overall temperature is low compare to other places and the linear line of average temperature is nearly flat, which indicates a not much of difference among the temperatures over years. Other than that, Spring and Fall share the same distribution, Summer being the overall hottest, and Winter being the overall coldest.\n\n\nDoes the Global Warming effect differently in these 4 climate zones?\nTo answer this question, for each zone, we can plot the yearly average temperature for each zone.\n\n# Retrieving climate data for different latitude ranges to analyze specific climate zones\n\n# Tropical Zone: Latitude between -23.5 and 23.5 degrees\ntropical = allyear_climate_region_database('HW1.db', -23.5, 23.5)\n\n# Subtropical Zone: Latitude between 23.5 and 40 degrees (sub1) and -40 to -23.5 degrees (sub2)\nsub1 = allyear_climate_region_database('HW1.db', 23.5, 40)\nsub2 = allyear_climate_region_database('HW1.db', -40, -23.5)\nsub = pd.concat([sub1, sub2], ignore_index=True)\n\n# Temperate Zone: Latitude between 40 and 60 degrees (temperate1) and -60 to -40 degrees (temperate2)\ntemperate1 = allyear_climate_region_database('HW1.db', 40, 60)\ntemperate2 = allyear_climate_region_database('HW1.db', -60, -40)\ntemperate = pd.concat([temperate1, temperate2], ignore_index=True)\n\n# Cold Zone: Latitude between 60 and 90 degrees\ncold = allyear_climate_region_database('HW1.db', 60, 90)\n\n# Calculate average yearly temperature for each climate zone\ntro_avg = tropical.groupby('Year')['Temp'].mean()\nsub_avg = sub.groupby('Year')['Temp'].mean()\ntemperate_avg = temperate.groupby('Year')['Temp'].mean()\ncold_avg = cold.groupby('Year')['Temp'].mean()\n\n\nfig = go.Figure()\n\n# Add trace for tropical dataset\nfig.add_trace(go.Scatter(x=tro_avg.index, y=tro_avg.values, mode='markers', name='Tropical', line=dict(color='blue')))\n\n# Add trace for sub dataset\nfig.add_trace(go.Scatter(x=sub_avg.index, y=sub_avg.values, mode='markers', name='Subtropical', line=dict(color='green')))\n\n# Add trace for temperate dataset\nfig.add_trace(go.Scatter(x=temperate_avg.index, y=temperate_avg.values, mode='markers', name='Temperate', line=dict(color='orange')))\n\n# Add trace for cold dataset\nfig.add_trace(go.Scatter(x=cold_avg.index, y=cold_avg.values, mode='markers', name='Cold', line=dict(color='red')))\n\n# Update layout for better readability\nfig.update_layout(title='Yearly Average Temperature',\n                  xaxis_title='Year',\n                  yaxis_title='Temperature',\n                  legend=dict(title='Dataset'))\n\n# Show the plot\nfig.show()\n\n                                                \n\n\nThe overall Yearly Average Temperature for each climate zone did not change a lot throughout years but with a slight increase for each scatter line especially for the cold and tropical zone. However, from the average temperature, we could barely see the global warming effect. There are more infos that we need to investigate!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jamie’s Blog",
    "section": "",
    "text": "Accelerate The Heat Diffusion!\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nMessage Hub\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nMovie or TV Show Recommendation\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Warming\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nJan 28, 2023\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin World\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nJamie\n\n\n\n\n\n\nNo matching items"
  }
]