[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Final Project/index.html",
    "href": "posts/Final Project/index.html",
    "title": "Final Project",
    "section": "",
    "text": "https://jamie1130.github.io/PIC-16B/posts/Final Project/"
  },
  {
    "objectID": "posts/Final Project/index.html#load-data",
    "href": "posts/Final Project/index.html#load-data",
    "title": "Final Project",
    "section": "Load Data",
    "text": "Load Data\nNow let us load the data we scraped by TripAdvisor as well as a Excel file containing coordinate points of our national parks so that we can create a geograpical plot later\n\ndf = pd.read_csv('https://raw.githubusercontent.com/torwar02/trails/main/trails/national_parks.csv')\n\n\ndf2 = pd.read_excel('https://raw.githubusercontent.com/torwar02/trails/main/trails/coords.xlsx')\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf2.head()\n\n\n  \n    \n\n\n\n\n\n\nLatitude\nLongitude\nPark\nState(s)\nPark Established\nArea\nVisitors (2018)\n\n\n\n\n0\n44.35\n-68.21\nAcadia\nMaine\nFebruary 26, 1919\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\n-14.25\n-170.68\nAmerican Samoa\nAmerican Samoa\nOctober 31, 1988\n8,256.67 acres (33.4 km2)\n28626\n\n\n2\n38.68\n-109.57\nArches\nUtah\nNovember 12, 1971\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3\n43.75\n-102.50\nBadlands\nSouth Dakota\nNovember 10, 1978\n242,755.94 acres (982.4 km2)\n1008942\n\n\n4\n29.25\n-103.25\nBig Bend\nTexas\nJune 12, 1944\n801,163.21 acres (3,242.2 km2)\n440091\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nTo merge the two files together, we utilize regex. Get string preceding ‘National Park’ in df such that we can merge with df2 on National Park name\n\nimport re\npattern = r'(.*?)(?:\\s+National Park)?$'\nresult = re.findall(pattern, df['national_park'].iloc[0])\npark = []\nfor row in df['national_park']:\n  test_park = re.findall(pattern, row)\n  park.append(test_park[0])\ndf['park'] = park\nnational_parks = pd.merge(df, df2, left_on='park', right_on='Park')\nnational_parks = national_parks.drop(columns = ['park', 'Park', 'State(s)', 'Park Established'])\nnational_parks.head()\n\n\n  \n    \n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575"
  },
  {
    "objectID": "posts/Final Project/index.html#word-embedding-and-comment-similarity-score",
    "href": "posts/Final Project/index.html#word-embedding-and-comment-similarity-score",
    "title": "Final Project",
    "section": "Word Embedding and Comment Similarity Score",
    "text": "Word Embedding and Comment Similarity Score\nFirst let us go over what Word Embedding is. Word embedding in NLP is an important technique that is used for representing words for text analysis in the form of real-valued vectors. In this approach, words and documents are represented in the form of numeric vectors allowing similar words to have similar vector representations. The extracted features are fed into a machine learning model so as to work with text data and preserve the semantic and syntactic information. This information once received in its converted form is used by NLP algorithms that easily digest these learned representations and process textual information.\n\nComment Similarity Function\nNow let us create a function called comment_similarity which takes in our national_parks.csv file we just created via the park_data parameter, a comment_index parameter, and an all_comments parameter which is our word embedding vector representation of all comments in our csv file.\n\nall_docs = [nlp(row) for row in national_parks['comment_text']] #getting vector representation of all comments in our csv file\n\n\ndef comment_similarity(parks_data, comment_index, all_comments):\n  example_comment = parks_data.loc[comment_index, 'comment_text']\n  reference_comment = nlp(example_comment) #vectorize our reference sentence\n  simularity_score = []\n  row_id = []\n  for i in range(len(all_comments)):\n    sim_score = all_comments[i].similarity(reference_comment)\n    simularity_score.append(sim_score)\n    row_id.append(i)\n  simularity_docs = pd.DataFrame(list(zip(row_id, simularity_score)), columns = ['Comment_ID', 'sims'])\n  simularity_docs_sorted = simularity_docs.sort_values(by = 'sims', ascending = False)\n  most_similar_comments = simularity_docs_sorted['Comment_ID'][1:2]\n  new_reviews = national_parks.iloc[most_similar_comments.values]\n  return(new_reviews)\n\nNow let us show what our returned dataframe looks like\n\nshowcase = comment_similarity(national_parks, 0, all_docs)\nshowcase\n\n\n  \n    \n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1552\nGrand Canyon National Park\nArizona (AZ)\nGrand Canyon South Rim\nCanyons\n5.0\nThe views do not disappoint!\n5.0 of 5 bubbles\nWe were staying with family in Sun City (near ...\n36.06\n-112.14\n1,201,647.03 acres (4,862.9 km2)\n6380495\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\nAs we can see, we return a dataframe of the most similar review to the review with the index 999. To see how similar this similar review is to our inputted review let us output both comments.\nFirst the original comment\n\nexample_comment = national_parks.loc[0, 'comment_text']\nexample_comment\n\n\"I have hiked to the fire tower a few times. Its a great hike, and not too strenuous elevation gains.  If the NO rangers are up there ( in the summer) they used to allow you to go up the tower. We had to turn back on 3/20 because of hard pack solid ice. We had our Katoohla micro spikes on, and solid hiking poles, and knew they simply  wouldn't be enough if the ice was on the steeper sections.  We walked into the trailhead because the access road gate is still closed. After deciding to cross the lot and hike Beech Cliff Loop, which was much more clear of ice, and has excellent views of Echo Lake and the ocean out toward  Southwest Harbor. We returned to BH to hear of the recovery of a young couple from Rutland Massachusetts  who had fallen 100 feet to their death on Dorr Mountain Gorge Trail. The tragedy attributed to ice on the trails. Anyone not experienced with full crampon travel, and ice climbing training should never attempt to hike or climb on solid ice. The danger is severe.. \"\n\n\nNow the similar comment.\n\nshowcase['comment_text'].iloc[0]\n\n'We were staying with family in Sun City (near the Phoenix airport) and drove in our rental vehicle the approximate 3.5 hour drive to the south entrance of the Grand Canyon.  The park entrance was easy to find.  Parking this year was $35/vehicle.  I was skeptical going in, as several friends had this excursion on their \"bucket list\" while others simply raved.  I worried I would be disappointed.  However, the views absolutely spectacular!  We self-guided/toured.  We both experienced some vertigo and were careful to hang on to the railings provided, or sit on available benches as needed. Also bring water.  With the high elevation, it is easier to get winded, and water helps. We did have a hiker in front of us fall a few times from experiencing vertigo,and with assistance from others were able to help him get off the stairs and onto level ground to sit down.  He was embarrassed but grateful.  It could (and does) happen to anyone.  There were some areas that were roped off due to ice and snow and I was amazed how many people stupidly ignored the warnings and bypassed the barriers to get closer to the edge of the Canyon for selfies!   Check the weather in advance and dress appropriately.  The temperature was 30 degrees cooler in the Canyon than in the Phoenix area.  There were many families present and some pushing young ones in strollers.  On Feb 10, it was a chilly, windy, 40 degrees F.   There are lots of signs at various points educating you on the history of rocks, the Colorado river running through the Canyon, etc., and a small museum you can enter about 1.5 hours into the walk.  After our hike, we were exhausted and wind blown, and caught a shuttle back to the parking lot.  Kudos to those who can manage to walk the entire thing.  We didn\\'t see everything the south side had to offer.  In our vehicle, we exited the park from the east side and for some 50+ miles, still saw the Grand Canyon from out the driver\\'s side window. There were several spots along the way to stop and take more photos.  All in all, it was a physically and mentally stimulating journey that I highly recommend.'\n\n\nAs we can see the comments are very similar! They both talk about the dangers of the trail and how they both saw people fall.\n\n\nTotal Trail Simularity\nNow let us create a function called total_similarity which takes in the same parameters as our last function except takes in the trail name instead of comment_index. We do so because we want to get all 10 comments per trail. Our total_similarity function calls comment_similarity to get the most similar comment per each individual comment of the 10 trails. As a result, we get 10 total similar trails returned to us.\n\ndef total_similarity(trail, parks_data, all_comments):\n  trail_subset = parks_data[parks_data['trail'] == trail].index\n  total_df = []\n  for number in trail_subset:\n    total_df.append(comment_similarity(national_parks, number, all_docs))\n  df = pd.concat(total_df)\n  return(df)\n\n\noutput = total_similarity(\"Landscape Arch\", national_parks, all_docs)\noutput\n\n\n  \n    \n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n242,755.94 acres (982.4 km2)\n1008942\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n241,904.50 acres (979.0 km2)\n1227627\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n310,044.22 acres (1,254.7 km2)\n3491151\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n236,381.64 acres (956.6 km2)\n1518491\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nAs we can see we get 10 similar trails to our desired trail Landscape Arch\n\n\nPlotly Function\nNow let us construct a geographical plot function called plotting_parks to get the location of these trails on a map. This is so that the user can better visualize where in the United States they may have to travel to. The function also analyzes other metrics from national_parks.csv such as visitors in 2018, type of activity, trail name, and overall TripAdvisor rating. This function calls total_similarity in order to get the dataframe with the most similar reviews!\n\nfrom plotly import express as px\nimport plotly.io as pio\nimport inspect\npio.renderers.default=\"iframe\"\n\n\ndef plotting_parks(trail, parks_data, all_comments, **kwargs):\n  output = total_similarity(trail, parks_data, all_comments)\n  fig = px.scatter_mapbox(output, lon = \"Longitude\", lat = \"Latitude\", color = \"overall_rating\",\n                        color_continuous_midpoint = 2.5, hover_name = \"national_park\", height = 600,\n                        hover_data = [\"Visitors (2018)\", \"activity\", \"trail\", \"overall_rating\"],\n                        title = \"Recommended National Park Trails\",\n                        size_max=50,\n                        **kwargs,\n                        )\n  return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # produce a color map\nfig = plotting_parks(\"Landscape Arch\", national_parks, all_docs, mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\n\n\nfig.show()\n\n\n\n\nimage.png\n\n\nGreat, as we can see, we get a geo plot of the most similar National Park trails in the United States to Landscape Arch!"
  },
  {
    "objectID": "posts/Final Project/index.html#how-do-you-get-started-with-selenium",
    "href": "posts/Final Project/index.html#how-do-you-get-started-with-selenium",
    "title": "Final Project",
    "section": "How do you get started with Selenium?",
    "text": "How do you get started with Selenium?\nSelenium is able to evade certain anti-bot measures by actually using an instance of a web browser (called a webdriver) that runs on your system while scraping. In fact, once you get the scraper to work, you can actually watch it run in real time! Unfortunately, that makes it a lot slower than scrapy, for instance, because your computer actually has to manually open every page. I used a Google Chrome webdriver. Below is (part of) the head of the scraper.py function which scrapes data from individual trails from TrailForks.\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nimport pandas as pd\nfrom selenium import webdriver\n\n\nchrome_options = webdriver.ChromeOptions()\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\n        \"prefs\", {\n            # block image loading\n            \"profile.managed_default_content_settings.images\": 2,\n            \"profile.managed_default_content_settings.javascript\": 2\n        }\n    )\ndriver = webdriver.Chrome(\n        service=service,\n        options=options\n    )\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--headless')\nOf note are the experimental options under prefs which block both images and javascript content from loading on a website. When I first made the scraper, I did not have these enabled, and a result, sometimes pages would take between 3 and 5 seconds to load (way too long!). Similarly, the --headless argument also make the pages load faster by disabling certain Google Chrome functionalities: https://www.selenium.dev/blog/2023/headless-is-going-away/"
  },
  {
    "objectID": "posts/Final Project/index.html#how-do-you-scrape-using-selenium",
    "href": "posts/Final Project/index.html#how-do-you-scrape-using-selenium",
    "title": "Final Project",
    "section": "How do you scrape using Selenium?",
    "text": "How do you scrape using Selenium?\nAt its core, Selenium isn’t that different from scrapy in that you can have a scraper download HTML code which you can then filter through in Python as more familiar objects. Both scraper.py and scraper_parks.py (which filters through park-related information on TrailForks) have the same general principles: 1. Look at what state a user has inputted 2. Get links to all of the park/trail pages for that state 3. Get corresponding information from each page, using helper functions if need be 4. Add data to SQL database (see next section for that!)"
  },
  {
    "objectID": "posts/Final Project/index.html#names-and-coordinates",
    "href": "posts/Final Project/index.html#names-and-coordinates",
    "title": "Final Project",
    "section": "Names and coordinates",
    "text": "Names and coordinates\nHere’s the first part of where we actually scrape:\n\n for url in href_list:\n            no_name_found = False\n            info_dict = {\"Name\":[\"NA\"], \"Location\":[\"NA\"], \"Coords\":[\"NA\"]}\n            stats_dict =  {\"Trails (view details)\":[\"NA\"],\"Total Distance\":[\"NA\"], \"State Ranking\":[\"NA\"],}\n            trail_difficulty_count = {\"Access Road/Trail\":0,\"White\":0,\"Green\":0,\"Blue\":0,\"Black\":0,\"Double Black Diamond\":0, \"Proline\":0}\n            print(url)\n            driver.get(url)\n            area_name_raw = driver.find_element(\"xpath\", \"//span[contains(@class, 'translate')][1]\")\n            info_dict[\"Name\"] = area_name_raw.text\n            try:\n                city_name_raw = driver.find_element(By.CLASS_NAME, \"small.grey2.mobile_hide\")\n                info_dict[\"Location\"] = city_name_raw.text\n            except:\n                no_name_found = True\n                \nWe create our three dictionaries that we want for each URL. The print(url) function is present as a debugging tool since, unfortunately, this scraper crashed multiple times due to unfixed bugs (which I eventually patched out, mostly due to elements not being present).\nWe get the name of the trail by finding a span with class translate (not sure why it’s stored like that, it’s actually within an h1 within a ul called page_title_container). Then, we try to look for the name of the city that it’s in by grabbing a small piece of text that’s next to the park’s name. Sometimes, this isn’t present, which is why we have a bool called no_name_found in case it’s not. There’s a way around this, though, which we’ll show later…\n###Ranking, Distance, and Trail numbers:\n    stats_items = [\"State Ranking\", \"Total Distance\", \"Trails (view details)\"]\n            dict_category = driver.find_elements(\"xpath\", \"//dl//dt\")\n            dict_information = driver.find_elements(\"xpath\", \"//dl//dd\")\n           \n            for idx, terms in enumerate(dict_category):\n                if terms.text in stats_items:\n                    stats_dict[terms.text] = [dict_information[idx].text]\n                    \n            try:\n                difficulty_ul = driver.find_element(By.CLASS_NAME, 'stats.flex.nostyle.inline.clearfix')\n\n                for li in difficulty_ul.find_elements(By.TAG_NAME, 'li'):\n                    difficulty_span = li.find_element(By.XPATH, './/span[contains(@class, \"stat-label clickable\")]/span')\n                    difficulty_name = difficulty_span.get_attribute('title')\n                    if difficulty_name in trail_difficulty_count.keys():\n                        num_trails_span = li.find_element(By.CLASS_NAME, 'stat-num')\n                        num_trails = int(num_trails_span.text)\n                        trail_difficulty_count[difficulty_name] = num_trails\nThe code here is somewhat dense thanks to the fact that all of this information is stored in a dictionary-like object called a dl which, in turn, has something like a key in a dl and something like a value in a dd. Essentially, we update the ranking and trail distances by inspecting these.\nIt’s a little bit harder to get the number of trails per difficulty. Basically, there’s an unordered list with a long class name ('stats.flex.nostyle.inline.clearfix' that sorts the number of trails by difficulty. Each li has the number of trails stored within it, but it also has a graphic that represents the difficulty (it’s a small picture), and it’s the graphic that actually hides the name of the difficulty, which is why we have to extract difficulty_name from a span of class stat-label clickable. Then, we simply grab the actual text that displays how many trails of a given difficulty there are, convert it to an integer, and then add it to our dictionary.\n\nCoordinates\nOne of the unfortunate parts of the parks list is that the coordinates of each park are not present! To get around this, we tell the scraper to go to the first trail in each park and grab its coordinates (remember scraper.py?) and then store it.\n  try:\n                green_link = driver.find_element(\"xpath\",\"//tr//a[contains(@class, 'green')]\")\n                park_link = green_link.get_attribute(\"href\")\n                driver.get(park_link)\n            except:\n                pass\n                \n            try:\n                coord_raw = driver.find_element(\"xpath\", \"//div[contains(@class, 'margin-bottom-15 grey')]/span[contains(@class, 'grey2')][2]\") #Get coords\n                info_dict['Coords'] = [coord_raw.text]\n                if no_name_found:\n                    city_name_raw = driver.find_element(By.CLASS_NAME, \"weather_date bold green\")\n                    info_dict[\"Location\"] = city_name_raw.text\n            except:\n                info_dict['Coords'] = [\"NA\"]\nIt’s here where we also resolve the issue of when we can’t find a city’s name. Basically, on each trail’s page, there’s a short infobox containing weather information for the nearest city which is guaranteed to appear, so we can get an approximate location name precisely by grabbing the city name from this box.\nOnce we’re done with that, it’s off to the database again!"
  },
  {
    "objectID": "posts/Final Project/index.html#connecting-national-parks-to-individual-trailpark-info",
    "href": "posts/Final Project/index.html#connecting-national-parks-to-individual-trailpark-info",
    "title": "Final Project",
    "section": "Connecting National Parks to Individual Trail/Park Info",
    "text": "Connecting National Parks to Individual Trail/Park Info\nNow we need to make sure to connect the data that we’ve collected here with the actual table generated by the recommender to give the user more information. Let’s take a look at our output from the similarity score model:\n\noutput\n\n\n  \n    \n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n242,755.94 acres (982.4 km2)\n1008942\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n241,904.50 acres (979.0 km2)\n1227627\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n310,044.22 acres (1,254.7 km2)\n3491151\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n236,381.64 acres (956.6 km2)\n1518491\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nBecause we have two different SQL databases, one for nation-wide park data (trails_new.db) and one with state-wide trail data (trails.db), let’s split this into two different frames.\n\ncalifornia_df = output[output['state'] == 'California (CA)']\nnon_california_df = output[output['state'] != 'California (CA)']\n\nNow we’ll get our databases in our notebook:\n\n!wget https://raw.githubusercontent.com/torwar02/trails/main/trails/trails.db -O trails.db\n!wget https://raw.githubusercontent.com/torwar02/trails/main/trails/trails_new.db -O trails_new.db\n\n--2024-03-22 20:55:28--  https://raw.githubusercontent.com/torwar02/trails/main/trails/trails.db\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3534848 (3.4M) [application/octet-stream]\nSaving to: ‘trails.db’\n\ntrails.db           100%[===================&gt;]   3.37M  --.-KB/s    in 0.07s   \n\n2024-03-22 20:55:28 (48.8 MB/s) - ‘trails.db’ saved [3534848/3534848]\n\n--2024-03-22 20:55:28--  https://raw.githubusercontent.com/torwar02/trails/main/trails/trails_new.db\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1339392 (1.3M) [application/octet-stream]\nSaving to: ‘trails_new.db’\n\ntrails_new.db       100%[===================&gt;]   1.28M  --.-KB/s    in 0.06s   \n\n2024-03-22 20:55:28 (23.1 MB/s) - ‘trails_new.db’ saved [1339392/1339392]\n\n\n\nThere’s a bit of an issue, though. Let’s look at our table names:\n\nimport sqlite3\ndb_path = 'trails_new.db'\n\nconn = sqlite3.connect(db_path) #Establish connection with DB\ncur = conn.cursor()\n\ncur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\") #This specifically grabs all table names from our datbaase.\ntables = cur.fetchall()\ntable_names = [table[0] for table in tables] #Places them into a list\nprint(\"List of tables in the database:\", table_names)\nconn.close()\n\nList of tables in the database: ['Maine', 'California', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'idaho-3166', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'new-hampshire', 'new-jersey', 'new-mexico', 'new-york', 'north-carolina', 'north-dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'rhode-island', 'south-carolina', 'south-dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'west-virginia', 'Wisconsin', 'Wyoming']\n\n\nOur tables aren’t completely in alphabetical order (I was testing around with Maine first, for instance). And some of them aren’t two words, like south-dakota, for instance. But if we compare this to what we have in output:\n\nset(output['state'])\n\n{'California (CA)',\n 'Montana (MT)',\n 'South Dakota (SD)',\n 'Utah (UT)',\n 'Washington (WA)',\n 'Wyoming (WY)'}\n\n\nHere we have nice, capitalized state names with two-letter abbreviations. So, then, how are we going to fix this? We’re going to create a dictionary that essentially works as a mapping that takes what we have in output and matches it to what exists in table_names based on some matching criteria:\n\n# Extract unique states and sort them\nunique_states_in_output = sorted(set(output['state']), key=str.lower)\ntable_names = sorted(table_names, key=str.lower)\n\n\n\ndef compare_letters(state_name, table_name):\n    clean_state_name = ''.join(filter(str.isalpha, state_name)).lower() #Eliminate non-alphabetical characters, condense together\n    clean_table_name = ''.join(filter(str.isalpha, table_name)).lower()\n    return sorted(clean_state_name) == sorted(clean_table_name) #Gives a boolean value.\n\nstate_name_to_table_name = {} #Create new dictionary\nfor state_with_abbreviation in unique_states_in_output:\n    state_name = state_with_abbreviation.split(' (')[0]  # Get rid of the parentheses in the abbreviation (like 'South Dakota (SD)')\n    match = next((table for table in table_names if compare_letters(state_name, table)), None) #Generator based on whether or not names are the same\n    if match:\n        state_name_to_table_name[state_with_abbreviation] = match #Update dict if match found\n\nprint(state_name_to_table_name)\n\n{'California (CA)': 'California', 'Montana (MT)': 'Montana', 'South Dakota (SD)': 'south-dakota', 'Utah (UT)': 'Utah', 'Washington (WA)': 'Washington', 'Wyoming (WY)': 'Wyoming'}\n\n\nNow that’s what we’re looking for! We do a few important things here:\nFirstly, we make sure to get both the states that we have in output and the tables in table_names in alphabetical order. The reason why we do key=str.lower is because some of the table names are written in uppercase while others are in lowercase. This makes it case-insensitive.\nThen we create a helper function called compare_letters which takes two state names (one from output, one from the database) and compares them to see if they have the same letters. We do this by filtering out non-alphabetical characters, spaces, and making everything lowercase and just checking if they have the same letters. The function will just return True or False depending on whether or not they match.\nWe actually use state_name_to_table_name in the for loop below this. We go through each of the states in output. Then, we extract just the part of the state name that comes before the two-letter abbreviation, and then we create a generator that individualls calls compare_letters on each of the names. If it returns True, then we have a match, which then causes the dictionary to be updated. Otherwise, nothing happens and we simply move onto the next entry (that’s why the second argument of next is none).\n##Logic for linking databases\nOur goal is to now go through each recommendation, and match up either the park or trail information corresponding to it (assuming that it’s present in the database). One issue that can arise with this, however, is that the name of the park in output might be different from that of the database. To mitigate this, we’re going to instead compare the coordinates of what’s in output to the rows inside of trails_new.db and trails.db. The idea is that if two parks are close enough to each other in terms of their coordinates, then they should represent the same thing. So, we’re going to make two functions that do similar (but different) things. One will be called fetch_park_info_based_on_coords which will look at parks (i.e., outside of California), and the other will be called fetch_trail_info_based_on_coords\n\ndef fetch_park_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor() #Connect to database\n\n    for table_name in state_name_to_table_name.values(): #This is what we made earlier\n        cursor.execute(f\"SELECT * FROM \\\"{table_name}\\\"\") #Grab everything from the table\n        rows = cursor.fetchall()\n\n        for row in rows: #For each row\n            coords_text = row[2]  # Coords are in the third column\n            try:\n                coords = eval(coords_text) #Kept as a tuple, essentially\n                lat_diff = abs(coords[0] - latitude)\n                long_diff = abs(coords[1] - longitude)\n\n                if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                    return row[3:]  # Don't need name and coords\n            except:\n                continue\n\n    conn.close()\n    return None\n\ndef fetch_trail_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n    table_name = 'California'  #Only getting CA trails\n\n    cursor.execute(f\"SELECT * FROM {table_name}\") #Grab everything\n    rows = cursor.fetchall()\n\n    for row in rows:\n        coords_text = row[1]  # Coords are in column 2\n        try:\n            coords = eval(coords_text)\n            lat_diff = abs(coords[0] - latitude)\n            long_diff = abs(coords[1] - longitude)\n\n            if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                return row[2:]\n        except:\n            continue  # Skip rows with invalid 'Coords'\n\n    conn.close()\n    return None\n\nOkay, so, it will make a lot more sense if we actually inspect the structure of our database again. Click the link below to see screenshots of two .csv files: the first of parks in Wyoming, and the second is of trails in California:\nhttps://imgur.com/a/6fCixEt\nWith that out of the way, let’s dive into the code. We go through the mapping dictionary that we made previously and we grab all of the possible parks from each one. Then, we look at the third column (i.e., row[2], which represents the third entry in the row) which corresponds to the coordinates (see screenshot), and we record the absolute difference in the coordinates between a given latitude and longitude (we’ll be taking those from output–they’re individual columns rather than a tuple). If both of them are within a specified margin of error, then we’ve found our match. Note that we’re only going to return everything starting from the third column: the first 2 are just the name and coordinates of the trail.\nFor fetch_trail_info_based_on_coords, we have a very similar set-up except for the fact that the coordinates are in the second column, and we’re interested in returning everything after the first two.\nNow, let’s move on so we can see how we actually use these functions!"
  },
  {
    "objectID": "posts/Final Project/index.html#putting-it-all-together",
    "href": "posts/Final Project/index.html#putting-it-all-together",
    "title": "Final Project",
    "section": "Putting it all together",
    "text": "Putting it all together\nThe first thing we’re going to do is to specify the names of the new columns that we want to put into california_df and non_california_df. I’ve just grabbed these from the database:\n\nnew_columns = [\n    'Trails (view details)', 'Total Distance', 'State Ranking',\n    'Access Road/Trail', 'White', 'Green', 'Blue', 'Black',\n    'Double Black Diamond', 'Proline'\n]\nnew_trail_columns = [\n    'Distance', 'Avg time', 'Climb', 'Descent', 'Activities',\n    'Riding Area', 'Difficulty Rating', 'Dogs Allowed',\n    'Local Popularity', 'Altitude start', 'Altitude end', 'Grade'\n]\n\nNow, all we need to do is iterate through the rows of non_california_df to match up the entires!\n\nmargin_lat = 0.1  # Decently generous\nmargin_long = 0.1\nfor index, row in non_california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']): #Some parks have NA coordinates\n        continue\n    park_info = fetch_park_info_based_on_coords('trails_new.db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n    #Remember, this grabs almost all of the columns if a match is found\n    if park_info:\n        non_california_df.loc[index, new_columns] = park_info #We can mass-add new columns\n\nIn the above code, we use the fetch_park_info_based_on_coords function to essentially create a new data frame that contains the information that we want once we match the coordinates. Then, we insert all of these as new columns, taking advantage of the .loc() method from pandas. Now let’s do the same thing for the California df:\n\n\nfor index, row in california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']):\n        continue\n\n    park_info = fetch_trail_info_based_on_coords('trails.db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n\n    if park_info and len(park_info) == len(new_trail_columns):\n        california_df.loc[index, new_trail_columns] = park_info\n    else:\n        pass\n\nOkay, let’s take a look at our results!\n\nnon_california_df\n\n\n  \n    \n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\n...\nTrails (view details)\nTotal Distance\nState Ranking\nAccess Road/Trail\nWhite\nGreen\nBlue\nBlack\nDouble Black Diamond\nProline\n\n\n\n\n303\nBadlands National Park\nSouth Dakota (SD)\nPinnacles Overlook\nPoints of Interest & Landmarks\n5.0\nMust See Pullover\n5.0 of 5 bubbles\nThis is one of a handful of overlooks you have...\n43.75\n-102.50\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n235\nArches National Park\nUtah (UT)\nDelicate Arch\nPoints of Interest & Landmarks\n5.0\nDelicate Arch\n5.0 of 5 bubbles\nOur family chose to hike to Delicate Arch late...\n38.68\n-109.57\n...\n40\n50 miles\n#7,493\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n863\nCapitol Reef National Park\nUtah (UT)\nCapitol Reef National Park\nNational Parks\n4.5\nAdd Capitol Reef to Your Utah National Park List\n5.0 of 5 bubbles\nJust to the northeast of more popular parks Br...\n38.20\n-111.17\n...\n60\n194 miles\n#9,609\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1611\nGrand Teton National Park\nWyoming (WY)\nTaggart Lake\nHiking Trails\n5.0\nDo this hike if you want to feel like you're a...\n5.0 of 5 bubbles\nIt's not a difficult hike and is right off the...\n43.73\n-110.80\n...\n26\n53 miles\n#4,761\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n222\nArches National Park\nUtah (UT)\nDouble Arch\nHiking Trails\n5.0\nEasy hike\n5.0 of 5 bubbles\nThe Double Arch is unreal. It is massive and b...\n38.68\n-109.57\n...\n40\n50 miles\n#7,493\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3198\nMount Rainier National Park\nWashington (WA)\nSunrise Visitor Center\nVisitor Centers\n4.5\nAmazing views\n5.0 of 5 bubbles\nAmazing hikes of all varieties. Many travel up...\n46.85\n-121.75\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1439\nGlacier National Park\nMontana (MT)\nGrinnell Glacier\nHiking Trails\n5.0\nIncredible vies and the end-point is rewarding\n5.0 of 5 bubbles\nThis 13 mile hike from Many Glacier to upper G...\n48.80\n-114.00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1366\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nMagnificent Falls in Glacier National Park - w...\n5.0 of 5 bubbles\nThis is the second falls on a hike in Glacier ...\n48.80\n-114.00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n...\n25\n177 miles\n#9,011\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n9 rows × 22 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nSuccess! It looks like we unfortunately have a few NA values. Unfortunately, it’s hard to guarantee precision in the coordinates. We only had one trail for California:\n\ncalifornia_df\n\n\n  \n    \n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1310\nDeath Valley National Park\nCalifornia (CA)\nZabriskie Point\nGeologic Formations\n4.5\nThe Most Iconic Place in Death Valley\n4.0 of 5 bubbles\nYou can't miss it. I don't mean you have to do...\n36.24\n-116.82\n3,373,063.14 acres (13,650.3 km2)\n1678660\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\nWait, really? I thought we would’ve had this for sure in our database…\nOn closer inspection, we actually do, but hte coordinates on TrailForks versus what we got from the National Park data is a bit off. On the TrailForks page for Zabriskie Point, the coordinates are (36.420820, -116.810120), which is just outside the margin of error."
  },
  {
    "objectID": "posts/Final Project/index.html#introduction-of-image-matting-and-modnet",
    "href": "posts/Final Project/index.html#introduction-of-image-matting-and-modnet",
    "title": "Final Project",
    "section": "1. Introduction of Image matting and MODNet",
    "text": "1. Introduction of Image matting and MODNet\n\nMODNet - Portrait Image Matting\nBefore web development, let’s take a look at a fun model about image matting - MODNet. With image matting, we could merge our selfies with pictures of any us national parks in the file background, or you can upload your own choice.\nImage matting, also known as foreground/background separation, is a computer vision technique that aims to accurately extract the foreground object or region of interest from an image, while preserving the fine details and transparency information around the object boundaries. This process generates an alpha matte, which represents the opacity values for each pixel, allowing for seamless composition of the foreground onto a new background.\nThe MODNet (Modulator-Decoupled Network) model is a deep learning architecture specifically designed for image matting tasks. It was introduced in a research paper by Zhanghan Ke, Jingyu Zhang, Kaihao Zhang, Qiong Yan, and Kaiqi Huang in 2022. MODNet stands out from other image matting models due to its unique approach and several key features:\n\nDecoupled Modulation: MODNet decouples the modulation process of the foreground and background features, allowing the model to better capture the intricate relationships between the foreground and background regions. This decoupling helps to improve the accuracy of the alpha matte predictions, especially around complex object boundaries.\nEffective Feature Fusion: MODNet incorporates an effective feature fusion mechanism that combines multi-level features from different stages of the network. This fusion strategy helps to capture both low-level details and high-level semantic information, leading to more accurate and coherent alpha matte predictions.\nLightweight Architecture: Despite its impressive performance, MoDNet has a relatively lightweight architecture compared to other state-of-the-art image matting models. This makes it more efficient and suitable for deployment on resource-constrained devices or in real-time applications.\nImproved Generalization: MODNet demonstrates strong generalization capabilities, meaning it can produce accurate alpha mattes even for objects or scenes that are significantly different from the training data. This is a crucial advantage over many traditional image matting methods that often struggle with generalization.\n\n\n\n\nimage.png\n\n\nThe key innovation of MODNet lies in its decoupled modulation approach, which allows the model to effectively disentangle the foreground and background features, leading to superior performance in capturing intricate object boundaries and transparency information. This architectural design, combined with effective feature fusion and a lightweight structure, has made MoDNet a notable advancement in the field of image matting.\nThere are several other state-of-the-art models for image matting tasks, in addition to the MODNet architecture. Here are some notable ones:\n\nGCA Matting: Proposed in 2020, the Guided Contextual Attention (GCA) model utilizes a two-stream encoder-decoder architecture with a contextual attention module. This module helps the model better capture long-range dependencies and global context information, leading to improved performance on complex scenes.\nAlphaMatting: Introduced in 2021, AlphaMatting is a transformer-based model that leverages the self-attention mechanism to effectively capture long-range dependencies in images. It achieves impressive results, particularly in handling highly complicated backgrounds and foreground objects with intricate structures.\nSHM Matting: The Spatially-Hierarchical Matting (SHM) model, proposed in 2022, employs a hierarchical architecture that processes the input image at multiple spatial scales. This approach helps the model capture both fine-grained details and global structures, leading to improved accuracy, especially around object boundaries.\nBGMatting: Introduced in 2022, BGMatting (Background Matting) is a two-stage model that first predicts a coarse alpha matte and then refines it using a background estimation module. This unique approach helps the model better handle challenging cases with complex backgrounds or semi-transparent objects.\nHDMatt: The High-Definition Matting (HDMatt) model, introduced in 2022, is designed to produce high-resolution alpha mattes by leveraging a progressive upsampling strategy. It achieves impressive results, particularly for high-resolution images, while maintaining a relatively lightweight architecture.\n\nThese models represent some of the latest advancements in the field of image matting, each with its own unique architectural design and strengths. The choice of model often depends on factors such as the complexity of the scenes, the required level of detail, and the computational resources available.\nReference: https://github.com/ZHKKKe/MODNet\nLet’s get start it!"
  },
  {
    "objectID": "posts/Final Project/index.html#preparation",
    "href": "posts/Final Project/index.html#preparation",
    "title": "Final Project",
    "section": "2. Preparation",
    "text": "2. Preparation\n\nIn the top menu of this session, select Runtime -&gt; Change runtime type, and set Hardware Accelerator to GPU.\n\n\nClone the repository, and download the pre-trained model:\n\nFirst we import the os module, which provides functions for interacting with the operating system.\n\nimport os\n\n\n# changes the current directory to /content.\n# %cd is a Jupyter Notebook magic command used to change directories within the notebook.\n%cd /content\n\n# checks if a directory named MODNet exists in the current directory.\n# If it doesn't exist, it clones the GitHub repository located at https://github.com/ZHKKKe/MODNet into a directory named MODNet.\nif not os.path.exists('MODNet'):\n  !git clone https://github.com/ZHKKKe/MODNet\n\n# changes the current directory to the MODNet directory created or found in the previous step.\n%cd MODNet/\n\n# defines the path where the pre-trained checkpoint file will be saved or checked for\npretrained_ckpt = 'pretrained/modnet_photographic_portrait_matting.ckpt'\n\n# checks if the file specified by pretrained_ckpt exists.\n# If it doesn't exist, it proceeds with downloading the file.\nif not os.path.exists(pretrained_ckpt):\n# downloads the pre-trained checkpoint file from Google Drive using gdown.\n# The file is saved in the specified path (pretrained/modnet_photographic_portrait_matting.ckpt).\n# The --id flag specifies the ID of the file on Google Drive, and -O specifies the output filename.\n  !gdown --id 1mcr7ALciuAsHCpLnrtG_eop5-EYhbCmz \\\n          -O pretrained/modnet_photographic_portrait_matting.ckpt\n\n/content\nCloning into 'MODNet'...\nremote: Enumerating objects: 276, done.\nremote: Counting objects: 100% (276/276), done.\nremote: Compressing objects: 100% (159/159), done.\nremote: Total 276 (delta 105), reused 252 (delta 98), pack-reused 0\nReceiving objects: 100% (276/276), 60.77 MiB | 37.53 MiB/s, done.\nResolving deltas: 100% (105/105), done.\n/content/MODNet\n/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1mcr7ALciuAsHCpLnrtG_eop5-EYhbCmz\nTo: /content/MODNet/pretrained/modnet_photographic_portrait_matting.ckpt\n100% 26.3M/26.3M [00:00&lt;00:00, 64.5MB/s]\n\n\nNow ler’s try this out."
  },
  {
    "objectID": "posts/Final Project/index.html#upload-images",
    "href": "posts/Final Project/index.html#upload-images",
    "title": "Final Project",
    "section": "3. Upload Images",
    "text": "3. Upload Images\n\nUpload portrait images to be processed (only PNG and JPG format are supported):\n\nThe following code ensures a clean slate by removing and recreating both input and output folders.\nUsers can then upload images, which are automatically moved into the input folder for processing.\nshutil: This module provides a higher-level interface for file operations, such as copying files and removing directories. google.colab.files: This module provides utilities for interacting with files in a Google Colab environment, including uploading and downloading files.\n\nimport shutil\nfrom google.colab import files\n\nSets up the input folder path where the images will be stored for processing. It checks if the input folder already exists. If it exists, it removes the entire folder and its contents (shutil.rmtree). Then, it creates a new, empty input folder (os.makedirs).\n\n# clean and rebuild the image folders\ninput_folder = 'demo/image_matting/colab/input'\nif os.path.exists(input_folder):\n  shutil.rmtree(input_folder)\nos.makedirs(input_folder)\n\nSimilar to the input folder, this block sets up the output folder path for storing processed images. It checks if the output folder already exists. If it exists, it removes the entire folder and its contents. Then, it creates a new, empty output folder.\n\noutput_folder = 'demo/image_matting/colab/output'\nif os.path.exists(output_folder):\n  shutil.rmtree(output_folder)\nos.makedirs(output_folder)\n\nThis part allows the user to upload images into the Colab environment.\nfiles.upload() prompts the user to select and upload files. It returns a dictionary where the keys are the uploaded file names and the values are the data.\nlist(files.upload().keys()) extracts the names of the uploaded files. A loop iterates through each uploaded image file: shutil.move(image_name, os.path.join(input_folder, image_name)) moves each uploaded image file from the current directory to the specified input folder. This step organizes the uploaded images into the input folder for further processing.\n\n# upload images (PNG or JPG)\nimage_names = list(files.upload().keys())\nfor image_name in image_names:\n  shutil.move(image_name, os.path.join(input_folder, image_name))\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving 170891_00_2x.jpg to 170891_00_2x.jpg"
  },
  {
    "objectID": "posts/Final Project/index.html#inference",
    "href": "posts/Final Project/index.html#inference",
    "title": "Final Project",
    "section": "4. Inference",
    "text": "4. Inference\nThe following code runs a Python script/module for image matting inference, specifying the input directory containing the images to be processed, the output directory where the processed images will be saved, and the path to the pre-trained model checkpoint file.\n\nRun the following command for alpha matte prediction:\n\n\n!python -m demo.image_matting.colab.inference \\\n        --input-path demo/image_matting/colab/input \\\n        --output-path demo/image_matting/colab/output \\\n        --ckpt-path ./pretrained/modnet_photographic_portrait_matting.ckpt\n\nProcess image: 170891_00_2x.jpg\n\n\nLet’s break down what each part of the command does:\n!python - This is a shell command that tells the system to run a Python interpreter.\n-m demo.image_matting.colab.inference - -m flag is used to run a module as a script. - demo.image_matting.colab.inference specifies the Python module to run. It’s likely that this module contains the code for performing image matting inference.\n--input-path demo/image_matting/colab/input - --input-path is a command-line argument for specifying the path to the input images directory. - demo/image_matting/colab/input is the path to the directory where the input images are stored.\n--output-path demo/image_matting/colab/output - --output-path is a command-line argument for specifying the path to the output directory where the processed images will be saved. - demo/image_matting/colab/output is the path to the directory where the processed images will be saved.\n--ckpt-path ./pretrained/modnet_photographic_portrait_matting.ckpt - --ckpt-path is a command-line argument for specifying the path to the checkpoint file for the pre-trained model used in image matting. - ./pretrained/modnet_photographic_portrait_matting.ckpt is the path to the pre-trained model checkpoint file relative to the current directory."
  },
  {
    "objectID": "posts/Final Project/index.html#visualization",
    "href": "posts/Final Project/index.html#visualization",
    "title": "Final Project",
    "section": "5. Visualization",
    "text": "5. Visualization\n\nDisplay the results (from left to right: image, foreground, and alpha matte):\n\n\nimport numpy as np\nfrom PIL import Image\n\nThe following function is useful for visualizing the process of image matting, where the foreground is extracted from the original image based on the provided matte.\n\nfrom mergePicture import combined_display\nimport inspect\n\n# Print the source code of the 'query_climate_database' function\nprint(inspect.getsource(combined_display))\n\ndef combined_display(image, matte):\n    # calculate display resolution\n    w, h = image.width, image.height\n    rw, rh = 800, int(h * 800 / (3 * w))\n\n    # obtain predicted foreground\n    image = np.asarray(image)\n    if len(image.shape) == 2:\n        image = image[:, :, None]\n    if image.shape[2] == 1:\n        image = np.repeat(image, 3, axis=2)\n    elif image.shape[2] == 4:\n        image = image[:, :, 0:3]\n    matte = np.repeat(np.asarray(matte)[:, :, None], 3, axis=2) / 255\n    foreground = image * matte + np.full(image.shape, 255) * (1 - matte)\n\n    # combine image, foreground, and alpha into one line\n    combined = np.concatenate((image, foreground, matte * 255), axis=1)\n    combined = Image.fromarray(np.uint8(combined)).resize((rw, rh))\n\n    # extract the middle image\n    middle_image = Image.fromarray(np.uint8(foreground))\n\n    return combined, middle_image\ncombined_display, takes an image and its corresponding matte (alpha channel) as inputs and returns two images: one for combined display and the other for the middle image (foreground).\nHere’s what each part of the function does:\n\nCalculate Display Resolution:\n\nIt calculates the display resolution for the output image.\nw and h store the width and height of the input image, respectively.\nrw is set to 800, indicating the desired width for the output image.\nrh is calculated to maintain the aspect ratio of the input image.\n\nObtain Predicted Foreground:\n\nConvert the input image and matte to NumPy arrays (image and matte).\nCheck if the input image is grayscale or has an alpha channel. If so, convert it to a 3-channel image.\nRepeat the matte across channels and normalize it.\nCalculate the predicted foreground by applying the matte to the input image.\n\nCombine Image, Foreground, and Matte:\n\nConcatenate the input image, predicted foreground, and matte along the horizontal axis.\nConvert the combined array back to an image (Image.fromarray) and resize it to the calculated resolution.\n\nExtract Middle Image:\n\nConvert the predicted foreground array to an image (Image.fromarray) to extract the middle image.\n\nReturn Output:\n\nReturn the combined display image and the middle image.\n\n\nHere’s the explanation of the return values: - combined: The combined image showing the original image, predicted foreground, and matte (alpha channel) concatenated horizontally. - middle_image: The image representing the predicted foreground, extracted from the combined image.\n\nbg_dir = '/content/sample_data/Badlands.jpeg'\n\n# Load the background image of Badlands National Park\nbackground_image = Image.open(bg_dir)\n\nThis code segment iterates through all the images in the input folder, visualizes each image with its corresponding matte, and then displays the merged image where the middle image is composited onto a background based on the matte.\n\n# visualize all images\nimage_names = os.listdir(input_folder)\nfor image_name in image_names:\n    matte_name = image_name.split('.')[0] + '.png'\n    image = Image.open(os.path.join(input_folder, image_name))\n    matte = Image.open(os.path.join(output_folder, matte_name))\n    combined, middle_image = combined_display(image, matte)\n\n    # Display combined image\n    display(combined)\n\n    # Display merged\n    merged = Image.composite(middle_image,background_image, matte)\n    print(image_name, '\\n')\n    display(merged)\n\n\n\n\n\n\n\n\n170891_00_2x.jpg \n\n\n\n\n\n\n\n\n\n\nAs you can see, the first line of images corresponds to original image, image we get, and matte(alpha)\nThe second line is the image we merged with background.\nLet’s break down what each part does:\n\nIterating Through Image Files:\nimage_names = os.listdir(input_folder)\nfor image_name in image_names:\n\nThis loop iterates through each file name in the input_folder, which contains the input images.\n\n\n\nObtaining Matte File Name:\n    matte_name = image_name.split('.')[0] + '.png'\n\nIt extracts the file name of the matte corresponding to the current image by splitting the image file name at the ‘.’ character and appending ‘.png’ to it.\n\n\n\nOpening Image and Matte:\n    image = Image.open(os.path.join(input_folder, image_name))\n    matte = Image.open(os.path.join(output_folder, matte_name))\n\nIt opens the input image and matte files using Image.open() from the PIL library, specifying their respective paths.\n\n\n\nVisualizing Combined Image and Middle Image:\n    combined, middle_image = combined_display(image, matte)\n\n    # Display combined image\n    display(combined)\n\n    # Display middle image\n    display(middle_image)\n\nIt calls the combined_display() function to create the combined image and extract the middle image (predicted foreground).\nThen, it displays both the combined image and the middle image using display().\n\n\n\nMerging Middle Image with Background:\n    merged = Image.composite(middle_image, background_image, matte)\n\nIt composites the middle image with a background image using the alpha channel provided by the matte.\n\n\n\nPrinting Image Name:\n    print(image_name, '\\n')\n\nIt prints the name of the current image file.\n\n\n\nDisplaying Merged Image:\n    display(merged)\n\nIt displays the merged image, which combines the middle image with a background using the provided matte."
  },
  {
    "objectID": "posts/Final Project/index.html#implementation-in-web",
    "href": "posts/Final Project/index.html#implementation-in-web",
    "title": "Final Project",
    "section": "6. Implementation in Web",
    "text": "6. Implementation in Web\n:With all the functions we made, we want to realize it in our web. Just like this:\n\n\n\nimage.png\n\n\nHow are we going to achieve this?"
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "Global Warming",
    "section": "",
    "text": "Data Wrangling and Visualization"
  },
  {
    "objectID": "posts/HW1/index.html#intro",
    "href": "posts/HW1/index.html#intro",
    "title": "Global Warming",
    "section": "Intro",
    "text": "Intro\nIn this blog, we will develop interactive tools that offer an engaging exploration of the impact of global warming on temperatures worldwide. Moreover, I’ll guide you through the process, assuming you possess fundamental Python skills. The primary tools employed include Matplotlib and Seaborn, with a significant focus on Plotly for creating interactive graphics. Additionally, we’ll utilize Pandas and NumPy for efficient dataframe manipulation and SQLite3 for establishing the database.\nThe data set that we are going to use are:\ntemperatures(NOAA-GHCN data): temps.csv\nCities temperature data collected by the National Oceanic and Atmospheric Administration (NOAA) in a monthly frequence for each year.\n\ntemps = pd.read_csv(\"temps.csv\")\ntemps.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\nThis dataset contains temperature data for different cities identified by their unique ID. Each row represents temperature observations for a specific city in a particular year. Here’s an explanation of the structure:\n\nID: Unique identifier for each city.\nYear: The year for which temperature data is recorded.\nVALUE1 to VALUE12: These columns represent the temperature values for each month of the year. For example, VALUE1 corresponds to January, VALUE2 to February, and so on until VALUE12 for December.\n\nThe temperature values are recorded in Fahrenheit and represent the temperature observations for each month throughout the year. Negative values might indicate temperatures below freezing point, while positive values indicate temperatures above freezing point.\nstations: station-metadata.csv\nEach city’s ID, location, elvation, and nation.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\nID: Unique identifier for each station or location.\nLATITUDE: Latitude coordinate of the station’s or location’s position.\nLONGITUDE: Longitude coordinate of the station’s or location’s position.\nSTNELEV: Elevation of the station or location in meters above sea level.\nNAME: Name or identifier of the station or location.\n\nThis dataset provides geographical information about various weather stations or locations, including their geographic coordinates (latitude and longitude), elevation, and names. These stations or locations are likely where temperature data, as mentioned in the previous dataset, has been recorded.\ncontries: https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\nThis dataset appears to contain information about countries, including their codes according to the FIPS 10-4 standard and ISO 3166 standard, along with the country names. Here’s an explanation of the structure:\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\nFIPS 10-4: Code assigned to countries according to the FIPS 10-4 standard.\nISO 3166: Code assigned to countries according to the ISO 3166 standard.\nName: Name of the country.\n\nThis dataset provides a reference for countries and their corresponding codes according to two different standards, as well as their names. These codes are often used to uniquely identify countries in various systems and datasets.\nWith all the information you needed for the dataset, let’s get start it!!"
  },
  {
    "objectID": "posts/HW1/index.html#import-libraries-1",
    "href": "posts/HW1/index.html#import-libraries-1",
    "title": "Global Warming",
    "section": "1. Import Libraries",
    "text": "1. Import Libraries\n\n# Set the default Plotly renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n\nimport pandas as pd\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport sqlite3\n\nfrom sklearn.linear_model import LinearRegression\nfrom plotly import express as px\nimport calendar\n\nimport plotly.subplots as sp\nimport plotly.graph_objs as go"
  },
  {
    "objectID": "posts/HW1/index.html#create-data-base-1",
    "href": "posts/HW1/index.html#create-data-base-1",
    "title": "Global Warming",
    "section": "2. Create Data Base",
    "text": "2. Create Data Base\nThe Data Base called HW1.db would contain 3 tables: temperatures, stations, and contries.\nWe will do this by : - First connect to the data base - Then prepare the data frames before write it into the data base. - Lastly, check the table information and close the connection.\n\n# Connect to the data base HW1.db\nconn = sqlite3.connect(\"HW1.db\")\n\n\nTable1: temperatures\nAs we have seen before, the temperature dataset contains temperatures for one month for each city, which is not easy to use if we want to investigate the temperature distribution of a nation or city for a specific month. Therefore, we are going to reframe it into a data set that contains only the temperature of the city for a specific month for each row. Plus, units for data seems to have some problem.\nWe are going to do this by following steps:\n\nSet the multi-index with keys ‘ID’ and ‘Year’.\nStack the DataFrame to transform columns into rows.\nReset the index to create a new default integer index.\nRename columns to ‘Month’ and ‘Temp’.\nExtract the numeric month from the ‘Month’ column.\nNormalize the ‘Temp’ column by dividing by 100.\n\n\ndef prepare_temp_df(df):\n    \"\"\"\n    Preprocesses a DataFrame containing temperature data.\n\n    Parameters:\n    - df (pd.DataFrame): Input DataFrame containing temperature information.\n\n    Returns:\n    - pd.DataFrame: Processed DataFrame with columns: 'ID', 'Year', 'Month', 'Temp'.\n      'Month' is extracted from the original DataFrame, and 'Temp' is normalized to degrees Celsius.\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns={\"level_2\": \"Month\", 0: \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"] / 100\n    return df\n\n\ntemperature_df = prepare_temp_df(temps)\n\n\ntemperature_df.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n\n\n\n\n\nNow the data is ready, we can write it into the data base now.\n\n# Write the DataFrame 'temperature_df' to an SQLite database table named 'temperatures'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\n\ntemperature_df.to_sql(\"temperatures\", conn, if_exists = \"replace\", index = False)\n\n13992662\n\n\n\n\nTable2: stations\nThis data set does not need extra processing since all seems to be fine and tight.\n\n# Write the DataFrame 'stations' to an SQLite database table named 'stations'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\n\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n27585\n\n\n\n\nTable3: countries\nThis data set also does not need extra processing since all seems to be fine and tight.\n\n# Write the DataFrame 'countries' to an SQLite database table named 'countries'\n# using the connection object 'conn'. If the table already exists, replace it.\n# Set the 'index' parameter to False to avoid writing the DataFrame index as a separate column.\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\n\n279\n\n\n\n# Create a cursor object from the SQLite connection\ncursor = conn.cursor()\n\n# Execute a SQL query to retrieve the CREATE TABLE statements for all tables in the database\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\n# Fetch and print the CREATE TABLE statements for each table\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nHere we checked the names of tables, columns of tables, and the storage type of each column.\n\n# Close the connection\nconn.close()"
  },
  {
    "objectID": "posts/HW1/index.html#query-function-query_climate_database-1",
    "href": "posts/HW1/index.html#query-function-query_climate_database-1",
    "title": "Global Warming",
    "section": "3. Query Function: query_climate_database",
    "text": "3. Query Function: query_climate_database\nThis Query Function located in climate_database.py would extract the information we need from those 3 tables and manipulate it with restrictions.\nWhat this query_climate_database really do is you give it the db file you are gonna use, the country that you want to investigate, the year range of the data you want, and the specific month of temperatures you want.\n\ndb_file, the file name for the database\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\n\nfrom climate_database import query_climate_database\nimport inspect\n\n# Print the source code of the 'query_climate_database' function\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Queries a climate database for temperature data based on specified criteria.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - country (str): Name of the country for which data is to be retrieved.\n    - year_begin (int): Starting year for the data retrieval.\n    - year_end (int): Ending year for the data retrieval.\n    - month (int): Month for which data is to be retrieved (1-12).\n\n    Returns:\n    - pandas.DataFrame: DataFrame containing temperature data matching the specified criteria.\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cmd = f\"\"\"\n        SELECT S.name, S.latitude, S.longitude, C.name as Country, T.year, T.month, T.temp\n        FROM countries C\n        JOIN stations S \n        ON SUBSTR(S.id, 1, 2) = C.\"FIPS 10-4\"\n        JOIN temperatures T\n        ON S.id = T.id\n        WHERE C.name = '{country}'\n        AND (T.year &gt;= {year_begin} AND T.year &lt;= {year_end})\n        AND T.month = {month}\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    return df\n\n\n\nWe try out the function we imported with parameters country = “India”, year_begin = 1980, year_end = 2020,month = 1. What the function would give us is a data frame with 7 columns: name, latitude, longitude, Name, Year, Month, Temp, so that we would know each station’s temperature of Jan. in India from 1980 to 2020.\nThe result has a shape of 3152*7.\n\n# test case\nquery_climate_database(db_file = \"HW1.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nName\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/HW1/index.html#a-geographic-scatter-function-for-yearly-temperature-increases-1",
    "href": "posts/HW1/index.html#a-geographic-scatter-function-for-yearly-temperature-increases-1",
    "title": "Global Warming",
    "section": "4. A Geographic Scatter Function for Yearly Temperature Increases",
    "text": "4. A Geographic Scatter Function for Yearly Temperature Increases\n\nHow does the average yearly change in temperature vary within a given country?\nGiven this question, we would like to write a function that would give us the answer vividly.\nThis function temperature_coefficient_plot should accept six explicit arguments, and an undetermined number of keyword arguments.\n\ndb_file, country, year_begin, year_end, and month should be as in the previous part.\nmin_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month should be plotted; the others should be filtered out. df.transform() plus filtering is a good way to achieve this task.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\n\n# convert the numbers of month into names so that we could use in the title\nmonth_names = calendar.month_name\nmonth_number_to_name = {index: month for index, month in enumerate(month_names) if month}\n\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, db_file='HW1.db', **kwargs):\n    \"\"\"\n    Generate a scatter plot on a map showing the estimated yearly increase in temperature for weather stations in a specified country and month.\n\n    Parameters:\n    - country (str): Country name.\n    - year_begin (int): Starting year for data retrieval.\n    - year_end (int): Ending year for data retrieval.\n    - month (int): Numeric representation of the month for data retrieval(1-12).\n    - min_obs (int): Minimum number of years of observation required for a station to be included.\n    - db_file (str, optional): Database file path. Default is 'HW1.db'.\n    - **kwargs: Additional keyword arguments to be passed to Plotly Express scatter_mapbox.\n\n    Returns:\n    - plotly.graph_objs.Figure: Scatter plot on a map.\n\n    Note:\n    - Requires the 'query_climate_database' function from the 'climate_database' module.\n\n    Example:\n    ```python\n    fig = temperature_coefficient_plot('USA', 2000, 2020, 7, 10)\n    fig.show()\n    ```\n    \"\"\"\n    # load the database\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    \n    # filtering \n    df['num_years'] = df.groupby(['NAME', 'Month'])['Year'].transform('nunique')\n    df = df[df['num_years'] &gt;= min_obs].drop(columns='num_years')\n    \n    # calculate the average yearly change in temperature for each station\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"] \n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n    \n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    \n    # merge the station info with yearly temp\n    station_info = df.groupby('NAME', as_index=False).agg({'LATITUDE': 'first', 'LONGITUDE': 'first'})\n    df_merge = pd.merge(coefs, station_info, on='NAME')\n    df_merge = df_merge.rename(columns={0: 'Estimated Yearly Increase(°C)'})\n    df_merge['Estimated Yearly Increase(°C)'] = df_merge['Estimated Yearly Increase(°C)'].round(4)\n    \n    month_name = month_number_to_name[month]\n    # plot\n    fig = px.scatter_mapbox(df_merge,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color='Estimated Yearly Increase(°C)',\n                            title=f\"Estimates of Yearly Increase in Temperature in {month_name} for Stations in {country}, Years {year_begin}-{year_end}\",\n                            color_continuous_midpoint=0,\n                            **kwargs\n                           )\n\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\n# if you want to make the graph larger:\n# fig.update_layout(margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 30})\n\nfig.show()\n\n\n\n\nAs we can see from the plot Ludhiana, India had a major increase in Jan. temperature from 1980-2020 with a rate of 0.1318. Some other stations near the coast also experienced a increase in Jan. temperature from 1980-2020. The stations inside the country mainly have a decrease in Jan. temperature from 1980-2020.\n\ncolor_map = px.colors.diverging.RdBu_r # choose a colormap\n\nfig = temperature_coefficient_plot('Russia', 1980, 2010, 6, \n                                   min_obs = 12,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\n# if you want to make the graph larger:\n# fig.update_layout(margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 30})\n\nfig.show()\n\n\n\n\nFrom the plot, it’s evident that Amguema, Russia experienced a significant increase in June temperature from 1980 to 2020, with a rate of 0.286. Most cities are located in the southern regions due to warmer climates, while city distribution in Russia is more dispersed in the north due to colder temperatures. It’s notable that the right-hand side of the plot shows mostly increasing temperatures, with western cities experiencing a slight decrease, albeit minimal as indicated by the lighter color close to white. Conversely, eastern cities exhibit more pronounced changes in temperature, indicated by darker colors. We can potentially observe the effects of global warming near the North Pole, as the upper region of Russia, which is close to the Arctic, shows an overall increase in temperature."
  },
  {
    "objectID": "posts/HW1/index.html#temperature-changes-in-difference-climate-zone",
    "href": "posts/HW1/index.html#temperature-changes-in-difference-climate-zone",
    "title": "Global Warming",
    "section": "5. Temperature Changes in Difference Climate Zone",
    "text": "5. Temperature Changes in Difference Climate Zone\n\nBackground\nThere are many different ways to classify climate zones, here is one of them:\nThere are 4 major climate zones.\n\nTropical zone from 0°– 23.5°/ -23.5° - 0°(between the tropics)\nIn the regions between the equator and the tropics (equatorial region), the solar radiation reaches the ground nearly vertically at noontime during almost the entire year. Thereby, it is very warm in these regions. Through high temperatures, more water evaporates and the air is often moist. The resulting frequent and dense cloud cover reduces the effect of solar radiation on ground temperature.\n\n\nSubtropics from 23.5°– 40°/ -40° - -23.5°\nThe subtropics receive the highest radiation in summer, since the Sun’s angle at noon is almost vertical to the Earth, whilst the cloud cover is relatively thin. These regions receive less moisture (see trade winds), what increases the effect of radiation. Therefore, most of the deserts in the world are situated in this zone. In winter, the radiation in these regions decreases significantly, and it can temporarily be very cool and moist.\n\n\nTemperate zone from 40°–60°/ -60° - -40°\nIn the temperate zone, the solar radiation arrives with a smaller angle, and the average temperatures here are much cooler than in the subtropics. The seasons and daylength differ significantly in the course of a year. The climate is characterised by less frequent extremes, a more regular distribution of the precipitation over the year and a longer vegetation period - therefore the name “temperate”.\n\n\nCold zone from 60°–90°/ -90° - -60°\nThe polar areas between 60° latitude and the poles receive less heat through solar radiation, since the Sun has a very flat angle toward the ground. Because of the changes of the Earth axis angle to the Sun, the daylength varies most in this zone. In the summer, polar days occur. Vegetation is only possible during a few months per year and even then is often sparse. The conditions for life in these regions are very hard.\n\n\n\nHow has global warming impacted the four climate zones throughout the years and across different seasons?\nThis query allyear_climate_region_database() would give you Name, NAME, Latitude, Year, Month, Temp if you input the db_file, la_lower, la_upper - db_file - la_lower and la_upper, the latitude range you want to investgate\n\nfrom climate_database import allyear_climate_region_database\nimport inspect\n\n# Print the source code of the 'allyear_climate_region_database' function\nprint(inspect.getsource(allyear_climate_region_database))\n\ndef allyear_climate_region_database(db_file, la_lower, la_upper):\n    \"\"\"\n    Queries a climate database for temperature data within a latitude range.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - la_lower (float): Lower latitude bound for the latitude range.\n    - la_upper (float): Upper latitude bound for the latitude range.\n\n    Returns:\n    - pandas.DataFrame: DataFrame containing temperature data within the specified latitude range.\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cmd = f\"\"\"\n        SELECT C.name, S.name, S.latitude, T.year, T.month, T.temp\n        FROM countries C\n        JOIN stations S \n        ON SUBSTR(S.id, 1, 2) = C.\"FIPS 10-4\"\n        JOIN temperatures T\n        ON S.id = T.id\n        WHERE (S.latitude &gt;= {la_lower} AND S.latitude &lt;= {la_upper})\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    return df\n\n\n\nThe two_season_plot(a, b,season_a, season_b) and four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d) would draw a 2*1 plot or 4*1 plot, coresponds to each season. X axis would be the Year, Y axis would be the Temperatures throughout years.\nThere would 3 lines, the red line is the Max temperature of each month throughout years, the blue line is the Min temperature of each month throughout years, and the black line is the Avg temperature of the year.\n\ndef two_season_plot(a, b, season_a, season_b):\n    \"\"\"\n    Generate a side-by-side plot comparing temperature trends for two different seasons over the years.\n\n    Parameters:\n    - a (pd.DataFrame): DataFrame for the first season's temperature data.\n    - b (pd.DataFrame): DataFrame for the second season's temperature data.\n    - season_a (str): Name or identifier for the first season.\n    - season_b (str): Name or identifier for the second season.\n\n    Returns:\n    - plotly.graph_objs.Figure: Side-by-side line plots with linear regression lines for both seasons.\n\n    Example:\n    ```python\n    fig = two_season_plot(df_season_spring, df_season_fall, 'Spring', 'Fall')\n    fig.show()\n    ```\n    \"\"\"\n    # Find the common x-axis range for both DataFrames\n    x_range = [min(min(a['Year']), min(b['Year'])), max(max(a['Year']), max(b['Year']))]\n\n    # Calculate y_range\n    y_range = [min(min(a['MinTemp']), min(b['MinTemp'])), max(max(a['MaxTemp']), max(b['MaxTemp']))]\n\n    # Create subplots with 1 row and 2 columns\n    fig = sp.make_subplots(rows=2, cols=1, subplot_titles=[f'{season_a} Temperature Over Years', f'{season_b} Temperature Over Years'], row_heights=[3, 3])\n\n    # Add line plot for DataFrame 'a' to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)')), row=1, col=1)\n\n    # Calculate linear regression for DataFrame 'a'\n    a_slope, a_intercept = np.polyfit(a['Year'], a['AvgTemp'], 1)\n    a_regression_line = a_slope * a['Year'] + a_intercept\n\n    # Add linear regression line to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a_regression_line, mode='lines', name=f'Regression A: {a_slope:.2f}x + {a_intercept:.2f}', line=dict(color='lightblue')), row=1, col=1)\n\n    # Add line plot for DataFrame 'b' to the second subplot\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['AvgTemp'], mode='lines',name='Avg Temperature', line=dict(color='black'), showlegend=False), row=2, col=1)\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['MinTemp'], mode='lines',name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)'), showlegend=False), row=2, col=1)\n    fig.add_trace(go.Scatter(x=b['Year'], y=b['MaxTemp'], mode='lines',name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)'), showlegend=False), row=2, col=1)\n\n    # Calculate linear regression for DataFrame 'b'\n    b_slope, b_intercept = np.polyfit(b['Year'], b['AvgTemp'], 1)\n    b_regression_line = b_slope * b['Year'] + b_intercept\n\n    # Add linear regression line to the second subplot\n    fig.add_trace(go.Scatter(x=b['Year'], y=b_regression_line, mode='lines', name=f'Regression B: {b_slope:.2f}x + {b_intercept:.2f}', line=dict(color='lightcoral')), row=2, col=1)\n\n    # Set the common x-axis range for both subplots\n    fig.update_xaxes(range=x_range, row=1, col=1)\n    fig.update_xaxes(range=x_range, row=2, col=1)\n\n    # Set the common y-axis range for both subplots\n    fig.update_yaxes(range=y_range, row=1, col=1)\n    fig.update_yaxes(range=y_range, row=2, col=1)\n    \n    fig.update_xaxes(title_text='Year', row=2, col=1)\n    fig.update_yaxes(title_text='Temperature', row=1, col=1)\n    fig.update_yaxes(title_text='Temperature', row=2, col=1)\n\n    # Update layout if needed\n    fig.update_layout(showlegend=True)  # Optional: set showlegend=False if you don't want legends for each subplot\n\n    return fig\n\n\ndef four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d):\n    \"\"\"\n    Generate a four-subplot plot comparing temperature trends for four different seasons over the years.\n\n    Parameters:\n    - a, b, c, d (pd.DataFrame): DataFrames for temperature data of four different seasons.\n    - season_a, season_b, season_c, season_d (str): Names or identifiers for the four seasons.\n\n    Returns:\n    - plotly.graph_objs.Figure: Four-subplot line plots with linear regression lines for each season.\n\n    Example:\n    ```python\n    fig = four_season_plot(df_season_spring, df_season_summer, df_season_fall, df_season_winter, 'Spring', 'Summer', 'Fall', 'Winter')\n    fig.show()\n    ```\n    \"\"\"\n    # Find the common x-axis range for all DataFrames\n    x_range = [min(min(a['Year']), min(b['Year']), min(c['Year']), min(d['Year'])),\n               max(max(a['Year']), max(b['Year']), max(c['Year']), max(d['Year']))]\n\n    # Calculate y_range\n    y_range = [min(min(a['MinTemp']), min(b['MinTemp']), min(c['MinTemp']), min(d['MinTemp'])),\n               max(max(a['MaxTemp']), max(b['MaxTemp']), max(c['MaxTemp']), max(d['MaxTemp']))]\n\n    # Create subplots with 4 rows and 1 column\n    fig = sp.make_subplots(rows=4, cols=1, subplot_titles=[f'{season_a} Temperature Over Years', f'{season_b} Temperature Over Years', f'{season_c} Temperature Over Years', f'{season_d} Temperature Over Years'], vertical_spacing=0.1)\n\n    # Add line plot for DataFrame 'a' to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)')), row=1, col=1)\n    fig.add_trace(go.Scatter(x=a['Year'], y=a['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)')), row=1, col=1)\n\n    # Calculate linear regression for DataFrame 'a'\n    a_slope, a_intercept = np.polyfit(a['Year'], a['AvgTemp'], 1)\n    a_regression_line = a_slope * a['Year'] + a_intercept\n\n    # Add linear regression line to the first subplot\n    fig.add_trace(go.Scatter(x=a['Year'], y=a_regression_line, mode='lines', name=f'Regression A: {a_slope:.2f}x + {a_intercept:.2f}', line=dict(color='lightblue')), row=1, col=1)\n\n    i = 2\n    for data in (b, c, d):\n        # Add line plot for DataFrame 'b' to the second subplot\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['AvgTemp'], mode='lines', name='Avg Temperature', line=dict(color='black'), showlegend=False), row=i, col=1)\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['MinTemp'], mode='lines', name='Min Temperature', line=dict(color='rgba(0, 0, 255, 0.3)'), showlegend=False), row=i, col=1)\n        fig.add_trace(go.Scatter(x=data['Year'], y=data['MaxTemp'], mode='lines', name='Max Temperature', line=dict(color='rgba(255, 0, 0, 0.3)'), showlegend=False), row=i, col=1)\n\n        # Calculate linear regression for DataFrame 'b'\n        slope, intercept = np.polyfit(data['Year'], data['AvgTemp'], 1)\n        regression_line = slope * data['Year'] + intercept\n\n        # Add linear regression line to the second subplot\n        fig.add_trace(go.Scatter(x=data['Year'], y=regression_line, mode='lines', name=f\"Regression {i}: {slope:.2f}x + {intercept:.2f}\", line=dict(color='lightcoral')), row=i, col=1)\n\n        i += 1\n\n    for r in range(1, 5):\n        # Set the common x-axis range for all subplots\n        fig.update_xaxes(range=x_range, row=r, col=1)\n\n        # Set the common y-axis range for all subplots\n        fig.update_yaxes(range=y_range, row=r, col=1)\n\n        fig.update_yaxes(title_text='Temperature', row=r, col=1)\n\n    # Update layout if needed\n    fig.update_xaxes(title_text='Year', row=4, col=1)\n    fig.update_layout(showlegend=True)  # Optional: set showlegend=False if you don't want legends for each subplot\n\n    return fig\n\nThe season_plot will notify you the latitude you inputted is locate in which climate zone and the yearly temprature of that zone in different seasons.\n\ndef season_plot(la_lower, la_upper, db_file='HW1.db'):\n    \"\"\"\n    Generate temperature plots for different seasons based on latitude range.\n\n    Parameters:\n    - la_lower, la_upper (float): Latitude range for temperature data.\n    - db_file (str): Database file containing temperature data. Default is 'HW1.db'.\n\n    Returns:\n    - plotly.graph_objs.Figure: Temperature plots based on latitude range and seasons.\n\n    Example:\n    ```python\n    fig = season_plot(-40, -23.5, 'climate_data.db')\n    fig.show()\n    ```\n    \"\"\"\n    df = allyear_climate_region_database(db_file, la_lower, la_upper)\n\n    # Northern Tropical Zone (0-23.5)\n    if 0 &lt;= la_lower &lt;= 23.5 and 0 &lt;= la_upper &lt;= 23.5:\n        print('This latitude range is in the Northern Tropical Zone and it has only 2 seasons!')\n        filter1 = df[df['Month'].isin([10, 1, 2, 3])]  # Dry season\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([4, 5, 6, 7, 8, 9])]  # Wet season\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        season_a = 'Dry Season'\n        season_b = 'Wet Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Southern Tropical Zone (-23.5 to 0)\n    if -23.5 &lt;= la_lower &lt;= 0 and -23.5 &lt;= la_upper &lt;= 0:\n        print('This latitude range is in the Southern Tropical Zone and it has only 2 seasons!')\n        a = avg2\n        b = avg1\n        season_a = 'Dry Season'\n        season_b = 'Wet Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Northern Subtropical Zone (23.5-40)\n    if 23.5 &lt;= la_lower &lt;= 40 and 23.5 &lt;= la_upper &lt;= 40:\n        print('This latitude range is in the Northern Subtropical Zone and it has only 2 seasons!')\n        filter1 = df[df['Month'].isin([10, 1, 2, 3])]  # Hot summer\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([4, 5, 6, 7, 8, 9])]  # Wet season\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        season_a = 'Summer Season'\n        season_b = 'Mild Winter Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Southern Subtropical Zone (-40 to -23.5)\n    if -40 &lt;= la_lower &lt;= -23.5 and -40 &lt;= la_upper &lt;= -23.5:\n        print('This latitude range is in the Southern Subtropical Zone and it has only 2 seasons!')\n        a = avg2\n        b = avg1\n        season_a = 'Summer Season'\n        season_b = 'Mild Winter Season'\n        return two_season_plot(a, b, season_a, season_b)\n\n    # Northern Temperate Zone (40-60)\n    if 40 &lt;= la_lower &lt;= 60 and 40 &lt;= la_upper &lt;= 60:\n        print('This latitude range is in the Northern Temperate Zone and it has 4 seasons!')\n        filter1 = df[df['Month'].isin([3, 4, 5])]  # Spring\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([6, 7, 8])]  # Summer\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter3 = df[df['Month'].isin([9, 10, 11])]  # Autumn\n        avg3 = filter3.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg3.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter4 = df[df['Month'].isin([12, 1, 2])]  # Winter\n        avg4 = filter4.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg4.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        c = avg3\n        d = avg4\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    # Southern Temperate Zone (-60 to -40)\n    if -60 &lt;= la_lower &lt;= -40 and -60 &lt;= la_upper &lt;= -40:\n        print('This latitude range is in the Southern Temperate Zone and it has 4 seasons!')\n        a = avg3\n        b = avg4\n        c = avg1\n        d = avg2\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    # Northern Cold Zone (60-90)\n    if 60 &lt;= la_lower &lt;= 90 and 60 &lt;= la_upper &lt;= 90:\n        print('This latitude range is in the Northern Cold Zone and it has 4 seasons!')\n        filter1 = df[df['Month'].isin([3, 4, 5])]  # Spring\n        avg1 = filter1.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg1.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter2 = df[df['Month'].isin([6, 7, 8])]  # Summer\n        avg2 = filter2.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg2.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter3 = df[df['Month'].isin([9, 10, 11])]  # Autumn\n        avg3 = filter3.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg3.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        filter4 = df[df['Month'].isin([12, 1, 2])]  # Winter\n        avg4 = filter4.groupby('Year')['Temp'].agg(['mean', 'max', 'min']).reset_index()\n        avg4.columns = ['Year', 'AvgTemp', 'MaxTemp', 'MinTemp']\n\n        a = avg1\n        b = avg2\n        c = avg3\n        d = avg4\n        season_a = 'Spring'\n        season_b = 'Summer'\n        season_c = 'Autumn'\n        season_d = 'Winter'\n        return four_season_plot(a, b, c, d, season_a, season_b, season_c, season_d)\n\n    warning = 'Your latitude should be inside the range of each zone!'\n    return warning\n\n\n# let's try out the Northern Cold Zone\nresult = season_plot(60, 90)\n\nThis latitude range is in the Northern Cold Zone and it has 4 seasons!\n\n\n\nresult.update_layout(height=600, width=1000)\nresult.show()\n\n                                                \n\n\nHere you could do the experiment by yourself with different latitude. I plug in 60 and 90, which is in Northern Cold Zone and it has 4 seasons. The overall temperature is low compare to other places and the linear line of average temperature is nearly flat, which indicates a not much of difference among the temperatures over years. Other than that, Spring and Fall share the same distribution, Summer being the overall hottest, and Winter being the overall coldest.\n\n\nDoes the Global Warming effect differently in these 4 climate zones?\nTo answer this question, for each zone, we can plot the yearly average temperature for each zone.\n\n# Retrieving climate data for different latitude ranges to analyze specific climate zones\n\n# Tropical Zone: Latitude between -23.5 and 23.5 degrees\ntropical = allyear_climate_region_database('HW1.db', -23.5, 23.5)\n\n# Subtropical Zone: Latitude between 23.5 and 40 degrees (sub1) and -40 to -23.5 degrees (sub2)\nsub1 = allyear_climate_region_database('HW1.db', 23.5, 40)\nsub2 = allyear_climate_region_database('HW1.db', -40, -23.5)\nsub = pd.concat([sub1, sub2], ignore_index=True)\n\n# Temperate Zone: Latitude between 40 and 60 degrees (temperate1) and -60 to -40 degrees (temperate2)\ntemperate1 = allyear_climate_region_database('HW1.db', 40, 60)\ntemperate2 = allyear_climate_region_database('HW1.db', -60, -40)\ntemperate = pd.concat([temperate1, temperate2], ignore_index=True)\n\n# Cold Zone: Latitude between 60 and 90 degrees\ncold = allyear_climate_region_database('HW1.db', 60, 90)\n\n# Calculate average yearly temperature for each climate zone\ntro_avg = tropical.groupby('Year')['Temp'].mean()\nsub_avg = sub.groupby('Year')['Temp'].mean()\ntemperate_avg = temperate.groupby('Year')['Temp'].mean()\ncold_avg = cold.groupby('Year')['Temp'].mean()\n\n\nfig = go.Figure()\n\n# Add trace for tropical dataset\nfig.add_trace(go.Scatter(x=tro_avg.index, y=tro_avg.values, mode='markers', name='Tropical', line=dict(color='blue')))\n\n# Add trace for sub dataset\nfig.add_trace(go.Scatter(x=sub_avg.index, y=sub_avg.values, mode='markers', name='Subtropical', line=dict(color='green')))\n\n# Add trace for temperate dataset\nfig.add_trace(go.Scatter(x=temperate_avg.index, y=temperate_avg.values, mode='markers', name='Temperate', line=dict(color='orange')))\n\n# Add trace for cold dataset\nfig.add_trace(go.Scatter(x=cold_avg.index, y=cold_avg.values, mode='markers', name='Cold', line=dict(color='red')))\n\n# Update layout for better readability\nfig.update_layout(title='Yearly Average Temperature',\n                  xaxis_title='Year',\n                  yaxis_title='Temperature',\n                  legend=dict(title='Dataset'))\n\n# Show the plot\nfig.show()\n\n                                                \n\n\nThe overall Yearly Average Temperature for each climate zone did not change a lot throughout years but with a slight increase for each scatter line especially for the cold and tropical zone. However, from the average temperature, we could barely see the global warming effect. There are more infos that we need to investigate!"
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Message Hub",
    "section": "",
    "text": "Building a Simple Message Bank Web Application with Flask: A Step-by-Step Tutorial"
  },
  {
    "objectID": "posts/HW3/index.html#table-of-contents",
    "href": "posts/HW3/index.html#table-of-contents",
    "title": "Message Hub",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1. Introduction\n\n\n2. Document composition\n2.1 templates\n    2.1.1 base.html\n    2.1.2 main.html\n    2.1.3 submit.html\n    2.1.4 view.html\n2.2 app.py\n2.3 static (style.css)\n\n\n1. Introduction\nWelcome to this tutorial where you’ll learn how to create a simple web application using Flask. In this blog post, I’ll guide you through the process of building a message bank web app from scratch and describe each step along the way. By the end of this tutorial, you’ll have a basic understanding of Flask fundamentals, database skills, and how to apply basic CSS to enhance the appearance of your web app.\nYou are free to build on any of the examples, as well as any other resources you are able to find.\nYou can find the code for this tutorial here: https://github.com/jamie1130/PIC-16B\nLet’s take a peek at what this website can do!\nThis is the first thing you would see when you open the page, the Main page: \nThen you can either view other’s messages by click Messages at the bottom or compose one by yourself. Let’s try to write one:\n\n\n\nimage-2.png\n\n\nMessage sent: \nAnd it will be stored in the message bank and you can view it when you want:\n\n\n\nimage-4.png\n\n\nAt the third row you can see your message!\n\n\n2. Document composition\n\nSome preparation:\nFirst we are going to deal with the webpage structure using Jinjia template and the logic behind it using Flask.\nA few things you might want to know before we proceed:\nJinja is a modern and designer-friendly templating engine for Python programming language. It is widely used in web development with frameworks like Flask and Django.\nA Jinja template is a text file containing a mixture of static text and template tags, which are special syntax blocks or expressions recognized by the Jinja engine. These template tags allow you to inject dynamic content, perform control flow operations, and execute logic directly within your HTML or other text-based documents.\nHere are some key features of Jinja templates:\n\nTemplate Inheritance: Jinja supports template inheritance, allowing you to define a base template with common layout and structure, and then extend or override specific sections in child templates.\nVariables: You can use variables within your templates to insert dynamic content. These variables are typically passed from the Python code to the template during rendering.\nControl Structures: Jinja provides control structures like loops and conditionals, allowing you to perform logic and iterate over data directly within your templates.\nFilters: Filters allow you to modify the output of variables in your templates. For example, you can apply filters to format dates, convert strings to uppercase, or perform other transformations.\nMacros: Macros are reusable blocks of code that can be defined once and called multiple times within a template. They are similar to functions in programming languages.\nComments: Jinja templates support comments, allowing you to add explanatory notes or reminders within your template files.\n\nOverall, Jinja templates provide a powerful and flexible way to generate dynamic content for web applications, making it easier to separate presentation logic from application logic.\nFlask is a lightweight and flexible web framework for Python. It is designed to make getting started with web development quick and easy, while still being powerful enough to build complex web applications.\nKey features of Flask include:\n\nMinimalistic: Flask is minimalist by design. It provides only the essential components needed for web development, allowing developers to add functionality as needed through extensions.\nEasy to Use: Flask is known for its simplicity and ease of use. Its API is intuitive and straightforward, making it a great choice for beginners and experienced developers alike.\nExtensible: Flask is highly extensible. It provides a modular architecture that allows developers to easily add third-party extensions to add additional functionality to their applications.\nWerkzeug and Jinja2 Integration: Flask is built on top of the Werkzeug WSGI toolkit and the Jinja2 template engine. Werkzeug provides low-level utilities for handling HTTP requests and responses, while Jinja2 provides a powerful and flexible templating engine for generating HTML and other text-based documents.\nDevelopment Server: Flask comes with a built-in development server, allowing developers to test their applications locally during development.\nRESTful Request Dispatching: Flask supports RESTful request dispatching out of the box, making it easy to build RESTful APIs.\nBuilt-in Development and Debugging Tools: Flask provides built-in development and debugging tools, including a built-in debugger and support for interactive debugging in the Python shell.\n\nOverall, Flask is a popular choice for web development in Python due to its simplicity, flexibility, and extensibility. It is widely used for building web applications, APIs, and microservices.\nHere you can find the tutorial of Jinja template: https://jinja.palletsprojects.com/en/3.1.x/templates/Links to an external site.\nHere you can find the tutorial of Flask: https://flask.palletsprojects.com/en/2.2.x/quickstart/Links to an external site.\nOk now you have a basic idea of what Jinja and Flask are, you need to make sure that you have installed Flask in your environment. Jinja is one of the dependencies of Flask, and Flask installs it automatically when you install Flask using pip. Flask relies on Jinja for its templating engine, so it includes Jinja as part of its package.\nIn terminal:\npip install Flask\nThen run the following:\nexport FLASK_ENV=development\nSetting FLASK_ENV to development tells Flask to run the application in development mode. In development mode, Flask enables additional features and behaviors that are helpful during the development process.\nAfter this, create a file and name whatever you want and that is the place we are going to operate and everything happened inside this file.\n\n\n2.1 templates\nCreate a file inside it and named it templates.\ntemplates directory: This directory contains HTML templates that Jinja uses to render dynamic content. Templates typically include HTML markup along with Jinja template tags and expressions for inserting dynamic content, variables, control structures, and more. When you call render_template() in app.py, Flask looks for templates in the templates directory by default.\nSince our webpage would have 3 pages and each cooresponds to a html: - main page (main.html): the page that we see first - submit page (submit.html): the page that we can compose our messages and submit it - view page (view.html): the messages in the bank\nNow we are going to see it one-by-one.\n\n2.1.1 base.html\nbase.html typically serves as the base or parent template in a Flask application’s template hierarchy. It contains the common structure, layout, and elements that are shared across multiple pages of a website or web application.\n&lt;!DOCTYPE html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\" /&gt;\n&lt;title&gt;{% block title %}{% endblock %} - Message Hub&lt;/title&gt;\n&lt;div class=\"container\"&gt;\n  &lt;div class=\"title\"&gt;{% block header %}{% endblock %}&lt;/div&gt;\n  &lt;div class=\"content\"&gt;\n    {% block content %}{% endblock %}\n  &lt;/div&gt;\n  &lt;div class=\"nav-bar\"&gt;\n    &lt;a href=\"{{ url_for('main') }}\" class=\"operation\"&gt;\n      &lt;img class=\"icon\" src=\"../static/home.svg\" /&gt;\n      &lt;span&gt;Main page&lt;/span&gt;\n    &lt;/a&gt;\n    &lt;a href=\"{{ url_for('view') }}\" class=\"operation\"&gt;\n      &lt;img class=\"icon\" src=\"../static/message.svg\" /&gt;\n      &lt;span&gt;Messages&lt;/span&gt;\n    &lt;/a&gt;\n    &lt;a href=\"{{ url_for('submit') }}\" class=\"operation\"&gt;\n      &lt;img class=\"icon\" src=\"../static/create.svg\" /&gt;\n      &lt;span&gt;Compose&lt;/span&gt;\n    &lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\nThe code block defines the basic structure of the website, which includes a title of Message Hub at the top, then there is a container that contains title, content, and a nav-bar that includes there buttons: Main page, Message, and Compose, where each button has a icon that is in the static folder, we will mention it later.\n\n&lt;!DOCTYPE html&gt;: This is a document type declaration that specifies the HTML version being used. It tells the web browser which version of HTML the document is written in.\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\" /&gt;: This line includes an external CSS stylesheet named style.css into the HTML document. The url_for() function is a Flask helper function that generates a URL for a given endpoint. Here, it generates the URL for the static file style.css located in the static directory of the Flask application.\n&lt;title&gt;{% block title %}{% endblock %} - Message Hub&lt;/title&gt;: This line sets the title of the HTML document. The {% block title %}{% endblock %} part is a Jinja template block that serves as a placeholder for child templates to provide a title specific to each page. The - Message Hub text is a static part of the title that is common across all pages.\n&lt;div class=\"container\"&gt;: This line starts a container div element with the CSS class container. It’s used to wrap the content of the page and apply styling or layout rules.\n&lt;div class=\"title\"&gt;{% block header %}{% endblock %}&lt;/div&gt;: This line defines a div element with the CSS class title. The {% block header %}{% endblock %} part is a Jinja template block that serves as a placeholder for child templates to provide a header specific to each page.\n&lt;div class=\"nav-bar\"&gt;...&lt;/div&gt;: This line defines a div element with the CSS class nav-bar, which typically contains navigation links. Inside this div, there are anchor (&lt;a&gt;) elements representing navigation links to different pages of the website. The href attributes of these anchor elements use the url_for() function to generate URLs for the corresponding endpoints in the Flask application.\n\nOverall, this base.html template provides a common layout structure for HTML pages in the Flask application, with placeholders for dynamic content such as page titles, headers, and main content. Child templates can extend this base template and fill in the content blocks with specific content relevant to each page.\n\n\n2.1.2 main.html\nThis would be the first page that you see when open the webpage. It would be like this: \nThe code looks like\n{% extends 'base.html' %}\n\n{% block title %}Overview{% endblock %}\n{% block header %}Message Hub{% endblock %}\n\n{% block content %}\n&lt;div class=\"welcome-messages\"&gt;\n  &lt;div&gt;Welcome to Message Hub!&lt;/div&gt;\n  &lt;div&gt;At here, you can either comspanose a message by yourself or view messages in the hub. Hope you will enjoy it!&lt;/div&gt;\n&lt;/div&gt;\n{% endblock %}\nLet’s break down each part:\n\n{% extends 'base.html' %}: This line specifies that the current template extends the base template base.html. It means that the content of the current template will replace the corresponding blocks in the base template.\n{% block title %}Overview{% endblock %}: This block overrides the title block defined in the base template. It sets the title of the HTML document to “Overview”. The content between {% block title %} and {% endblock %} will replace the corresponding block in the base template.\n{% block header %}Message Hub{% endblock %}: This block overrides the header block defined in the base template. It sets the header of the page to “Message Hub”. Similar to the title block, the content between {% block header %} and {% endblock %} will replace the corresponding block in the base template.\n{% block content %}...{% endblock %}: This block overrides the content block defined in the base template. It provides specific content for the main content area of the page. In this case, it contains a &lt;div&gt; element with the class welcome-messages and two paragraphs of text welcoming users to the Message Hub.\n\nBy using the extends keyword and defining blocks within the child template, you can create modular and reusable templates in Flask applications.\n\n\n2.1.3 submit.html\nThe submit page would contain 3 components: 1. A text box for submitting a message. 2. A text box for submitting the name of the user. 3. A “submit” button.\nThe page looks like this:\n\n\n\nimage.png\n\n\nThe code looks like this:\n{% extends 'base.html' %}\n\n{% block header %}\n  {% block title %}New Message{% endblock %}\n{% endblock %}\n\n{% block content %}\n  &lt;form action=\"/submit\" method=\"post\" class=\"form\"&gt;\n    &lt;div class='form-item form-item-name'&gt;\n      &lt;label for=\"name\"&gt;Your Name or Handle:&lt;/label&gt;\n      &lt;input type=\"text\" id=\"name\" name=\"name\" required&gt;\n    &lt;/div&gt;\n    &lt;div class='form-item form-item-message'&gt;\n      &lt;label for=\"message\"&gt;Your Message:&lt;/label&gt;\n      &lt;textarea id=\"message\" name=\"message\" required&gt;&lt;/textarea&gt;\n    &lt;/div&gt;\n    &lt;button type=\"submit\"&gt;Submit Message&lt;/button&gt;\n  &lt;/form&gt;\n\n  {% if thanks %}\n    &lt;div class=\"success\"&gt;\n      Message Sent\n    &lt;/div&gt;\n  {% endif %}\n\n{% endblock %}\nLet’s break down each part:\n\n{% extends 'base.html' %}: This line specifies that the current template extends the base template base.html. It means that the content of the current template will replace the corresponding blocks in the base template.\n{% block header %}...{% endblock %}: This block overrides the header block defined in the base template. Inside this block, there is another block called title. This nested block defines the title of the HTML document to be “New Message”. The content between {% block header %} and {% endblock %} will replace the corresponding block in the base template.\n{% block content %}...{% endblock %}: This block overrides the content block defined in the base template. It provides specific content for the main content area of the page. In this case, it contains an HTML form with input fields for the user’s name (or handle) and message, along with a submit button. Below the form, there is an {% if thanks %} condition, which checks if the thanks variable is set to true. If it is, a success message “Message Sent” is displayed and we are going to see this variable soon in the app.py.\n\n\n\n2.1.4 view.html\nThis one would be easier since it only contains the messages we retrived from the data base.\nThe page would looks like this: \nThe code looks like this:\n{% extends 'base.html' %}\n\n{% block header %}\n  {% block title %}Sinp of Messages{% endblock %}\n{% endblock %}\n\n{% block content %}\n&lt;div class=\"message-container\"&gt;\n  {% for tuple in message_tuples%}\n    &lt;div class=\"message\"&gt;\n      &lt;div class=\"message-owner\"&gt;{{tuple[0]}}&lt;/div&gt;\n      &lt;div class=\"message-content\"&gt;\"{{tuple[1]}}\"&lt;/div&gt;\n    &lt;/div&gt;\n  {% endfor %}\n&lt;/div&gt;\n{% endblock %}\nLet’s break down each part:\n\n{% extends 'base.html' %}: This line specifies that the current template extends the base template base.html. It means that the content of the current template will replace the corresponding blocks in the base template, just like before.\n{% block header %}...{% endblock %}: This block overrides the header block defined in the base template. Inside this block, there is another block called title. This nested block defines the title of the HTML document to be “Sinp of Messages”. The content between {% block header %} and {% endblock %} will replace the corresponding block in the base template.\n{% block content %}...{% endblock %}: This block overrides the content block defined in the base template. It provides specific content for the main content area of the page. In this case, it contains a &lt;div&gt; element with the class message-container, which serves as a container for displaying messages.\nInside the message-container div, there is a for loop that iterates over a list of message_tuples. For each tuple in the list, it generates a &lt;div&gt; element with the class message, containing two inner &lt;div&gt; elements:\n\n&lt;div class=\"message-owner\"&gt;{{tuple[0]}}&lt;/div&gt;: This div displays the owner of the message. It accesses the first element of the tuple using { tuple[0] } Jinja syntax.\n&lt;div class=\"message-content\"&gt;\"{{tuple[1]}}\"&lt;/div&gt;: This div displays the content of the message. It accesses the second element of the tuple using { tuple[1] } Jinja syntax. The content is enclosed in double quotes.\n\n\nWe will see why we get the data this way once we know how the data were stored.\n\n\n\n2.2 app.py\nThis is the main Python script where you define your Flask application. In app.py, you define routes, configure the application, and handle requests. Jinja is used in conjunction with render_template() function calls in app.py to render HTML templates dynamically.\nFirst let’s import libraries we are going to use:\nfrom flask import Flask, g, render_template, request\nfrom flask import redirect, url_for\n\nimport sqlite3\nimport pandas as pd\nThen creates a Flask application instance by:\napp = Flask(__name__)\nHere’s what it does:\n\nCreates an Application Instance: The Flask() constructor creates a new Flask application instance. This instance represents your web application and allows you to configure routes, define view functions, handle HTTP requests, and more.\n__name__ Argument: The __name__ argument is a special Python variable that represents the name of the current module. When you use __name__ as an argument in the Flask() constructor, Flask uses it to determine the root path of the application’s resources, such as templates and static files. It’s important to note that __name__ is different depending on whether the Python script is being run as the main program or imported as a module.\nAssigns the Application Instance to app: The resulting Flask application instance is assigned to the variable app, which you can then use throughout your Flask application to define routes, configure settings, and run the application.\n\n@app.route('/')\ndef main():\n    return render_template('main.html')\nIn Flask, @app.route('/') is a decorator that is used to define a route for a specific URL.\nHere’s how it works:\n\n@app.route('/'): This line decorates the following function, main(), indicating that the function should be executed when a request is made to the root URL '/'. The @app.route() decorator takes the URL pattern as its argument. In this case, '/' represents the root URL of the Flask application.\ndef main():: This is the definition of the main() function. This function is executed when a request is made to the root URL '/'.\nreturn render_template('main.html'): Inside the main() function, render_template() is called with the argument 'main.html'. This function renders the specified HTML template, main.html, and returns the resulting HTML content as the response to the client’s request.\nrender_template() is a function provided by Flask that renders HTML templates. It looks for the specified template file in the templates directory of the Flask application and processes any Jinja template tags or blocks contained within the template file.\n\nIn summary, the code @app.route('/') decorates the main() function, specifying that it should be executed when a request is made to the root URL '/'. When this route is accessed, the main() function returns the contents of the main.html template to the client’s web browser.\n@app.route(\"/submit/\", methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    else:\n        insert_message(request)\n        return render_template('submit.html', thanks = True)\nThis code is a Flask route definition for handling requests to the URL “/submit/” with both GET and POST methods.\nHere’s how it works:\n\n@app.route(\"/submit/\", methods=['POST', 'GET']): This decorator defines a route for the URL “/submit/” and specifies that it should handle both GET and POST requests. The methods argument is a list containing the HTTP methods that this route should accept.\ndef submit():: This is the definition of the submit() function, which will be executed when a request is made to the “/submit/” URL.\nif request.method == 'GET':: This conditional statement checks if the HTTP request method is GET. If it is, it means that the user is accessing the “/submit/” page via a GET request (e.g., by typing the URL into their browser’s address bar or following a link). In this case, the function returns the rendered template ‘submit.html’ using render_template(). This template likely contains a form that users can fill out to submit a message.\nelse:: If the request method is not GET, it means that it is a POST request (e.g., the user submitted the form). In this case, the function calls insert_message(request) to handle inserting the submitted message into the database. After inserting the message, it returns the rendered template ‘submit.html’ again, this time passing in an additional variable thanks = True. This variable is used to display a message on the page indicating that the message was successfully submitted.\n\nIn summary, this route handler allows users to access the “/submit/” page via both GET and POST requests. If accessed via GET, it renders the ‘submit.html’ template containing a form. If accessed via POST (i.e., when the form is submitted), it inserts the submitted message into the database and renders the ‘submit.html’ template again, this time displaying a message indicating that the message was successfully submitted.\n@app.route(\"/view/\")\ndef view(): \n    rdm_mesg = random_messages(5)\n    length = 5\n    message_tuples = []\n    for i in range(length):\n        message_tuples.append(tuple(rdm_mesg.iloc[i,:]))\n    return render_template('view.html', message_tuples = message_tuples)\nThis code defines a Flask route for handling requests to the URL “/view/”.\nHere’s how it works:\n\n@app.route(\"/view/\"): This decorator defines a route for the URL “/view/”. When a request is made to this URL, the associated function, view(), will be executed.\ndef view():: This is the definition of the view() function, which handles requests to the “/view/” URL.\nrdm_mesg = random_messages(5): This line calls a function random_messages() to retrieve a random selection of 5 messages. The exact implementation of random_messages() is not shown, but it presumably retrieves random messages from some data source.\nlength = 5: This line sets the variable length to 5, indicating the number of messages to retrieve.\nmessage_tuples = []: This line initializes an empty list called message_tuples. This list will store tuples, where each tuple represents a message.\nfor i in range(length):: This line starts a loop that iterates length times. It iterates through the range of numbers from 0 to length - 1.\nmessage_tuples.append(tuple(rdm_mesg.iloc[i,:])): Inside the loop, this line retrieves a row from the rdm_mesg DataFrame (presumably containing the random messages) using rdm_mesg.iloc[i,:], converts it to a tuple, and appends it to the message_tuples list.\nreturn render_template('view.html', message_tuples = message_tuples): Finally, this line renders the ‘view.html’ template using render_template(). It passes the message_tuples list as a variable named message_tuples to the template. This allows the template to access and display the messages retrieved in the view function.\n\nIn summary, when a request is made to the “/view/” URL, the view() function retrieves a random selection of messages, converts them to tuples, and passes them to the ‘view.html’ template for rendering. The template can then display these messages to the user.\nAs you might noticed, there are functions we need to write that appeared in front:\ndef get_message_db():\n    try:\n        # gets datbase from `g` object\n        return g.message_db\n    except:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cmd = \"\"\"\n        CREATE TABLE IF NOT EXISTS messages(\n        handle TEXT,\n        message TEXT\n        );\n        \"\"\"\n        cursor.execute(cmd)\n        cursor.close()\n        return g.message_db\nThis Python function get_message_db() serves the purpose of managing the database connection for the Flask application.\nHere’s how it works:\n\nTry-Except Block: The function first attempts to retrieve the database connection from the g attribute of the Flask application context. The g object is a special object provided by Flask for storing global variables during the lifetime of a request. If the database connection is already present in g, it is returned immediately.\nIf the database connection is not found in g, the except block is executed.\nDatabase Connection: Inside the except block, a new SQLite database connection is established using sqlite3.connect(). The database file is specified as \"messages_db.sqlite\". This creates a new SQLite database file if it doesn’t already exist.\nTable Creation: After establishing the database connection, the function checks whether a table named “messages” exists in the database. If the table doesn’t exist, it creates it using the SQL command specified in the cmd variable. This command creates a table with two columns: handle and message, both of type TEXT.\nCursor Operation: The function creates a cursor using g.message_db.cursor() to execute SQL commands. It then executes the CREATE TABLE command to create the “messages” table if it doesn’t exist.\nClosing Cursor: After executing the SQL command, the cursor is closed using cursor.close() to free up resources.\nReturning Database Connection: Finally, the function returns the database connection stored in g.message_db. This ensures that subsequent calls to get_message_db() within the same Flask request context reuse the same database connection.\n\nIn summary, get_message_db() ensures that a database connection is established, a “messages” table is created if it doesn’t exist, and returns the database connection for use in other parts of the Flask application.\ndef insert_message(request):\n\n    # Extract the message and the handle from request\n    handle = request.form['name']\n    message = request.form['message']\n\n    # Connect to the message database\n    db = get_message_db()\n\n    # Insert message into the messages table\n    cursor = db.cursor()\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    \n    # Commit changes to the database\n    db.commit()\n    \n    # Close the database connection\n    db.close()\n    \n    # Return the message and the handle\n    return message, handle\nThis Python function insert_message(request) is responsible for handling the insertion of a user message into the database of messages.\nHere’s a breakdown of how it works:\n\nParameter Explanation: The docstring also explains that the function takes a request object as a parameter. This request object contains form data submitted by the user.\nExtracting Message and Handle: The function extracts the message and handle from the request object using request.form['name'] and request.form['message'], respectively. These values correspond to the form fields named “name” and “message” submitted by the user.\nDatabase Connection: The function connects to the message database using the get_message_db() function. This function ensures that a database connection is established and returns the connection.\nInserting Message: The function creates a cursor using db.cursor() to execute SQL commands. It then executes an SQL INSERT statement to insert the message and handle into the “messages” table of the database. The VALUES (?, ?) part of the statement indicates that the values are provided as parameters, which helps prevent SQL injection attacks.\nCommitting Changes: After inserting the message into the database, the function commits the changes using db.commit(). This ensures that the changes are saved permanently in the database.\nClosing Database Connection: Finally, the function closes the database connection using db.close(). Closing the connection frees up resources and prevents potential issues with concurrent access to the database.\nReturning Message and Handle: The function returns a tuple containing the message and handle. This allows the caller to access these values if needed.\n\nIn summary, insert_message(request) handles the insertion of user messages into the database by extracting the message and handle from the request, inserting them into the database, committing the changes, closing the database connection, and returning the message and handle.\ndef random_messages(n):\n    db = get_message_db()\n    cmd = f\"\"\" SELECT * FROM messages ORDER BY RANDOM() LIMIT {n}; \"\"\"\n    rdm_mesg = pd.read_sql_query(cmd, db)\n    db.close()\n    return rdm_mesg\nThis Python function random_messages(n) retrieves a random selection of messages from the database of messages.\nHere’s a breakdown of how it works:\n\nDatabase Connection: The function first connects to the message database using the get_message_db() function. This function ensures that a database connection is established and returns the connection.\nSQL Command: The function constructs an SQL command to select a random sample of messages from the “messages” table. The ORDER BY RANDOM() clause ensures that the rows are returned in random order, and the LIMIT {n} clause limits the number of rows returned to n. The f-string syntax is used to include the value of n in the SQL command.\nExecuting SQL Command: The function executes the SQL command using pd.read_sql_query(), a function provided by the pandas library. This function executes the SQL query against the database connection (db) and returns the results as a pandas DataFrame (rdm_mesg).\nClosing Database Connection: After retrieving the random messages, the function closes the database connection using db.close(). Closing the connection frees up resources and prevents potential issues with concurrent access to the database.\nReturning Random Messages: Finally, the function returns the pandas DataFrame rdm_mesg, which contains the randomly selected messages.\n\nIn summary, random_messages(n) retrieves a random selection of messages from the database by executing an SQL query, reading the results into a pandas DataFrame, and returning the DataFrame containing the random messages.\n\n\n2.3 static (style.css)\nstatic directory: This directory contains static assets such as CSS, JavaScript, images, and other files that are served directly to clients without modification. While Jinja itself doesn’t directly interact with the static directory, Flask applications often use Jinja to generate URLs for static assets using the url_for() function. This allows you to reference static assets in your HTML templates in a way that’s flexible and consistent with your application’s routing.\nhtml {\n  font-family: sans-serif;\n  background: #fff;\n}\n\nbody {\n  max-width: 400px;\n  margin: auto;\n}\n\n.container {\n  width: 368px;\n  height: 780px;\n  padding: 39px 16px;\n  position: relative;\n  background-image: url(./phone_background.png);\n  background-size: cover;\n  /* background-repeat: no-repeat; */\n  /* background-size: 400px 800pwx; */\n}\n\n.title {\n  width: 368px;\n  height: 54px;\n  position: absolute;\n  left: 16px;\n  top: 36px;\n  font-size: 18px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  color: #fff;\n  background-color: #444444;\n}\n\n.content {\n  padding: 20px;\n  margin: 54px 0;\n  height: 580px;\n  overflow-y: scroll;\n}\n\n.nav-bar {\n  position: absolute;\n  left: 16px;\n  bottom: 90px;\n  width: 368px;\n  height: 54px;\n  background-color: #444444;\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n}\n\n.operation {\n  width: calc(100% / 3);\n  height: 100%;\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  justify-content: center;\n}\n\n.operation:hover {\n  background-color: #333;\n  transition: all 0.3;\n}\n\n.icon {\n  color: #fff;\n  width: 24px;\n  height: 24px;\n}\n\na {\n  color: #fff;\n  text-decoration: none;\n}\n\n.welcome-messages {\n  display: flex;\n  flex-direction: column;\n  gap: 18px;\n  font-size: 16px;\n}\n\n.welcome-messages &gt; div:first-child {\n  font-size: 24px;\n}\n\n.welcome-messages &gt; div:last-child {\n  line-height: 1.4;\n}\n\n.form {\n  display: flex;\n  flex-direction: column;\n  gap: 24px;\n}\n.form-item {\n  display: flex;\n  flex-direction: column;\n  gap: 12px;\n}\n\ninput {\n  outline: none;\n  height: 24px;\n}\n\ntextarea {\n  outline: none;\n  height: 120px;\n  resize: none;\n}\n\nbutton {\n  height: 24px;\n}\n\n.success {\n  margin-top: 22px;\n  color: green;\n}\n\n.message-container {\n  display: flex;\n  flex-direction: column;\n  gap: 9px;\n}\n\n.message {\n  display: flex;\n  flex-direction: column;\n  gap: 9px;\n  padding-bottom: 18px;\n  border-bottom: #eeeeee 1px solid;\n}\n.message:hover {\n  border-bottom: #555555 1px solid;\n}\n\n.message-owner {\n  font-size: 24px;\n  color: #444444;\n}\nThis is a CSS stylesheet defining various styles for different elements of a web page. Here’s a breakdown of the styles defined:\n\nhtml: Sets the default font family to sans-serif and sets the background color to white.\nbody: Sets the maximum width of the body to 400px and centers it horizontally using auto margins.\n.container: Styles a container element with a fixed width and height, positioned relative to its containing block. It sets a background image and ensures it covers the entire container.\n.title: Styles a title element with a fixed width and height, positioned absolutely within its containing block. It centers the text vertically and horizontally, sets the font size, and applies colors.\n.content: Styles a content area with padding, margin, height, and overflow properties. It ensures that content exceeding the height is scrollable.\n.nav-bar: Styles a navigation bar with a fixed position at the bottom of its containing block. It sets the background color and arranges its child elements horizontally with space-between alignment.\n.operation: Styles individual navigation bar items, including width, height, and alignment properties. It changes the background color on hover.\n.icon: Styles icons within navigation bar items, setting the color, width, and height.\na: Styles anchor links, setting the color and removing the underline.\n.welcome-messages: Styles a section containing welcome messages, arranging its child elements vertically with a gap between them.\n.form: Styles a form element, arranging its child elements vertically with a gap between them.\n.form-item: Styles form items, arranging their child elements vertically with a gap between them.\ninput and textarea: Styles form input and textarea elements, removing their outline and setting their height and other properties.\nbutton: Styles buttons, setting their height.\n.success: Styles a success message, setting its margin and color.\n.message-container: Styles a container for messages, arranging its child elements vertically with a gap between them.\n.message: Styles individual message elements, arranging their child elements vertically with a gap between them and adding a border.\n.message:hover: Styles individual message elements on hover, changing the border color.\n.message-owner: Styles message owner elements, setting the font size and color.\n\nOverall, these styles define the appearance and layout of various elements on a web page, providing consistency and visual appeal.\nAlright, we’ve completed all the tasks! Now you can get creative with your own message bank."
  },
  {
    "objectID": "posts/HW5/index.html",
    "href": "posts/HW5/index.html",
    "title": "Doggie & Catie Classification",
    "section": "",
    "text": "Dog and Cat Images Classifications"
  },
  {
    "objectID": "posts/HW5/index.html#table-of-contents",
    "href": "posts/HW5/index.html#table-of-contents",
    "title": "Doggie & Catie Classification",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1. Introduction\n\n\n2. Setup and Data Exploration\n\n\n3. Model1 (keras.Sequential)\n\n\n4. Model2 (Model1 + Data Augmentation)\n\n\n5. Model3 (Data Preprocessing + Model2)\n\n\n6. Model4 (Data Augmentation + Transfer Learning)"
  },
  {
    "objectID": "posts/HW5/index.html#introduction-1",
    "href": "posts/HW5/index.html#introduction-1",
    "title": "Doggie & Catie Classification",
    "section": "1. Introduction",
    "text": "1. Introduction\nIn this section, we are going to use TensorFlow as primiary tool.\nTensorFlow is a powerful tool for building and training machine learning models. It offers flexibility to work on different devices, scalability for large projects, and easy-to-use interfaces like Keras for beginners. With features like TensorBoard for visualization and a supportive community, TensorFlow is a popular choice for both learning and real-world applications.\nAnother well-know tool is PyTorch, which is developed by Facebook’s AI Research lab. PyTorch is known for its dynamic computation graph, making it more flexible for experimentation and research. It has gained popularity for its intuitive interface and strong support for deep learning.\nThe choice between PyTorch and TensorFlow often depends on factors such as project requirements, personal preferences, and familiarity with the frameworks. Here are some considerations for when to use each:\nUse PyTorch: 1. Dynamic Computation Graphs: If you prefer a dynamic computation graph where the graph structure can change for each iteration, PyTorch might be a better choice. This flexibility is useful for research and experimentation, especially in fields like natural language processing and reinforcement learning.\n\nEase of Use: PyTorch is often praised for its simplicity and Pythonic syntax, making it more accessible for beginners and researchers who want to quickly prototype and experiment with different models.\nCommunity and Ecosystem: While TensorFlow has a larger ecosystem due to its longer presence in the field, PyTorch has a rapidly growing community with strong support from both academia and industry. If you prefer a community that’s more focused on cutting-edge research and experimentation, PyTorch might be more suitable.\n\nUse TensorFlow: 1. Static Computation Graphs: If you require a static computation graph where the graph structure is defined before execution, TensorFlow’s static graph paradigm might be more suitable. This can offer performance optimizations and easier deployment in production environments.\n\nScalability and Production Readiness: TensorFlow is well-suited for building and deploying large-scale production systems, thanks to its support for distributed computing, deployment options like TensorFlow Serving, and integration with tools like TensorFlow Extended (TFX) for end-to-end machine learning pipelines.\nWider Adoption and Support: TensorFlow has been around longer and has a larger user base, which means it has more resources, tutorials, and pre-trained models available. If you’re working on a project where existing resources and models are crucial, TensorFlow might be the better choice.\n\nUltimately, both PyTorch and TensorFlow are powerful frameworks with their own strengths and weaknesses. It’s a good idea to experiment with both and choose the one that best fits your specific project requirements and preferences."
  },
  {
    "objectID": "posts/HW5/index.html#setup-and-data-exploration-1",
    "href": "posts/HW5/index.html#setup-and-data-exploration-1",
    "title": "Doggie & Catie Classification",
    "section": "2. Setup and Data Exploration",
    "text": "2. Setup and Data Exploration\n\nSetup\nImport libraries\n\n!pip install keras==2.15.0\n\nRequirement already satisfied: keras==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras\nfrom keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nThe data we are going to use is cats_vs_dogs from tensorflow datasets, which is a commonly used dataset for binary image classification tasks. It contains images of cats and dogs, which are labeled accordingly 0 as cat 1 as dog.\n\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\nNow we are going to load and split the “cats_vs_dogs” dataset using TensorFlow Datasets (TFDS) API into training, validation, and test sets.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nLet’s break down the code:\nLoading the Dataset: - tfds.load(\"cats_vs_dogs\", ...) : loads the “cats_vs_dogs” dataset using the TensorFlow Datasets (TFDS) API. The dataset is specified by its name.\nSplitting the Dataset: - split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"] : splits the dataset into three subsets: training, validation, and test sets. The percentages indicate the proportion of data allocated to each set. In this case, 40% of the data is allocated for training, 10% for validation, and 10% for testing.\nLoading as Supervised: - as_supervised=True : This parameter indicates that the dataset should be loaded with labels. When set to True, the dataset will be returned as tuples (image, label) where image represents the input data (image) and label represents the corresponding label (cat or dog).\nPrinting Dataset Cardinalities: - train_ds.cardinality() : returns the number of elements in the training dataset. - validation_ds.cardinality() : returns the number of elements in the validation dataset. - test_ds.cardinality() : returns the number of elements in the test dataset.\nOverall, this code snippet loads the “cats_vs_dogs” dataset, splits it into training, validation, and test sets, and prints the number of samples in each set. This is a common practice in machine learning to ensure that the dataset is properly divided for training, validation, and evaluation purposes.\nThe training dataset is used to teach the model, the validation dataset is used to fine-tune it, and the test dataset is used to evaluate its performance on unseen data. Splitting the dataset helps ensure the model learns effectively, generalizes well, and is properly evaluated.\nNow since the picture have difference sizes, we resize them to a fixed size of 150x150.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nLet’s break it down:\n\nresize_fn = keras.layers.Resizing(150, 150) : creates a resizing layer using the Resizing class from the Keras API (a part of TensorFlow). The Resizing layer is configured to resize images to a target size of 150x150 pixels.\ntrain_ds.map(lambda x, y: (resize_fn(x), y)) : maps the resizing operation to each sample in the training dataset. The map function applies the provided lambda function to each element of the dataset. In this case, the lambda function resizes the input image x using the resize_fn layer and keeps the label y unchanged. This effectively resizes each image in the training dataset to the specified size.\n\nSimilarly, the resizing operation is applied to each sample in the validation and test datasets using validation_ds.map(lambda x, y: (resize_fn(x), y)) and test_ds.map(lambda x, y: (resize_fn(x), y)), respectively.\n\n\nPurpose of Resizing: - Resizing the images to a uniform size (in this case, 150x150 pixels) is a common preprocessing step in computer vision tasks. It ensures that all images have the same dimensions, which is often required by machine learning models. This allows the model to process the images efficiently and ensures consistency during training. - Additionally, resizing can help reduce computational complexity and memory usage, especially when working with large datasets or when training deep learning models.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\nprint('Number of train batches: %d' % tf.data.experimental.cardinality(train_ds))\nprint('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_ds))\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_ds))\n\nNumber of train batches: 146\nNumber of validation batches: 37\nNumber of test batches: 37\n\n\nBatching: - train_ds.batch(batch_size) : This line batches the samples in the training dataset into groups or batches, where each batch contains batch_size number of samples, which is 64 in this case. Batching is a common technique used in machine learning to process multiple samples simultaneously, which can improve training efficiency. - Similarly, the validation and test datasets are also batched using validation_ds.batch(batch_size) and test_ds.batch(batch_size), respectively.\nPrefetching: - prefetch(tf_data.AUTOTUNE) : This line prefetches data from the dataset to improve performance. The AUTOTUNE parameter allows TensorFlow to automatically determine the optimal number of elements to prefetch dynamically based on available resources and the current workload. Prefetching overlaps data preprocessing and model execution, reducing the time spent waiting for data during training. - Prefetching is especially useful when working with large datasets or when using complex preprocessing pipelines.\nCaching: - cache() : This line caches the elements of the dataset into memory or storage to improve data access speed. Caching allows TensorFlow to avoid reloading and preprocessing the data for each epoch during training, which can significantly speed up training, especially if the dataset fits into memory. - Caching the datasets can be particularly beneficial when working with smaller datasets or when the dataset preprocessing is computationally expensive.\nLastly, we count on the number of batched in each dataset, which could be calculated by the number of data in the dataset divided by batch size.\n\n\nData Exploration\nYou can get a piece of a data set using the take method; e.g. train_ds.take(1) will retrieve one batch (32 images with labels) from the training data.\nLet’s briefly explore our data set. Write a function to create a two-row visualization. In the first row, show three random pictures of cats. In the second row, show three random pictures of dogs. You can see some related code in the linked tutorial above, although you’ll need to make some modifications in order to separate cats and dogs by rows. A docstring is not required.\n\ndef preview_plot(data_set):\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Counter variables for dogs and cats\n    num_cats = 0\n    num_dogs = 0\n\n    ds_shuffled = data_set.shuffle(buffer_size=len(data_set))\n    for images, labels in ds_shuffled.take(1):  # Iterate over the first batch\n          for image, label in zip(images, labels):\n                if label.numpy() == 0 and num_cats &lt; 3:\n                    axes[0, num_cats].imshow(image.numpy() / 255.0)\n                    axes[0, num_cats].set_title(\"Cat\")\n                    num_cats += 1\n                elif label.numpy() == 1 and num_dogs &lt; 3:\n                    axes[1, num_dogs].imshow(image.numpy() / 255.0)\n                    axes[1, num_dogs].set_title(\"Dog\")\n                    num_dogs += 1\n\nplt.show()\n\npreview_plot visualizes a subset of images from a dataset. In the first row, there are three random pictures of cats. In the second row shows three random pictures of dogs.\nLet’s break down the code:\nfig, axes = plt.subplots(2, 3, figsize=(15, 10)): creates a figure and an array of subplots with a 2x3 grid layout (2 rows and 3 columns). Each subplot will display an image.\nnum_cats = 0 and num_dogs = 0: These variables are used to count the number of displayed images for cats and dogs, respectively. The reason why we need these numbers is that we only need 3 data with label 0 and 3 data with label 1.\nds_shuffled = data_set.shuffle(buffer_size=len(data_set)): shuffles the dataset to ensure randomness in the displayed images. It uses the shuffle method of the dataset, specifying the buffer size as the length of the dataset.\nfor images, labels in ds_shuffled.take(1):: iterates over the first batch of the shuffled dataset. - ds_shuffled.take(1) extracts the first batch from the shuffled dataset. - Within the loop, each image and its corresponding label are processed: - for image, label in zip(images, labels): - image.numpy() / 255.0 normalizes the image pixel values to the range [0, 1]. - If the label corresponds to a cat (label.numpy() == 0) and the number of displayed cat images (num_cats) is less than 3, the image is displayed in the upper row of subplots. - If the label corresponds to a dog (label.numpy() == 1) and the number of displayed dog images (num_dogs) is less than 3, the image is displayed in the lower row of subplots. - The titles of the subplots are set to “Cat” or “Dog” accordingly.\nplt.show(): This line displays the plot containing the images.\nOverall, the preview_plot function visualizes a subset of images from the dataset, showing up to 3 cat images and 3 dog images in a 2x3 grid layout. This function is useful for quickly inspecting the content of the dataset and verifying that the images are correctly labeled.\nNow let’s see what it does.\n\npreview_plot(data_set = validation_ds)\n\n\n\n\n\n\n\n\nThe following line of code will create an iterator called labels_iterator, that is used to extract labels from the training dataset.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nunbatch(): unrolls the batches in the dataset, converting it from a batched dataset into a dataset of individual samples.\nmap(lambda image, label: label): This map operation applies a function to each element of the dataset. In this case, it’s a lambda function that extracts the label from each sample in the dataset.\nas_numpy_iterator():converts the dataset into an iterator that yields elements as NumPy arrays. In this case, it returns an iterator containing only the labels as NumPy arrays.\nNow let’s Compute the number of images in the training data with label 0 (corresponding to “cat”) and label 1 (corresponding to “dog”).\n\nimport collections\n\n# Convert labels_iterator to a list\nlabels_list = list(labels_iterator)\n\n# Count the frequency of each label\nlabel_counts = collections.Counter(labels_list)\n\nprint(label_counts)\n\nCounter({1: 4668, 0: 4637})\n\n\ncollections module provides specialized container datatypes in Python, including the Counter class.\nlabels_list = list(labels_iterator):converts the labels_iterator into a list. The labels_iterator contains all the labels extracted from the dataset using the pipeline described earlier.\nlabel_counts = collections.Counter(labels_list): This line creates a Counter object called label_counts by passing the labels_list to the Counter class constructor. The Counter object counts the frequency of each unique label in the list.\nThe result of Counter({1: 4668, 0: 4637}) shows that number of cats and dog are near balance, which is good since the model would not be biased toward either result. Moreover, The baseline machine learning model is the model that always guesses the most frequent label. Since the frequency is almost the same, the accuracy of the baseline model would be super low as it will tend to identify any picture as dog since label 1 tends to have a higher frequency.\nWe’ll treat this as the benchmark for improvement. Our models should do much better than baseline in order to be considered good data science achievements!"
  },
  {
    "objectID": "posts/HW5/index.html#model1-keras.sequential-1",
    "href": "posts/HW5/index.html#model1-keras.sequential-1",
    "title": "Doggie & Catie Classification",
    "section": "3. Model1 (keras.Sequential)",
    "text": "3. Model1 (keras.Sequential)\nOur first model would be the one that use keras.Sequential, which could contain several layers.\n\n# Define the model\nmodel1 = models.Sequential([\n\n    # Convolutional layers\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer\n    layers.Flatten(),\n\n    # Dense layers\n    layers.Dense(128, activation='relu'),\n\n    # Dropout layer\n    layers.Dropout(0.5),\n\n    # Output layer\n    layers.Dense(2, activation='softmax')\n])\n\nWe defined a convolutional neural network (CNN) model using TensorFlow’s Keras API.\nA couple things you might want to know before we proceed: An activation function is a mathematical operation applied to the output of each neuron (or unit) in a neural network layer. It introduces non-linearity to the network, enabling it to learn complex patterns and relationships in the data. Activation functions determine whether a neuron should be activated (produce an output) or not, based on the weighted sum of its inputs.\nThere are several types of activation functions commonly used in neural networks. Some of the popular ones include:\n\nReLU (Rectified Linear Unit): ReLU is one of the most commonly used activation functions. It outputs the input directly if it is positive, and zero otherwise. Mathematically, it can be expressed as: \\[f(x) = \\max(0, x)\\] ReLU helps alleviate the vanishing gradient problem and accelerates convergence in training deep neural networks.\nSigmoid: Sigmoid function squashes the input values to the range of [0, 1]. It is often used in binary classification tasks where the output needs to be interpreted as probabilities. The mathematical expression for the sigmoid function is: \\[f(x) = \\frac{1}{1 + e^{-x}}\\]\nTanh (Hyperbolic Tangent): Tanh function squashes the input values to the range of [-1, 1]. It is similar to the sigmoid function but centered at zero. Tanh is commonly used in hidden layers of neural networks. The mathematical expression for the tanh function is: \\[f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\]\nSoftmax: Softmax function is used in the output layer of multi-class classification tasks. It normalizes the output values into a probability distribution over multiple classes, ensuring that the sum of the probabilities is equal to 1. Softmax is often used to interpret the output of the neural network as class probabilities. The mathematical expression for the softmax function is: \\[f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}\\] where K is the number of classes.\n\nChoosing an appropriate activation function is crucial for training neural networks effectively, as it directly impacts the network’s ability to learn and generalize from the data. Different activation functions may be more suitable for different types of tasks and network architectures.\nLet’s break down the model architecture and the purpose of each layer used:\n\nSequential Model:\n\nmodels.Sequential([...]): This initializes a sequential model, where layers are added sequentially one after the other.\n\nConvolutional Layers:\n\nlayers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)): This adds a 2D convolutional layer with 32 filters, each with a kernel size of (3, 3). The activation function used is ReLU (Rectified Linear Unit). The input_shape parameter specifies the shape of the input data, which is a 3D tensor representing an image with dimensions (height=150, width=150, channels=3) for RGB images.\nlayers.MaxPooling2D((2, 2)): This adds a max-pooling layer with a pool size of (2, 2) to downsample the spatial dimensions of the feature maps obtained from the convolutional layers. It helps reduce computational complexity and control overfitting by retaining important features.\n\nFlatten Layer:\n\nlayers.Flatten(): This layer flattens the output from the convolutional layers into a 1D array. It prepares the data for input into the subsequent fully connected (dense) layers.\n\nDense Layers:\n\nlayers.Dense(128, activation='relu'): This adds a fully connected (dense) layer with 128 neurons and ReLU activation function. This layer learns complex patterns from the flattened input data.\n\nDropout Layer:\n\nlayers.Dropout(0.5): This dropout layer randomly drops a fraction of the neurons (50% in this case) during training to prevent overfitting. It helps improve the generalization of the model by forcing it to learn redundant representations.\n\nOutput Layer:\n\nlayers.Dense(2, activation='softmax'): This adds the output layer with 2 neurons, corresponding to the two classes (cats and dogs) in the classification task. The activation function used is softmax, which converts the raw scores into probabilities, indicating the likelihood of each class.\n\n\nOverall, this model consists of convolutional layers for feature extraction, followed by dense layers for learning high-level representations, dropout for regularization, and an output layer for prediction. It is a typical CNN architecture for image classification tasks.\n\n# Compile the model\nmodel1.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Display the model summary\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n flatten (Flatten)           (None, 82944)             0         \n                                                                 \n dense (Dense)               (None, 128)               10616960  \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 2)                 258       \n                                                                 \n=================================================================\nTotal params: 10636610 (40.58 MB)\nTrainable params: 10636610 (40.58 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nCompiling the Model:\n\nmodel1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']): This line compiles the neural network model. During compilation, you specify three key components:\n\noptimizer: The optimization algorithm used to update the model’s weights during training. In this case, the Adam optimizer is used, which is a popular choice for gradient-based optimization.\nloss: The loss function used to measure the difference between the predicted outputs and the true labels. Here, ‘sparse_categorical_crossentropy’ is used, which is suitable for multi-class classification problems where the labels are integers.\nmetrics: A list of metrics used to evaluate the performance of the model during training and testing. In this case, ‘accuracy’ is chosen as the metric, which calculates the proportion of correctly classified samples.\n\n\n\nThere are various optimizers, loss functions, and evaluation metrics available in TensorFlow’s Keras API, each suited for different types of tasks and model architectures. Here’s an overview of some common choices and their typical use cases:\nOptimizers: - Adam: Adam is a popular adaptive learning rate optimization algorithm that combines the advantages of AdaGrad and RMSProp. It is well-suited for most deep learning tasks and is often a good default choice. - SGD (Stochastic Gradient Descent): SGD is the classic optimization algorithm used in training neural networks. While it’s simple and easy to implement, it may require careful tuning of the learning rate and momentum parameters. - RMSprop: RMSprop is another adaptive learning rate optimization algorithm that adjusts the learning rate based on the moving average of squared gradients. It is particularly effective for recurrent neural networks (RNNs) and other sequential models. - Adagrad: Adagrad adapts the learning rate based on the frequency of updates for each parameter. It is well-suited for sparse data and tasks with features that occur infrequently.\nLoss Functions: - Mean Squared Error (MSE): MSE is commonly used for regression tasks, where the model predicts continuous values. It measures the average squared difference between the predicted and true values. - Binary Crossentropy: Binary crossentropy is used for binary classification tasks, where the model predicts probabilities for two classes. It measures the difference between the predicted and true class labels. - Categorical Crossentropy: Categorical crossentropy is used for multi-class classification tasks, where the model predicts probabilities for multiple classes. It is suitable when the labels are one-hot encoded. - Sparse Categorical Crossentropy: Sparse categorical crossentropy is similar to categorical crossentropy but is used when the labels are integers instead of one-hot encoded vectors. It is suitable for multi-class classification tasks where the class labels are integers.\nMetrics: - Accuracy: Accuracy measures the proportion of correctly classified samples. It is a common metric for classification tasks but may not be suitable for imbalanced datasets. - Precision, Recall, F1-score: These metrics provide insights into the performance of the model, especially in scenarios with imbalanced classes. Precision measures the proportion of true positive predictions among all positive predictions, recall measures the proportion of true positive predictions among all actual positives, and F1-score is the harmonic mean of precision and recall. - Mean Squared Error (MSE): MSE can also be used as a metric for regression tasks to evaluate the average squared difference between predicted and true values.\nThe choice of optimizer, loss function, and metrics depends on the specific characteristics of the dataset, task requirements, and the architecture of the model. Experimentation and tuning may be necessary to find the most suitable combination for your particular problem.\n\nmodel1.summary(): This line displays a summary of the model architecture, including the layers, output shapes, and total number of parameters. The summary provides useful information for understanding the structure of the model, such as the number of trainable parameters and the flow of data through the network.\n\nOverall, after compiling the model with the specified optimizer, loss function, and metrics, calling model1.summary() provides a concise overview of the model’s architecture, helping you understand its structure and ensure it is configured correctly for training.\n\n# Train the model\nhistory = model1.fit(train_ds,\n                    epochs=20,  # Adjust number of epochs as needed\n                    validation_data=validation_ds)\n\n# Evaluate the model on the test dataset\ntest_loss, test_accuracy = model1.evaluate(test_ds)\n\nprint(\"Test Accuracy:\", test_accuracy)\n\nEpoch 1/20\n146/146 [==============================] - 14s 53ms/step - loss: 27.4379 - accuracy: 0.5378 - val_loss: 0.6796 - val_accuracy: 0.5421\nEpoch 2/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6351 - accuracy: 0.6216 - val_loss: 0.6892 - val_accuracy: 0.5615\nEpoch 3/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5402 - accuracy: 0.7032 - val_loss: 0.7278 - val_accuracy: 0.5911\nEpoch 4/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.4463 - accuracy: 0.7845 - val_loss: 0.7997 - val_accuracy: 0.5903\nEpoch 5/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.3611 - accuracy: 0.8352 - val_loss: 0.9493 - val_accuracy: 0.5954\nEpoch 6/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.3543 - accuracy: 0.8400 - val_loss: 0.9521 - val_accuracy: 0.5890\nEpoch 7/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2833 - accuracy: 0.8786 - val_loss: 1.0162 - val_accuracy: 0.5942\nEpoch 8/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2558 - accuracy: 0.8933 - val_loss: 1.0801 - val_accuracy: 0.5817\nEpoch 9/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.2169 - accuracy: 0.9132 - val_loss: 1.2342 - val_accuracy: 0.5911\nEpoch 10/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1917 - accuracy: 0.9268 - val_loss: 1.2070 - val_accuracy: 0.5903\nEpoch 11/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1784 - accuracy: 0.9305 - val_loss: 1.3432 - val_accuracy: 0.5877\nEpoch 12/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1714 - accuracy: 0.9330 - val_loss: 1.2284 - val_accuracy: 0.5890\nEpoch 13/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1155 - accuracy: 0.9584 - val_loss: 1.4662 - val_accuracy: 0.6079\nEpoch 14/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.1324 - accuracy: 0.9544 - val_loss: 1.4332 - val_accuracy: 0.6096\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.1323 - accuracy: 0.9558 - val_loss: 1.4723 - val_accuracy: 0.6062\nEpoch 16/20\n146/146 [==============================] - 6s 41ms/step - loss: 0.1051 - accuracy: 0.9646 - val_loss: 1.4721 - val_accuracy: 0.5959\nEpoch 17/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.0917 - accuracy: 0.9674 - val_loss: 1.5394 - val_accuracy: 0.6028\nEpoch 18/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1036 - accuracy: 0.9675 - val_loss: 1.6018 - val_accuracy: 0.6075\nEpoch 19/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.0923 - accuracy: 0.9703 - val_loss: 1.6863 - val_accuracy: 0.6105\nEpoch 20/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.0912 - accuracy: 0.9702 - val_loss: 2.1597 - val_accuracy: 0.6156\n37/37 [==============================] - 3s 71ms/step - loss: 2.0807 - accuracy: 0.6113\nTest Accuracy: 0.6113499402999878\n\n\nThis code snippet is responsible for training and evaluating a neural network model using TensorFlow’s Keras API. Let’s break it down:\n\nTraining the Model:\n\nhistory = model1.fit(train_ds, epochs=20, validation_data=validation_ds): This line trains the neural network model (model1) using the fit method. It takes the following arguments:\n\ntrain_ds: The training dataset containing input samples and their corresponding labels.\nepochs: The number of training epochs, i.e., the number of times the entire dataset is passed forward and backward through the network for training. In this case, the model is trained for 20 epochs.\nvalidation_data: Optional validation data to be used during training. Here, validation_ds is provided, which contains validation samples and their labels. During training, the model’s performance on this data will be evaluated after each epoch.\n\nThe fit method returns a history object, which contains information about the training process, such as training/validation loss and metrics over epochs. This object can be used for visualization and analysis.\n\nEvaluating the Model on the Test Dataset:\n\ntest_loss, test_accuracy = model1.evaluate(test_ds): This line evaluates the trained model on the test dataset (test_ds) using the evaluate method. It computes the loss and metrics (accuracy, in this case) of the model on the test data.\nThe evaluate method returns the test loss and test accuracy, which are assigned to the variables test_loss and test_accuracy, respectively.\n\nPrinting Test Accuracy:\n\nprint(\"Test Accuracy:\", test_accuracy): This line prints the test accuracy of the model obtained from evaluating it on the test dataset.\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nI tried to change the number of filters and neurons for example 32 into 64 filters, 64 neurons, and find out that less filters and neurons would make the model perform worse a little bit but substantially faster since it is a simpler model. You can try to add more layers or alternate the numbers or the functions on your own a few times!\nThe above model has an accuracy of 0.5 on both train and validation dataset at the begining, and achieved an accuracy of 0.9702 on train data and 0.6156 on validation data, which improves at the end. We also get accuracy of 0.6113 on the test data, which aligh well with the validation data test result.\nCompare to the baseline model, we only improved for about 10%. Plus, overfitting can be observed in this model since the training accuracy is much higher than the validation accuracy."
  },
  {
    "objectID": "posts/HW5/index.html#model2-model1-data-augmentation-1",
    "href": "posts/HW5/index.html#model2-model1-data-augmentation-1",
    "title": "Doggie & Catie Classification",
    "section": "4. Model2 (Model1 + Data Augmentation)",
    "text": "4. Model2 (Model1 + Data Augmentation)\nNow we’re going to add some data augmentation layers to the model.\nData augmentation involves applying various transformations to the training images, such as rotation, flipping, scaling, cropping, or shifting, to create modified versions of the original images. These transformations produce new training samples that are similar to the original ones but have slight variations. For example, flipping an image horizontally does not change its content fundamentally; it’s still the same object, just seen from a different perspective.\nThe goal of data augmentation is to increase the diversity and robustness of the training data, helping the model to generalize better to unseen variations of the input data. By exposing the model to a wider range of variations during training, data augmentation encourages the model to learn more invariant features that are useful for making accurate predictions. In other words, the model learns to focus on the essential characteristics of the images, regardless of their orientation, position, or other minor variations.\nFirst we are going to create a keras.layers.RandomFlip() layer and make a plot of the original image and a few copies to which RandomFlip() has been applied.\n\nfrom tensorflow.keras.layers import RandomFlip\n\n# Define a function to plot images\ndef plot_images(images, title):\n    plt.figure(figsize=(10, 5))\n    for i in range(len(images)):\n        plt.subplot(1, len(images), i + 1)\n        plt.imshow(images[i] / 255.0)  # Normalize pixel values to [0, 1]\n        plt.title(f'{title} {i + 1}')\n        plt.axis('off')\n    plt.show()\n\n# Load a sample image from the dataset (assuming train_ds contains images)\nsample_image = next(iter(train_ds))[0][0].numpy()\n\n# Create a RandomFlip layer\nflip_layer = RandomFlip(mode='horizontal')\n\n# Apply RandomFlip to the original image to generate augmented images\naugmented_images = [flip_layer(tf.expand_dims(sample_image, axis=0)) for _ in range(5)]\n\n# Convert augmented images to numpy arrays\naugmented_images = [image.numpy()[0] for image in augmented_images]\n\n# Plot the original image and augmented images\nplot_images([sample_image] + augmented_images, 'Image')\n\n\n\n\n\n\n\n\nAs you can see Image 1 as the original image, and Image3 and 6 are flipped horizonally.\nNext, we are going to create a keras.layers.RandomRotation() layer. Then, make a plot of both the original image and a few copies to which RandomRotation() has been applied.\n\nfrom tensorflow.keras.layers import RandomRotation\n\n# Create a RandomRotation layer\nrotation_layer = RandomRotation(factor=0.2)  # You can adjust the rotation factor as needed\n\n# Apply RandomRotation to the original image to generate augmented images\naugmented_images = [rotation_layer(tf.expand_dims(sample_image, axis=0)) for _ in range(5)]\n\n# Convert augmented images to numpy arrays\naugmented_images = [image.numpy()[0] for image in augmented_images]\n\n# Plot the original image and augmented images\nplot_images([sample_image] + augmented_images, 'Image')\n\n\n\n\n\n\n\n\nSetting factor = 0.2 means that the layer will randomly rotate the input images by a maximum angle of 0.2 radians in both clockwise and counterclockwise directions.\nAs you can see Image 1 as the original image, and others are rotated.\nWe would see the power of two layer in the plot showed below.\n\ndata_augmentation = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip('horizontal'),\n  tf.keras.layers.RandomRotation(0.2),\n])\n\nfor image, _ in train_ds.take(1):\n    plt.figure(figsize=(10, 10))\n    first_image = image[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n        plt.imshow(augmented_image[0] / 255)\n        plt.axis('off')\n\n\n\n\n\n\n\n\nNow, we create a new keras.models.Sequential model called model2 in which the first two layers are augmentation layers. Use a RandomFlip() layer and a RandomRotation() layer.\n\nfrom keras import optimizers\nmodel2 = models.Sequential([\n    data_augmentation,\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n\n    layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n    layers.Dropout(0.5),\n\n    layers.Dense(128, activation='relu', kernel_regularizer='l2'),\n    layers.Dropout(0.5),\n\n    layers.Dense(2, activation='softmax')\n])\noptimizer = optimizers.Adam(learning_rate=0.001)\nmodel2.compile(optimizer=optimizer,\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nmodel2.build(input_shape=(0, 150, 150, 3))\nmodel2.summary()\n\nModel: \"sequential_73\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n sequential_69 (Sequential)  (None, 150, 150, 3)       0         \n                                                                 \n conv2d_92 (Conv2D)          (0, 148, 148, 32)         896       \n                                                                 \n batch_normalization_36 (Ba  (0, 148, 148, 32)         128       \n tchNormalization)                                               \n                                                                 \n max_pooling2d_92 (MaxPooli  (0, 74, 74, 32)           0         \n ng2D)                                                           \n                                                                 \n conv2d_93 (Conv2D)          (0, 72, 72, 64)           18496     \n                                                                 \n batch_normalization_37 (Ba  (0, 72, 72, 64)           256       \n tchNormalization)                                               \n                                                                 \n max_pooling2d_93 (MaxPooli  (0, 36, 36, 64)           0         \n ng2D)                                                           \n                                                                 \n conv2d_94 (Conv2D)          (0, 34, 34, 128)          73856     \n                                                                 \n batch_normalization_38 (Ba  (0, 34, 34, 128)          512       \n tchNormalization)                                               \n                                                                 \n max_pooling2d_94 (MaxPooli  (0, 17, 17, 128)          0         \n ng2D)                                                           \n                                                                 \n flatten_46 (Flatten)        (0, 36992)                0         \n                                                                 \n dense_113 (Dense)           (0, 256)                  9470208   \n                                                                 \n dropout_63 (Dropout)        (0, 256)                  0         \n                                                                 \n dense_114 (Dense)           (0, 128)                  32896     \n                                                                 \n dropout_64 (Dropout)        (0, 128)                  0         \n                                                                 \n dense_115 (Dense)           (0, 2)                    258       \n                                                                 \n=================================================================\nTotal params: 9597506 (36.61 MB)\nTrainable params: 9597058 (36.61 MB)\nNon-trainable params: 448 (1.75 KB)\n_________________________________________________________________\n\n\n\n# Train the model\nhistory2 = model2.fit(train_ds,\n                      epochs=20,  # Adjust number of epochs as needed\n                      validation_data=validation_ds)\n\n# Evaluate the model on the test dataset\ntest_loss2, test_accuracy2 = model2.evaluate(test_ds)\n\nprint(\"Test Accuracy:\", test_accuracy2)\n\nEpoch 1/20\n146/146 [==============================] - 12s 59ms/step - loss: 10.2594 - accuracy: 0.5320 - val_loss: 6.6252 - val_accuracy: 0.5361\nEpoch 2/20\n146/146 [==============================] - 8s 57ms/step - loss: 4.7472 - accuracy: 0.5433 - val_loss: 3.2592 - val_accuracy: 0.5649\nEpoch 3/20\n146/146 [==============================] - 8s 58ms/step - loss: 2.6569 - accuracy: 0.5442 - val_loss: 2.1948 - val_accuracy: 0.5907\nEpoch 4/20\n146/146 [==============================] - 8s 58ms/step - loss: 1.9697 - accuracy: 0.5592 - val_loss: 1.8180 - val_accuracy: 0.5666\nEpoch 5/20\n146/146 [==============================] - 8s 57ms/step - loss: 2.0790 - accuracy: 0.5768 - val_loss: 1.6866 - val_accuracy: 0.6148\nEpoch 6/20\n146/146 [==============================] - 8s 58ms/step - loss: 1.6990 - accuracy: 0.5999 - val_loss: 1.5983 - val_accuracy: 0.6720\nEpoch 7/20\n146/146 [==============================] - 8s 58ms/step - loss: 1.4614 - accuracy: 0.6354 - val_loss: 1.3530 - val_accuracy: 0.6991\nEpoch 8/20\n146/146 [==============================] - 9s 59ms/step - loss: 1.4409 - accuracy: 0.6603 - val_loss: 1.3416 - val_accuracy: 0.6672\nEpoch 9/20\n146/146 [==============================] - 9s 63ms/step - loss: 1.2363 - accuracy: 0.6926 - val_loss: 1.1391 - val_accuracy: 0.7390\nEpoch 10/20\n146/146 [==============================] - 9s 61ms/step - loss: 1.1374 - accuracy: 0.7116 - val_loss: 1.0972 - val_accuracy: 0.7326\nEpoch 11/20\n146/146 [==============================] - 9s 58ms/step - loss: 1.0648 - accuracy: 0.7272 - val_loss: 1.0668 - val_accuracy: 0.7046\nEpoch 12/20\n146/146 [==============================] - 9s 59ms/step - loss: 1.0030 - accuracy: 0.7387 - val_loss: 1.0666 - val_accuracy: 0.5813\nEpoch 13/20\n146/146 [==============================] - 8s 58ms/step - loss: 0.9709 - accuracy: 0.7508 - val_loss: 0.9421 - val_accuracy: 0.7571\nEpoch 14/20\n146/146 [==============================] - 9s 58ms/step - loss: 0.9539 - accuracy: 0.7617 - val_loss: 0.9857 - val_accuracy: 0.7833\nEpoch 15/20\n146/146 [==============================] - 9s 58ms/step - loss: 0.9781 - accuracy: 0.7735 - val_loss: 0.9953 - val_accuracy: 0.7734\nEpoch 16/20\n146/146 [==============================] - 8s 58ms/step - loss: 0.9934 - accuracy: 0.7815 - val_loss: 1.0280 - val_accuracy: 0.7528\nEpoch 17/20\n146/146 [==============================] - 9s 59ms/step - loss: 0.9874 - accuracy: 0.7968 - val_loss: 1.6072 - val_accuracy: 0.5972\nEpoch 18/20\n146/146 [==============================] - 9s 59ms/step - loss: 1.0057 - accuracy: 0.7936 - val_loss: 1.0957 - val_accuracy: 0.7141\nEpoch 19/20\n146/146 [==============================] - 9s 58ms/step - loss: 1.0037 - accuracy: 0.8043 - val_loss: 1.0728 - val_accuracy: 0.7726\nEpoch 20/20\n146/146 [==============================] - 9s 59ms/step - loss: 1.0365 - accuracy: 0.8056 - val_loss: 1.0029 - val_accuracy: 0.8177\n37/37 [==============================] - 1s 14ms/step - loss: 1.0126 - accuracy: 0.8009\nTest Accuracy: 0.8009458184242249\n\n\n\n# Visualize training history\nplt.plot(history2.history['accuracy'], label='accuracy')\nplt.plot(history2.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n\n# Visualize training history\nplt.plot(history2.history['accuracy'], label='accuracy')\nplt.plot(history2.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\n\n\n\nUpon reviewing the learning curve before making adjustments to our model (model1), I noticed signs of overfitting. To address this, I introduced normalization, which helped stabilize the training process. However, the validation accuracy still displayed erratic behavior, indicating fluctuations across epochs. In response, I fine-tuned the learning rate to mitigate these fluctuations and further enhance model performance.\nThe accuracy of my model stabilized between 0.5320 and 0.8056 during training on the training set, 0.5361 to 0.8177 on validation set.\nThe result is better than that of model1.\nThere are not much of overfitting observed since the accuracy on validation date changes in a same rate as that on training set. There might be a little problem showed in the plot about the learning rate since there is a big jump."
  },
  {
    "objectID": "posts/HW5/index.html#model3-data-preprocessing-model2-1",
    "href": "posts/HW5/index.html#model3-data-preprocessing-model2-1",
    "title": "Doggie & Catie Classification",
    "section": "5. Model3 (Data Preprocessing + Model2)",
    "text": "5. Model3 (Data Preprocessing + Model2)\nOptimizing the training process often involves preprocessing the input data. For instance, scaling RGB pixel values from 0-255 to a standardized range like 0-1 or -1 to 1 can accelerate model convergence. By doing this preprocessing upfront, we ensure the model focuses on learning meaningful patterns, rather than adjusting to the data scale during training.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\ni = keras.Input(shape=(150, 150, 3)): creates an input layer for the neural network with a specified shape of (150, 150, 3). This shape indicates that the input data consists of images with a height and width of 150 pixels and 3 color channels (RGB).\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1): creates a rescaling layer using keras.layers.Rescaling. The purpose of this layer is to rescale the pixel values of the input images. - The scale parameter specifies the scaling factor applied to the pixel values. Here, the scale factor is set to 1 divided by 127.5, which rescales the pixel values from the original range of (0, 255) to (-1, 1). - The offset parameter specifies the offset applied to the pixel values after scaling. In this case, an offset of -1 is used, which shifts the scaled pixel values to be centered around -1.\nx = scale_layer(i): applies the rescaling layer (scale_layer) to the input layer (i). It takes the input images and rescales their pixel values according to the specified scale and offset parameters.\npreprocessor = keras.Model(inputs=i, outputs=x): This line creates a Keras model (preprocessor) by specifying the input and output layers. - The inputs argument specifies the input layer (i) of the model. - The outputs argument specifies the output layer (x) of the model, which is the result of applying the rescaling layer to the input images.\n\nmodel3 = models.Sequential([\n    preprocessor,\n    data_augmentation,\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n\n    layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n    layers.Dropout(0.5),\n\n    layers.Dense(128, activation='relu', kernel_regularizer='l2'),\n    layers.Dropout(0.5),\n\n    layers.Dense(2, activation='softmax')\n])\nmodel3.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nmodel3.summary()\n\nModel: \"sequential_70\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_3 (Functional)        (None, 150, 150, 3)       0         \n                                                                 \n sequential_69 (Sequential)  (None, 150, 150, 3)       0         \n                                                                 \n conv2d_83 (Conv2D)          (None, 148, 148, 32)      896       \n                                                                 \n batch_normalization_27 (Ba  (None, 148, 148, 32)      128       \n tchNormalization)                                               \n                                                                 \n max_pooling2d_83 (MaxPooli  (None, 74, 74, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_84 (Conv2D)          (None, 72, 72, 64)        18496     \n                                                                 \n batch_normalization_28 (Ba  (None, 72, 72, 64)        256       \n tchNormalization)                                               \n                                                                 \n max_pooling2d_84 (MaxPooli  (None, 36, 36, 64)        0         \n ng2D)                                                           \n                                                                 \n conv2d_85 (Conv2D)          (None, 34, 34, 128)       73856     \n                                                                 \n batch_normalization_29 (Ba  (None, 34, 34, 128)       512       \n tchNormalization)                                               \n                                                                 \n max_pooling2d_85 (MaxPooli  (None, 17, 17, 128)       0         \n ng2D)                                                           \n                                                                 \n flatten_43 (Flatten)        (None, 36992)             0         \n                                                                 \n dense_104 (Dense)           (None, 256)               9470208   \n                                                                 \n dropout_57 (Dropout)        (None, 256)               0         \n                                                                 \n dense_105 (Dense)           (None, 128)               32896     \n                                                                 \n dropout_58 (Dropout)        (None, 128)               0         \n                                                                 \n dense_106 (Dense)           (None, 2)                 258       \n                                                                 \n=================================================================\nTotal params: 9597506 (36.61 MB)\nTrainable params: 9597058 (36.61 MB)\nNon-trainable params: 448 (1.75 KB)\n_________________________________________________________________\n\n\n\n# Train the model\nhistory3 = model3.fit(train_ds,\n                      epochs=20,  # Adjust number of epochs as needed\n                      validation_data=validation_ds)\n\n# Evaluate the model on the test dataset\ntest_loss3, test_accuracy3 = model3.evaluate(test_ds)\n\nprint(\"Test Accuracy:\", test_accuracy3)\n\nEpoch 1/20\n146/146 [==============================] - 12s 60ms/step - loss: 10.0631 - accuracy: 0.5254 - val_loss: 5.9596 - val_accuracy: 0.5688\nEpoch 2/20\n146/146 [==============================] - 9s 59ms/step - loss: 4.3297 - accuracy: 0.5499 - val_loss: 2.7815 - val_accuracy: 0.5572\nEpoch 3/20\n146/146 [==============================] - 9s 59ms/step - loss: 2.2299 - accuracy: 0.5754 - val_loss: 1.7637 - val_accuracy: 0.6522\nEpoch 4/20\n146/146 [==============================] - 9s 59ms/step - loss: 1.7072 - accuracy: 0.6127 - val_loss: 1.5206 - val_accuracy: 0.6883\nEpoch 5/20\n146/146 [==============================] - 9s 59ms/step - loss: 1.4526 - accuracy: 0.6515 - val_loss: 1.2516 - val_accuracy: 0.6986\nEpoch 6/20\n146/146 [==============================] - 9s 59ms/step - loss: 1.2677 - accuracy: 0.6750 - val_loss: 1.1779 - val_accuracy: 0.7089\nEpoch 7/20\n146/146 [==============================] - 9s 59ms/step - loss: 1.1140 - accuracy: 0.7018 - val_loss: 1.0114 - val_accuracy: 0.7343\nEpoch 8/20\n146/146 [==============================] - 9s 60ms/step - loss: 1.0681 - accuracy: 0.7045 - val_loss: 1.0099 - val_accuracy: 0.7455\nEpoch 9/20\n146/146 [==============================] - 9s 59ms/step - loss: 0.9997 - accuracy: 0.7148 - val_loss: 0.9187 - val_accuracy: 0.7356\nEpoch 10/20\n146/146 [==============================] - 9s 59ms/step - loss: 0.9316 - accuracy: 0.7268 - val_loss: 0.8829 - val_accuracy: 0.7463\nEpoch 11/20\n146/146 [==============================] - 9s 59ms/step - loss: 0.9327 - accuracy: 0.7350 - val_loss: 0.9016 - val_accuracy: 0.7347\nEpoch 12/20\n146/146 [==============================] - 9s 60ms/step - loss: 0.9197 - accuracy: 0.7484 - val_loss: 0.8830 - val_accuracy: 0.7696\nEpoch 13/20\n146/146 [==============================] - 9s 60ms/step - loss: 0.9175 - accuracy: 0.7618 - val_loss: 0.8468 - val_accuracy: 0.7760\nEpoch 14/20\n146/146 [==============================] - 9s 59ms/step - loss: 0.8820 - accuracy: 0.7687 - val_loss: 0.8446 - val_accuracy: 0.7923\nEpoch 15/20\n146/146 [==============================] - 9s 59ms/step - loss: 0.8882 - accuracy: 0.7796 - val_loss: 0.8653 - val_accuracy: 0.8160\nEpoch 16/20\n146/146 [==============================] - 9s 60ms/step - loss: 0.9015 - accuracy: 0.7896 - val_loss: 0.8698 - val_accuracy: 0.8095\nEpoch 17/20\n146/146 [==============================] - 10s 66ms/step - loss: 0.9165 - accuracy: 0.7855 - val_loss: 1.0431 - val_accuracy: 0.7313\nEpoch 18/20\n146/146 [==============================] - 9s 60ms/step - loss: 0.9613 - accuracy: 0.7932 - val_loss: 1.1202 - val_accuracy: 0.7498\nEpoch 19/20\n146/146 [==============================] - 9s 63ms/step - loss: 1.0268 - accuracy: 0.7967 - val_loss: 0.9967 - val_accuracy: 0.8310\nEpoch 20/20\n146/146 [==============================] - 9s 65ms/step - loss: 1.0292 - accuracy: 0.8098 - val_loss: 0.9447 - val_accuracy: 0.8121\n37/37 [==============================] - 1s 17ms/step - loss: 0.9431 - accuracy: 0.8104\nTest Accuracy: 0.8104041218757629\n\n\n\n# Visualize training history\nplt.plot(history3.history['accuracy'], label='accuracy')\nplt.plot(history3.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\n\n\n\nNow that I think you can read pretty well about the layers of the model.\nThe accuracy of my model stabilized between 0.5254 and 0.8098 during training on the training set, 0.5688 to 0.8121 on validation set.\nThe result is better than that of model1 and similar to model2.\nThere are not much of overfitting observed since the accuracy on validation date changes in a same rate as that on training set."
  },
  {
    "objectID": "posts/HW5/index.html#model4-data-augmentation-transfer-learning-1",
    "href": "posts/HW5/index.html#model4-data-augmentation-transfer-learning-1",
    "title": "Doggie & Catie Classification",
    "section": "6. Model4 (Data Augmentation + Transfer Learning)",
    "text": "6. Model4 (Data Augmentation + Transfer Learning)\nTransfer learning is a machine learning technique where a model trained on one task is reused or adapted as a starting point for a second related task. In transfer learning, knowledge gained while solving one problem is applied to a different but related problem. This approach is particularly useful in scenarios where the second task has less data available for training or where training a model from scratch might be time-consuming or resource-intensive.\nSo far, we’ve been training models for distinguishing between cats and dogs from scratch. In some cases, however, someone might already have trained a model that does a related task, and might have learned some relevant patterns. For example, folks train machine learning models for a variety of image recognition tasks. Maybe we could use a pre-existing model for our task?\nTo do this, we need to first access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model.\nMobileNetV3Large is a convolutional neural network architecture designed for efficient and accurate image classification tasks. It is part of the MobileNet family of models developed by Google, which are specifically optimized for mobile and embedded devices.\nMobileNetV3Large builds upon the success of its predecessors, MobileNetV1 and MobileNetV2, by introducing novel architectural changes and optimization techniques aimed at improving performance while maintaining efficiency. The “Large” variant of MobileNetV3 typically refers to a larger and more powerful version of the model, capable of handling more complex tasks and providing higher accuracy.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 1s 0us/step\n\n\nThis code snippet involves setting up and configuring a pre-trained MobileNetV3Large model as a feature extractor using TensorFlow’s Keras API.\nLet’s break it down step by step:\n\nSetting Input Shape:\n\nIMG_SHAPE = (150, 150, 3): defines the shape of the input images. Here, (150, 150, 3) indicates images with a height and width of 150 pixels and 3 color channels (RGB).\n\nLoading Pre-trained MobileNetV3Large Model:\n\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE, include_top=False, weights='imagenet'): This line loads the MobileNetV3Large model pre-trained on the ImageNet dataset.\n\ninput_shape=IMG_SHAPE: Specifies the input shape of the images to be fed into the model.\ninclude_top=False: Indicates that we don’t want to include the fully connected layers (top layers) of the pre-trained model, as we intend to use it as a feature extractor.\nweights='imagenet': Specifies that we want to load the weights pre-trained on the ImageNet dataset.\n\n\nFreezing Base Model Weights:\n\nbase_model.trainable = False: freezes the weights of the pre-trained MobileNetV3Large model. By setting trainable to False, we ensure that the weights of the base model remain fixed during training, preventing them from being updated.\n\nDefining Input Layer:\n\ni = keras.Input(shape=IMG_SHAPE): defines an input layer for the model with the specified input shape.\n\nConnecting Input and Base Model:\n\nx = base_model(i, training=False): passes the input layer i through the pre-trained MobileNetV3Large model (base_model) with training=False. This means that the base model will run in inference mode, without updating its weights.\n\nCreating Base Model Layer:\n\nbase_model_layer = keras.Model(inputs=i, outputs=x): creates a Keras model (base_model_layer) with the input layer i and the output layer x. The output of this model will be the features extracted by the MobileNetV3Large model from the input images.\n\n\n\nmodel4 = models.Sequential([\n  base_model_layer,\n\n  layers.Flatten(),\n  # Output layer\n  layers.Dense(1, activation='sigmoid')\n])\n\nmodel4.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n\n# Display model summary\nmodel4.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model (Functional)          (None, 5, 5, 960)         2996352   \n                                                                 \n flatten (Flatten)           (None, 24000)             0         \n                                                                 \n dense (Dense)               (None, 1)                 24001     \n                                                                 \n=================================================================\nTotal params: 3020353 (11.52 MB)\nTrainable params: 24001 (93.75 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\nLayer Information: - The first layer is named “model” and is described as a Functional layer. It has an output shape of (None, 5, 5, 960), indicating that it produces feature maps with dimensions 5x5 and 960 channels. - The second layer is a Flatten layer, which converts the multi-dimensional output of the previous layer into a one-dimensional vector. It has an output shape of (None, 24000). - The third layer is a Dense layer with a single neuron, producing a scalar output. Its output shape is (None, 1).\nParameter Count: - “Total params” indicates the total number of parameters (weights and biases) in the model, which is 3,020,353 (approximately 3 million parameters). This number is a sum of trainable and non-trainable parameters. - “Trainable params” indicates the number of parameters that are trainable during the training process, which is 24,001 parameters. - “Non-trainable params” indicates the number of parameters that are not trainable, typically associated with pre-trained layers or frozen layers. In this case, there are 2,996,352 non-trainable parameters.\nOverall, the parameter use is huge.\nLike before:\n\nSequential Model Creation:\n\nmodel4 = models.Sequential([...]): This line initializes a sequential model (Sequential) which is a linear stack of layers. Layers are added one by one in sequence.\n\nAdding Layers to the Model:\n\nbase_model_layer: The pre-trained MobileNetV3Large model (base_model_layer) that we defined earlier is added as the first layer in the sequential model. This serves as a feature extractor, providing high-level features extracted from the input images.\nlayers.Flatten(): This layer is added to flatten the output of the previous layer (which is typically a multi-dimensional tensor) into a one-dimensional vector. This prepares the data for the subsequent dense (fully connected) layers.\nlayers.Dense(1, activation='sigmoid'): This is the output layer of the model, consisting of a single neuron with a sigmoid activation function. It produces a binary classification output (0 or 1) indicating the probability that the input image belongs to a particular class (e.g., cat or dog).\n\nModel Compilation:\n\nmodel4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']): This line compiles the model, configuring its training process.\n\noptimizer='adam': Adam is selected as the optimizer for gradient descent, which is widely used for its efficiency and effectiveness in training neural networks.\nloss='binary_crossentropy': Binary crossentropy is chosen as the loss function, which is suitable for binary classification tasks where the output is either 0 or 1.\nmetrics=['accuracy']: During training, the model’s performance will be evaluated based on accuracy, which measures the proportion of correctly classified samples.\n\n\n\n\nhistory4 = model4.fit(train_ds,\n                      epochs=20,  # Adjust number of epochs as needed\n                      validation_data=validation_ds)\n\n# Evaluate the model on the test dataset\ntest_loss4, test_accuracy4 = model4.evaluate(test_ds)\n\nprint(\"Test Accuracy:\", test_accuracy4)\n\nEpoch 1/20\n146/146 [==============================] - 14s 62ms/step - loss: 0.2100 - accuracy: 0.9503 - val_loss: 0.1657 - val_accuracy: 0.9682\nEpoch 2/20\n146/146 [==============================] - 6s 40ms/step - loss: 0.0524 - accuracy: 0.9855 - val_loss: 0.1651 - val_accuracy: 0.9764\nEpoch 3/20\n146/146 [==============================] - 6s 40ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.1684 - val_accuracy: 0.9751\nEpoch 4/20\n146/146 [==============================] - 6s 41ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.1697 - val_accuracy: 0.9738\nEpoch 5/20\n146/146 [==============================] - 6s 40ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.1758 - val_accuracy: 0.9742\nEpoch 6/20\n146/146 [==============================] - 6s 41ms/step - loss: 8.7343e-04 - accuracy: 0.9998 - val_loss: 0.1844 - val_accuracy: 0.9712\nEpoch 7/20\n146/146 [==============================] - 6s 42ms/step - loss: 6.2764e-04 - accuracy: 0.9999 - val_loss: 0.1802 - val_accuracy: 0.9738\nEpoch 8/20\n146/146 [==============================] - 6s 41ms/step - loss: 1.5256e-04 - accuracy: 1.0000 - val_loss: 0.1757 - val_accuracy: 0.9746\nEpoch 9/20\n146/146 [==============================] - 6s 41ms/step - loss: 5.0254e-05 - accuracy: 1.0000 - val_loss: 0.1754 - val_accuracy: 0.9746\nEpoch 10/20\n146/146 [==============================] - 6s 40ms/step - loss: 3.9323e-05 - accuracy: 1.0000 - val_loss: 0.1752 - val_accuracy: 0.9746\nEpoch 11/20\n146/146 [==============================] - 6s 41ms/step - loss: 3.4001e-05 - accuracy: 1.0000 - val_loss: 0.1751 - val_accuracy: 0.9751\nEpoch 12/20\n146/146 [==============================] - 6s 41ms/step - loss: 3.0114e-05 - accuracy: 1.0000 - val_loss: 0.1750 - val_accuracy: 0.9751\nEpoch 13/20\n146/146 [==============================] - 6s 41ms/step - loss: 2.7043e-05 - accuracy: 1.0000 - val_loss: 0.1748 - val_accuracy: 0.9751\nEpoch 14/20\n146/146 [==============================] - 6s 41ms/step - loss: 2.4513e-05 - accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9755\nEpoch 15/20\n146/146 [==============================] - 6s 41ms/step - loss: 2.2374e-05 - accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9755\nEpoch 16/20\n146/146 [==============================] - 6s 41ms/step - loss: 2.0533e-05 - accuracy: 1.0000 - val_loss: 0.1746 - val_accuracy: 0.9755\nEpoch 17/20\n146/146 [==============================] - 6s 41ms/step - loss: 1.8924e-05 - accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9755\nEpoch 18/20\n146/146 [==============================] - 6s 41ms/step - loss: 1.7504e-05 - accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9759\nEpoch 19/20\n146/146 [==============================] - 6s 42ms/step - loss: 1.6240e-05 - accuracy: 1.0000 - val_loss: 0.1744 - val_accuracy: 0.9759\nEpoch 20/20\n146/146 [==============================] - 6s 42ms/step - loss: 1.5106e-05 - accuracy: 1.0000 - val_loss: 0.1744 - val_accuracy: 0.9759\n37/37 [==============================] - 1s 33ms/step - loss: 0.1828 - accuracy: 0.9716\nTest Accuracy: 0.9716250896453857\n\n\n\n# Visualize training history\nplt.plot(history4.history['accuracy'], label='accuracy')\nplt.plot(history4.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between accuracy: 0.9503 and 1.0000 during training on the training set, 0.9682 to 0.9759 on validation set.\nThe result is better than that of model1, model2, and model3\nThere is overfitting since the accuracy on training set is better than that on the validation data set and not to mention that the accuracy achieved 1 at the end on the training set.\nSince model4 that uses MobileNetV3Large performs best, the test score of model4 on test set is 0.9716250896453857!!!"
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Movie or TV Show Recommendation",
    "section": "",
    "text": "Unleashing the Power of Scrapy: Scraping and Recommending Movies and TV Shows from TMDB\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/HW2/index.html#intro",
    "href": "posts/HW2/index.html#intro",
    "title": "Movie or TV Show Recommendation",
    "section": "Intro",
    "text": "Intro\nWelcome to the world of web scraping and data-driven recommendations! In today’s digital era, where information is abundant and readily accessible, leveraging the power of data extraction tools like Scrapy opens up a realm of possibilities. In this tutorial, we’ll embark on a journey to scrape data from TMDB (The Movie Database), a treasure trove of information about movies and TV shows. But we’re not stopping there; we’ll dive deeper into the web of interconnected data, exploring the crew members’ profiles and unraveling the threads of their acting history.\nUsing Scrapy, a powerful and versatile web crawling framework in Python, we’ll navigate through TMDB’s vast database with ease. Our mission? To gather detailed insights into specific movie and TV show pages, extract information about the cast and crew, and then traverse the web of actor profiles to unearth their acting repertoire. By analyzing the overlaps in their performances, we’ll construct a recommendation engine that suggests similar movies or TV shows based on shared talent.\nWhether you’re a data enthusiast, a budding web developer, or simply curious about the magic behind personalized recommendations, this tutorial will equip you with the tools and knowledge to embark on your scraping and recommendation journey. So, let’s roll up our sleeves, fire up our code editors, and dive headfirst into the fascinating world of Scrapy and TMDB scraping!"
  },
  {
    "objectID": "posts/HW2/index.html#table-of-contents",
    "href": "posts/HW2/index.html#table-of-contents",
    "title": "Movie or TV Show Recommendation",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1. Web Scraper\n1.1 Setup\n    1.1.1 Setting up a Scrapy Project\n    1.1.2 Test Scrapy for TMDB Scraping\n1.2 Implementation\n\n\n2. Visualization and Recommendation"
  },
  {
    "objectID": "posts/HW2/index.html#web-scraper-1",
    "href": "posts/HW2/index.html#web-scraper-1",
    "title": "Movie or TV Show Recommendation",
    "section": "1. Web Scraper",
    "text": "1. Web Scraper\n\n1.1 Setup\n\n1.1.1 Setting up a Scrapy Project\nIn this section, we’ll walk through the process of setting up a Scrapy project to scrape data from TMDB (The Movie Database). By following these steps, you’ll be able to create a structured project environment ready for web scraping.\n\nStep 1: Activate Your Conda Environment\nOpen a terminal or command prompt and activate your Conda environment where you want to set up your Scrapy project. Type the following command:\nconda activate PIC16B-24W\nThis command activates the specified Conda environment named “PIC16B-24W”. Ensure that you have created and configured this environment beforehand.(You can choose whatever environment you have)\n\n\nStep 2: Create a Scrapy Project\nNow, we’ll create a Scrapy project named “TMDB_scraper”. In your terminal, type the following command:\nscrapy startproject TMDB_scraper\nThis command initializes a new Scrapy project named “TMDB_scraper”. Scrapy will generate several files and directories within this project. When you create a Scrapy project using the scrapy startproject command with the name “TMDB_scraper”, the generated project directory (TMDB_scraper) typically contains the following files and directories:\n\nscrapy.cfg: This file is the configuration file for your Scrapy project. It contains settings for Scrapy deployment and other global configurations.\nTMDB_scraper/ (directory inside TMDB_scraper/): This is the Python package directory containing your project’s code.\n\nitems.py: This file defines the data structure (items) that your spider will extract during scraping. You define the fields you want to extract from the website here.\nmiddlewares.py: This file contains the middlewares used by Scrapy, such as user-agent rotation, proxy usage, etc.\npipelines.py: This file contains the pipelines used by Scrapy for processing scraped items. Pipelines are used to perform tasks like cleaning, validation, and storing data.\nsettings.py: This file contains Scrapy project settings, such as user-agent, download delay, etc. You can customize these settings according to your requirements.\nspiders/ (directory): This directory contains your spider scripts.\n\n__init__.py: This file makes the spiders directory a Python package.\n\n\n\nThese are the essential files and directories that are created when you initialize a Scrapy project. Depending on your specific requirements and configurations, your project might have additional files or directories.\n\n\nStep 3: Navigate to Your Project Directory\nNavigate to the newly created project directory “TMDB_scraper” using the cd command:\ncd TMDB_scraper\nThis command changes your current directory to the “TMDB_scraper” directory, where your Scrapy project files are located. From here, you’ll be able to configure and customize your scraping scripts as needed.\nCongratulations! You’ve successfully set up a Scrapy project named “TMDB_scraper” ready for web scraping.\n\n\n\n1.1.2 Test Scrapy for TMDB Scraping\nWe’ll start by configuring settings in the settings.py file to prevent excessive data downloads and avoid being blocked by the website. Then, we’ll explore using the Scrapy shell for interactive testing and debugging.\n\nStep 1: Modify settings.py to Limit Page Count\nOpen the settings.py file located in your Scrapy project directory (TMDB_scraper) using a text editor. Add the following line to limit the number of pages to be scraped:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading too much data during testing. Remember to remove this line later for full-scale scraping.\n\n\nStep 2: Configure User-Agent to Mimic a Web Browser\nTo avoid getting blocked by TMDB, we’ll mimic a web browser’s behavior by modifying the USER_AGENT in the settings.py file. Replace the existing USER_AGENT line with the following:\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'\nThis user-agent string resembles a typical web browser’s user-agent, which makes it less likely for TMDB to block our requests.\nOr you can use other methods:\nhttps://doc.scrapy.org/en/latest/topics/practices.html#avoiding-getting-banned\nhttps://scrapeops.io/python-scrapy-playbook/scrapy-403-unhandled-forbidden-error/\nhttps://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/\nhttps://scrapingrobot.com/blog/most-common-user-agents/\n\n\nStep 3: Test Scraping with Scrapy Shell\nNow, let’s test our scraping capability using Scrapy shell. Open a terminal or command prompt and navigate to your Scrapy project directory (TMDB_scraper). Then, run the following command:\nscrapy shell https://www.themoviedb.org/movie/157336-interstellar/cast\nReplace the URL with the desired TMDB page to scrape. Here I used Interstellar. This command launches the Scrapy shell with the specified user-agent and the provided URL.\nStay tuned for the next part of this tutorial series where we’ll explore writing spider scripts and extracting data from TMDB using Scrapy.\n\n\n\n\n1.2 Website Navigation\nBefore writing the scraper method, we need to have a basic idea of what the website looks like and what information we need to scrap from the website.\nStart by visiting the website you want to scrape. In our case, it’s TMDB (The Movie Database). Browse through different pages, such as movie pages and the Full Cast & Crew page, to get familiar with the layout and structure.\n\nExplore the Website\nStart by visiting the TMDB website and navigating to the movie page of interest. For our example, let’s consider the movie “Interstellar”. You can find its page at:\nInterstellar Movie Page\nThe url is: https://www.themoviedb.org/movie/157336-interstellar\nThis is the url that we want to start with in the __init__, where we define the start url, that can be modified with whatever movie or tv show names after the slash movie.\n\n\nIdentify Relevant Pages\nOnce on the movie page, explore the different sections and links available. In our case, we’re interested in extracting information about the movie’s cast. We’ll navigate to the Full Cast & Crew page by appending “/cast” to the movie page URL:\nInterstellar Cast Page\nThe url is: https://www.themoviedb.org/movie/157336-interstellar/cast\nNote that there is a ‘cast’ was added at the end of the initial url and that is what we are going to do in the parse method.\nFor the other 2 method parse_full_credits and parse_actor_page we need to dig deeper to the page construction. Here I am just going to introduce the overall structer of the TmdbSpider class.\n\n\nPlan Scraping Strategy\nNow that we’ve identified the relevant pages, we can plan our scraping strategy. We’ll create a Scrapy spider with the following methods:\n\nparse: Navigates to the Full Cast & Crew page and calls parse_full_credits.\nparse_full_credits: Extracts information about the cast members and navigates to each actor’s personal page.\nparse_actor_page: Extracts the actor’s acting history from their personal page.\n\nUnderstanding the structure of the website you’re scraping is essential for developing an effective scraping method. By exploring different pages, identifying relevant information, and planning your scraping strategy, you’ll be better prepared to write the scraper method and extract the desired data.\nOnce we’ve outlined the blueprint of our class, the next step is bringing our plan to life. In web scraping, this primarily involves inspecting and extracting data. Once we understand the data structure, our task is to pinpoint its location within the HTML and then select the most appropriate method for extraction.\n\n\nInspect Elements\nMost modern web browsers offer developer tools that allow you to inspect the HTML and CSS of web pages. Right-click on any element of interest (e.g., movie title, actor name) and select “Inspect” or “Inspect Element” from the context menu.\n\n\n\nimage.png\n\n\nYou can see it at the bottom!\n\n\nIdentify Data to Scrape\nOnce you’re in the developer tools, explore the HTML structure to identify the elements containing the data you want to scrape. Look for unique identifiers such as class names, IDs, or tag names that can help you locate the desired information. I will guide you with the details later, don’t worry.\n\n\nPlan Scraping Strategy\nBased on the information you want to extract and its location in the HTML structure, plan your scraping strategy. Determine which Scrapy selectors (e.g., CSS selectors, XPath expressions) you’ll use to target and extract the data.\nYou might want to get familar with these before we proceed:\nRequest objects: https://docs.scrapy.org/en/latest/topics/request-response.html#request-objectsLinks\nResponse objects: https://docs.scrapy.org/en/latest/topics/request-response.html#response-objectsLinks\nSelector objects: https://docs.scrapy.org/en/latest/topics/selectors.html#scrapy.selector.Selector\n\n\n\n1.3 Implementation\nIn this section, we’ll create a Scrapy spider named tmdb_spider.py to scrape data from TMDB (The Movie Database) for a specific movie. We’ll set it up to accept the movie’s subdirectory as a command-line argument for easy customization.\n\nStep 1: Create tmdb_spider.py\nInside the spiders directory of your Scrapy project (TMDB_scraper), create a new Python file named tmdb_spider.py. You can do this manually or by running the following command in your terminal:\ntouch TMDB_scraper/spiders/tmdb_spider.py\nOpen this file in a text editor and add the following lines:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThe __init__ method in the spider class initializes the spider with the provided subdirectory argument. This allows us to customize the URL to scrape based on the movie’s subdirectory.\nTo run the spider and scrape data for a specific movie, use the following command:\nscrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nReplace 671-harry-potter-and-the-philosopher-s-stone with the subdirectory of the movie you want to scrape. This command instructs Scrapy to crawl the specified URL and save the scraped data to a CSV file named movies.csv.\nYou’ve successfully created a Scrapy spider named tmdb_spider.py configured to scrape data from TMDB for a specific movie. By providing the movie’s subdirectory as a command-line argument, you can easily customize the spider to scrape data for different movies.\nHere’s a basic outline of the TmdbSpider class:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        # Navigate to the Full Cast & Crew page\n        pass\n\n    def parse_full_credits(self, response):\n        # Extract information about the cast members\n        # Navigate to each actor's personal page\n        # Call parse_actor_page for each actor\n        pass\n\n    def parse_actor_page(self, response):\n        # Extract the actor's acting history from their personal page\n        pass\n\n\n1.2.1 parse(self, response)\nIn the parse method: - We construct the URL for the Full Cast & Crew page by appending “cast” to the movie page’s URL. - We then yield a scrapy.Request object with the URL of the Full Cast & Crew page and specify parse_full_credits as the callback method.\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n    def parse(self, response):\n        \"\"\"\n        Parse method to navigate to the Full Cast & Crew page and call parse_full_credits.\n\n        Args:\n            response (scrapy.http.Response): The response object containing the web page data.\n\n        Yields:\n            scrapy.Request: A scrapy request object to navigate to the Full Cast & Crew page.\n\n        \"\"\"\n        # Navigate to the Full Cast & Crew page by adding \"cast\" after the start URL\n        full_credits_url = response.url + \"cast\"\n        \n        # After navigation, call parse_full_credits\n        yield scrapy.Request(full_credits_url, callback=self.parse_full_credits)\nLet’s break down each part of the provided code snippet:\n\ndef parse(self, response): This is a method definition in a Scrapy spider class. It’s the default method called by Scrapy to handle responses downloaded for each request made.\nfull_credits_url = response.url + \"cast\": This line of code constructs the URL for the Full Cast & Crew page. It takes the current URL of the response object and appends “cast” to it, which is the relative URL for the Full Cast & Crew page.\nyield scrapy.Request(full_credits_url, callback=self.parse_full_credits): This line of code yields a new Scrapy Request object. Scrapy uses Request and Response objects for crawling web sites.Typically, Request objects are generated in the spiders and pass across the system until they reach the Downloader, which executes the request and returns a Response object which travels back to the spider that issued the request. This initiates a new request to the full_credits_url URL and specifies self.parse_full_credits as the callback function to handle the response.\n\nIn summary, this parse method is responsible for constructing a request to navigate to the Full Cast & Crew page and then calling the parse_full_credits method to handle the response from that page.\n\n\n1.2.2 parse_full_credits(self, response)\nThe overall code looks like this:\ndef parse_full_credits(self, response):\n    \"\"\"\n    Parse method to extract actor URLs from the Full Cast & Crew page and yield requests to parse actor pages.\n\n    Args:\n        response (scrapy.http.Response): The response object containing the Full Cast & Crew page data.\n\n    Yields:\n        scrapy.Request: A scrapy request object to parse each actor's page.\n\n    \"\"\"\n    cast_list = response.css('h3:contains(\"Cast\") + ol li div.info')\n    for actor in cast_list:\n        actor_url = actor.css(\"a::attr(href)\").get()\n        actor_page_url = response.urljoin(actor_url)\n        yield scrapy.Request(actor_page_url, callback=self.parse_actor_page)\n\ndef parse_full_credits(self, response): This is a method definition within a Scrapy spider class. It’s intended to handle the response from the Full Cast & Crew page.\ncast_list = response.css('h3:contains(\"Cast\") + ol li div.info'): This line of code uses CSS selectors to locate the elements containing information about each actor in the cast. It selects the &lt;div&gt; elements with the class “info” that are within list items (&lt;li&gt;) following an &lt;ol&gt; element immediately after an &lt;h3&gt; element containing the text “Cast” since we only want the Cast section instead of Crew. For how to use selector, make sure go through this: https://docs.scrapy.org/en/latest/topics/selectors.html\n\n\n\n\nimage.png\n\n\nNow we have all the casts information, we need to extract each person’s page url by iteration.\n\nfor actor in cast_list:: This loop iterates over each element in the cast_list.\nactor_url = actor.css(\"a::attr(href)\").get(): Within each iteration of the loop, this line of code extracts the URL of the actor’s personal page. It selects the value of the “href” attribute of the &lt;a&gt; element within the current actor’s &lt;div&gt; as you see in the screenshot above.\nactor_page_url = response.urljoin(actor_url): This line constructs the absolute URL for the actor’s personal page by joining the relative URL extracted in the previous step with the base URL of the current response.\nyield scrapy.Request(actor_page_url, callback=self.parse_actor_page): Finally, this line yields a new Scrapy Request object for each actor’s personal page. It specifies self.parse_actor_page as the callback function to handle the response from each actor’s page.\n\nIn summary, this parse_full_credits method is responsible for extracting the URLs of each actor’s personal page from the Full Cast & Crew page and yielding requests to scrape data from those individual pages using the parse_actor_page method.\n\n\n1.2.3 parse_actor_page(self, response)\nIn this tutorial, we’ll cover how to scrape data from an actor’s personal page, focusing on extracting their past or current movies or TV shows.\nThe overall code looks like this:\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parse method to extract acting credits for an actor from their personal page.\n\n    Args:\n        response (scrapy.http.Response): The response object containing the actor's personal page data.\n\n    Yields:\n        dict: A dictionary containing the actor's name and the movie or TV show they acted in.\n\n    \"\"\"\n    # Yield a dictionary for each movie or TV show the actor has worked in an \"Acting\" role\n    actor_name = response.css('div.title h2.title a::text').get()\n    acting_table = response.css('h3:contains(\"Acting\") + table.card.credits')\n    name_list = acting_table.css(\"a.tooltip\")\n\n    for name in name_list:\n        movie_or_TV_name = name.css(\"bdi::text\").get() \n        yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\n\n\nimage.png\n\n\n\nactor_name = response.css('div.title h2.title a::text').get(): This line extracts the actor’s name from the title of their personal page, which is under &lt;div class='title'&gt;, then under &lt;h2 class='title'&gt; then inside the &lt;a&gt;``&lt;/a&gt; here we can get the name of the actor and store it in the actor_name, which also can be done in pervious method, feel free to try it out!\n\n\n\n\nimage.png\n\n\n\nacting_table = response.css('h3:contains(\"Acting\") + table.card.credits'): This line selects the table containing the actor’s acting credits. It looks for the heading “Acting” and then selects the adjacent table with the class “card” and “credits”. Note that there are several tables under the &lt;div class='credits_list'&gt; make sure you only capture the table with Acting as the header.\n\n\n\n\nimage.png\n\n\n\nname_list = acting_table.css(\"a.tooltip\"): This line selects all the name of a movie or TV show in which the actor has acted.\nfor name in name_list:: This loop iterates over each link in the name_list since we only want the text part of the list.\n\n\n\n\nimage.png\n\n\n\nmovie_or_TV_name = name.css(\"bdi::text\").get(): Within each iteration of the loop, this line extracts the text representing the name of the movie or TV show from the link.\nyield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}: Finally, this line yields a dictionary containing the actor’s name and the name of the movie or TV show they acted in.\n\nFinally, we are done with the scraping part. We’ve learned how to extract an actor’s acting roles from their personal page using Scrapy. By understanding the structure of the actor’s page and using appropriate CSS selectors, we were able to extract the desired information effectively.\nNow try to run this in terminal to implement the scraper:\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=157336-interstellar\nFeel free to replace any Movie or TV show you like!\nThis would give you a results.csv scraped. Before we proceed, remember to remove the restriction in the settings.py."
  },
  {
    "objectID": "posts/HW2/index.html#visualization-and-recommendation-1",
    "href": "posts/HW2/index.html#visualization-and-recommendation-1",
    "title": "Movie or TV Show Recommendation",
    "section": "2. Visualization and Recommendation",
    "text": "2. Visualization and Recommendation\n\n# import libraries\nimport pandas as pd\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom plotly import express as px\n\nLet’s import this file into our notebook to see what we got!\n\ndf = pd.read_csv(\"results.csv\")\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2063 entries, 0 to 2062\nData columns (total 2 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   actor             2063 non-null   object\n 1   movie_or_TV_name  2063 non-null   object\ndtypes: object(2)\nmemory usage: 32.4+ KB\n\n\nWe’ve got 2063 movies and TV shows!\n\ndf.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nWilliam Patrick Brown\nInterstellar\n\n\n1\nWilliam Patrick Brown\nMighty Med\n\n\n2\nWilliam Patrick Brown\nFriends with Benefits\n\n\n3\nWilliam Patrick Brown\nChuck\n\n\n4\nMatthew McConaughey\n2024\n\n\n\n\n\n\n\nLet’s determine the overlaps between these movies and TV shows to form the basis of our recommendation. The logic is straightforward: we’ll analyze the movies or TV shows that feature the actors from your favorite movie or show in the highest quantity.\n\nrecmd_movies_or_TV = pd.DataFrame(df.movie_or_TV_name.value_counts().reset_index())\n\ndf.movie_or_TV_name.value_counts() we extract the movie_or_TV_name column from the dataframe and get unique values of this column with its count through .value_counts(), which would give us a Series. In order to make use of this Series, we reset the index through .reset_index() and transfer it into another data frame via pd.DataFrame().\n\n# original column name are movie_or_TV_name and count, we change them to understand it easier into movie names and number of shared actors\nrecmd_movies_or_TV = recmd_movies_or_TV.rename(columns={'movie_or_TV_name':'movie names', 'count':'number of shared actors'})\n\n\nrecmd_movies_or_TV['number of shared actors'].value_counts()\n\nnumber of shared actors\n1     1602\n2      107\n3       18\n4        9\n7        6\n6        6\n9        2\n8        2\n35       1\n10       1\nName: count, dtype: int64\n\n\nWhat we observe here is that 1602 rows of the data correspond to a single actor. This indicates that these actors are the only ones in the Interstellar crew who have performed in another movie or show, without any other members of the Interstellar crew. The same interpretation applies to the subsequent results, and the output is sorted based on the count.\nNext, we may want to split the data to visualize it in two parts, as we are not interested in the solo performances for those 1602 rows, or in movies or TV shows that contain only 2, 3, or 4 members of the full crew. You can define the splitting criteria according to your own preferences; here, I am simply dividing it into recmd_more and recmd_less based on the presence of 4 or more shared actors.\n\nrecmd_more = recmd_movies_or_TV[recmd_movies_or_TV['number of shared actors'] &gt;= 4 ]\n\n\nrecmd_less = recmd_movies_or_TV[recmd_movies_or_TV['number of shared actors'] &lt; 4 ]\n\nWe create a horizontal bar plot using Plotly Express. We specify the DataFrame recmd_more as the data source, set the ‘number of shared actors’ as the x-axis, ‘movie names’ as the y-axis, and provide additional parameters for orientation and text.\nimport plotly.express as px\n\nfig = px.bar(recmd_more, x='number of shared actors', y='movie names', orientation='h',\n             text='number of shared actors', title='Movie Names and Number of Shared Actors &gt;= 5')\nWe can adjust the position of the text annotations on the bars using the update_traces method.\nfig.update_traces(textposition='outside')\nNext, we customize the layout of the plot by specifying axis titles, ordering categories, adjusting tick marks, and setting the height of the plot.\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='total ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=600\n)\nBy following these steps, you can create a customized horizontal bar plot using Plotly Express in Python. Experiment with different parameters and settings to achieve the desired visualization for your data.\nFeel free to further customize the plot according to your specific requirements, such as adjusting colors, fonts, or adding annotations.\nHere is the full code:\n\nfig = px.bar(recmd_more, x='number of shared actors', y='movie names', orientation='h',\n             text='number of shared actors', title='Movie/Show Names and Number of Shared Actors &gt;= 5')\n\nfig.update_traces(textposition='outside')\n\n# Customizing the layout\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='total ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=500\n)\n\n                                                \n\n\nAs we can see from the plot, Interstellar would of course be the top one we would recommend the it appears to be Saturday Night Live, which shares 10 actors. The View, Late Night with Seth Meyers share 9 actors with Interstellar. Well this would be a tool to do the recommand that is super obvious for the reader or the viewer.\n\nplt.figure(figsize=(8, 4))\nsns.histplot(recmd_less['number of shared actors'], bins=range(1, 6), kde=False, color='skyblue')\nplt.xlabel('Number of Shared Actors')\nplt.ylabel('Number of Movies/TV Shows')\nplt.title('Distribution of Movies/TV Shows with Less Than 5 Shared Actors')\nplt.show()\n\n\n\n\n\n\n\n\nSince we cannot plot the thousands rows, we can visualize it through histplot though this provides not that much of the information we need for recommendation.\nNow we are going to see that for each actor, the percentage of sole performance on the overall acting histroy, sole performance here means not with any other actors in the Interstellar crew.\nBefore we begin, let’s define the required columns in our DataFrame:\n\nisDup: A boolean column indicating whether the movie or TV show is unique. If True, it means there is a duplicate movie or TV show in the DataFrame, indicating it’s not a sole performance.\ndup_cnt: The count of occurrences where isDup is True or False, grouped by actor and isDup.\ntotal_cnt: The total count of all acting roles for each actor.\n\nCalculate isDup Column\nWe’ll calculate the isDup column using the duplicated method, which marks duplicate entries as True and unique entries as False.\ndf['isDup'] = df.duplicated(subset=['movie_or_TV_name'], keep=False)\nCalculate dup_cnt and total_cnt Columns\nNext, we’ll calculate the dup_cnt and total_cnt columns using the groupby method along with the transform function to get the count of occurrences for each actor.\ndf['dup_cnt'] = df.groupby([\"actor\", 'isDup'])['movie_or_TV_name'].transform('count')\ndf['total_cnt'] = df.groupby([\"actor\"])['movie_or_TV_name'].transform('count')\n\ndf['isDup'] = df.duplicated(subset=['movie_or_TV_name'], keep=False)\n\ndf['dup_cnt'] = df.groupby([\"actor\", 'isDup'])['movie_or_TV_name'].transform('count')\n\ndf['total_cnt'] = df.groupby([\"actor\"])['movie_or_TV_name'].transform('count')\n\n\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nisDup\ndup_cnt\ntotal_cnt\n\n\n\n\n0\nWilliam Patrick Brown\nInterstellar\nTrue\n2\n4\n\n\n1\nWilliam Patrick Brown\nMighty Med\nFalse\n2\n4\n\n\n2\nWilliam Patrick Brown\nFriends with Benefits\nTrue\n2\n4\n\n\n3\nWilliam Patrick Brown\nChuck\nFalse\n2\n4\n\n\n4\nMatthew McConaughey\n2024\nFalse\n91\n127\n\n\n...\n...\n...\n...\n...\n...\n\n\n2058\nJohn Lithgow\nDealing: Or the Berkeley-to-Boston Forty-Brick...\nFalse\n151\n189\n\n\n2059\nJohn Lithgow\nGreat Performances\nFalse\n151\n189\n\n\n2060\nJohn Lithgow\nTony Awards\nTrue\n38\n189\n\n\n2061\nJohn Lithgow\nToday\nTrue\n38\n189\n\n\n2062\nJohn Lithgow\nHallmark Hall of Fame\nTrue\n38\n189\n\n\n\n\n2063 rows × 5 columns\n\n\n\nNow we contruct the plot: Let’s break down what each line of code does:\npx.bar(df, x='total_cnt', y='actor', orientation='h', color='isDup', text='total_cnt', title='Sole/Shared Performance Count for Each Actor'): - This line creates a horizontal bar plot using Plotly Express (px.bar). - The DataFrame df is used as the data source. - The x-axis represents the total count of performances (total_cnt). - The y-axis represents the actors (actor). - The bars are oriented horizontally (orientation='h'). - The color of the bars is determined by the isDup column. - The text annotations on the bars display the total count of performances. - The title of the plot is set to ‘Sole/Shared Performance Count for Each Actor’.\nfig.update_traces(textposition='outside'): - This line updates the position of the text annotations on the bars to be outside the bars. - By default, the text annotations are placed inside the bars, but setting textposition='outside' moves them outside for better visibility.\nyaxis_categoryorder='max ascending': This line specifies the order of categories (actors) on the y-axis based on the maximum value of their associated data points. Categories with higher maximum values will appear towards the top of the plot, while categories with lower maximum values will appear towards the bottom. You can alter the values to experiment.\nyaxis=dict(tickmode='linear'): This line ensures that the tick marks on the y-axis are displayed in a linear fashion. This is the default behavior, so it’s not necessary to explicitly specify it unless you want to override any previous settings.\n\nfig = px.bar(df, x='total_cnt', y='actor', orientation='h', color='isDup',\n             text='total_cnt', title='Sole/Shared Performance Count for Each Actor')\n\nfig.update_traces(textposition='outside')\n\n# Customizing the layout\nfig.update_layout(\n    xaxis_title='Number of Shared Actors',\n    yaxis_title='Movie Names',\n    yaxis_categoryorder='max ascending',  # Optional: Order y-axis categories by total count\n    yaxis=dict(tickmode='linear'),  # Optional: Ensure that y-axis ticks are shown for every movie\n    height=600\n)\n\n\n\n\nBy hovering the plot, we can see that Michael Caine, John Lithgow, and Matt Damon have the greatest number of shared movies or TV shows with other crews in the Interstellar even though Michael Caine, John Lithgow, and Ellen Burstyn have the greatest number of actings. Therefore, we could not only recommand by the number of overlap among movies or shows, but also by the ‘popularity’ of the actor."
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "Accelerate The Heat Diffusion!",
    "section": "",
    "text": "Conduct a simulation of two-dimensional heat diffusion in various ways\n# Set the default Plotly renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\""
  },
  {
    "objectID": "posts/HW4/index.html#table-of-contents",
    "href": "posts/HW4/index.html#table-of-contents",
    "title": "Accelerate The Heat Diffusion!",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n1. Introduction\n\n\n2. Initial conditions\n\n\n3. Methods\n3.1 Matrix Multiplication\n3.2 Sparse matrix in JAX\n3.3 Direct operation with `numpy`\n3.4 Direction operation with `jax`"
  },
  {
    "objectID": "posts/HW4/index.html#introduction-1",
    "href": "posts/HW4/index.html#introduction-1",
    "title": "Accelerate The Heat Diffusion!",
    "section": "1. Introduction",
    "text": "1. Introduction\nTwo-dimensional heat diffusion refers to the process by which heat spreads across a surface or within a two-dimensional region over time. It’s governed by the heat equation, which describes how the temperature at any point in the region changes over time due to the flow of heat.\nThe heat equation in two dimensions can be expressed as:\n\\[\n\\frac{\\partial u}{\\partial t} = \\alpha \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right)\n\\]\nWhere: - \\(u\\) is the temperature distribution across the two-dimensional region, which typically depends on the spatial coordinates \\((x, y)\\) and time \\(t\\). - \\(\\alpha\\) is the thermal diffusivity, which represents how readily heat spreads through the material. - \\(\\frac{{\\partial u}}{{\\partial t}}\\) represents the rate of change of temperature with respect to time. - \\(\\frac{{\\partial^2 u}}{{\\partial x^2}}\\) and \\(\\frac{{\\partial^2 u}}{{\\partial y^2}}\\) represent the second spatial derivatives of temperature with respect to the \\(x\\) and \\(y\\) coordinates, respectively. They describe how temperature changes spatially along the \\(x\\) and \\(y\\) directions.\nThe heat equation essentially states that the change in temperature at any point is proportional to the rate of change of temperature over time and the spatial curvature of the temperature distribution.\nTo simulate two-dimensional heat diffusion numerically, we discretize the region into a grid of points. At each point on the grid, we calculate the change in temperature over time based on the temperature differences with neighboring points, following the heat equation. This can be done using finite difference methods or other numerical techniques.\n\n\n\nimage.png\n\n\nThe simulation proceeds in discrete time steps, with the temperature at each point being updated based on the temperatures of neighboring points and the thermal diffusivity. Over time, the heat spreads from hotter regions to cooler regions, gradually evening out the temperature distribution until it reaches equilibrium.\nVisualizing the simulation results often involves plotting heatmaps or contour plots, showing how the temperature evolves across the two-dimensional region over time.\nLet’s get start it!"
  },
  {
    "objectID": "posts/HW4/index.html#initial-conditions-1",
    "href": "posts/HW4/index.html#initial-conditions-1",
    "title": "Accelerate The Heat Diffusion!",
    "section": "2. Initial conditions",
    "text": "2. Initial conditions\n\n# import libraries (you should be familiar with these 2!)\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nFor this tutorial, we will use:\n\n# set the grid to have 101 rows and 101 columns.\nN = 101\n\n# stability constant or the time step size used in numerical simulations we use is 0.2\nepsilon = 0.2\n\n# construct initial condition: 1 unit of heat at midpoint\n\n# initialize a two-dimensional NumPy array `u0` of size `N` by `N`, filled with zeros. This array represents the initial temperature distribution across the grid.\nu0 = np.zeros((N, N))\n\n# sets the value at the center of the grid to 1.0, representing one unit of heat. \n# The expression `int(N/2)` calculates the index of the center of the grid in both dimensions, and the value 1.0 is assigned to that cell\nu0[int(N/2), int(N/2)] = 1.0\n\n# This function displays the array as an image, where each element of the array corresponds to a pixel in the image, and the value of the element determines the color of the pixel. \n# Since `u0` is a two-dimensional array, `imshow()` will display a heatmap where lighter colors represent higher temperatures and darker colors represent lower temperatures.\nplt.imshow(u0)"
  },
  {
    "objectID": "posts/HW4/index.html#methods-1",
    "href": "posts/HW4/index.html#methods-1",
    "title": "Accelerate The Heat Diffusion!",
    "section": "3. Methods",
    "text": "3. Methods\n\n3.1 Matrix Multiplication\n\n# you might have seen this in the climate change tutorial before\nimport inspect\nfrom heat_equation import advance_time_matvecmul\nfrom heat_equation import get_A\n\n# Print the source code of the 'advance_time_matvecmul' function\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = int(np.sqrt(A.shape[0]))\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nadvance_time_matvecmul advances a simulation of the heat equation by one timestep using matrix-vector multiplication.\nLet’s break down the function and its components:\n\nadvance_time_matvecmul takes three parameters:\n\nA: The 2D finite difference matrix representing the discretized differential operator of the heat equation. It has dimensions \\(N^2\\) times \\(N^2\\).\nu: The current state of the temperature grid at timestep k. It’s a 2D array with dimensions N times N.\nepsilon: The stability constant or time step size used in the simulation.\n\nN = int(np.sqrt(A.shape[0])): calculates the size of the grid based on the shape of the finite difference matrix A. Since A is square with dimensions \\(N^2\\) times \\(N^2\\), the square root of the number of rows (or columns) gives us N, the size of the grid in one dimension.\nu = u + epsilon * (A @ u.flatten()).reshape((N, N)): advances the temperature grid u by one timestep. It does this by performing matrix-vector multiplication between the finite difference matrix A and the flattened temperature grid u. The result is then reshaped to a 2D array with dimensions N times N. This operation represents the application of the discrete heat equation to update the temperature distribution.\nreturn u: returns the updated temperature grid after advancing it by one timestep.\n\nIn summary, this function takes the current state of the temperature grid, applies the heat equation using matrix-vector multiplication, and returns the updated temperature grid for the next timestep.\n\n# Print the source code of the 'get_A' function\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"\n    Constructs the finite difference matrix A for a 2D Laplace operator.\n\n    Parameters:\n    - N: int\n        The size of the grid in one dimension.\n\n    Returns:\n    array_like\n        The finite difference matrix A representing the discretized Laplace operator.\n\n    This function constructs the finite difference matrix A for a 2D Laplace operator based on the size of the grid N.\n    The Laplace operator is discretized using a five-point stencil finite difference scheme.\n    The resulting matrix A is a square matrix of size N^2 x N^2.\n\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n\nget_A(N) generates the finite difference matrix A for the 2D discretization of the Laplace operator.\nLet’s break down the function and its components: 1. get_A that takes a single parameter N, which represents the size of the grid in one dimension.\n\nn = N * N: calculates the total number of grid points in the 2D grid by squaring the size of the grid in one dimension.\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]: initializes a list diagonals containing arrays representing the diagonals of the finite difference matrix A.\n\nThe first diagonal is set to -4 for each element (corresponding to the main diagonal).\nThe second and third diagonals are set to 1 for each element, except for the last element in each row, where the value is 0. This is because these diagonals represent the elements directly adjacent to the main diagonal, excluding the edges.\nThe fourth and fifth diagonals are also set to 1, representing the elements that are N positions away from the main diagonal, corresponding to the elements above and below the main diagonal.\n\ndiagonals[1][(N-1)::N] = 0 and diagonals[2][(N-1)::N] = 0: set the elements of the second and third diagonals to 0 at the positions corresponding to the last element in each row. This is done to ensure that the finite difference matrix A respects the boundary conditions of the grid.\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N): constructs the finite difference matrix A by summing the diagonals specified in the diagonals list.\n\nnp.diag(diagonals[0]) constructs a diagonal matrix using the main diagonal elements.\nnp.diag(diagonals[1], 1) constructs a diagonal matrix using the elements from the second diagonal shifted one position to the right.\nnp.diag(diagonals[2], -1) constructs a diagonal matrix using the elements from the third diagonal shifted one position to the left.\nnp.diag(diagonals[3], N) constructs a diagonal matrix using the elements from the fourth diagonal shifted N positions down.\nnp.diag(diagonals[4], -N) constructs a diagonal matrix using the elements from the fifth diagonal shifted N positions up.\nThe matrices are then summed together to obtain the final finite difference matrix A.\n\nreturn A: returns the constructed finite difference matrix A.\n\nIn summary, this function generates the finite difference matrix A for the 2D discretization of the Laplace operator based on the size of the grid N, taking into account the boundary conditions. This matrix is then used in advance_time_matvecmul.\n\nfrom timeit import default_timer\n\ndef heat_plot(f, u0, epsilon, A=None):\n    \"\"\"\n    Visualizes the progression of a simulation over multiple time steps.\n\n    Parameters:\n    - f: function\n        The method used to advance the simulation by one time step. \n        If A is provided, f should take three parameters (A, u, epsilon). \n        Otherwise, f should take two parameters (u, epsilon).\n    - u0: array_like\n        The initial state of the simulation.\n    - epsilon: float\n        The stability constant or time step size.\n    - A: array_like, optional\n        The finite difference matrix representing the discretized differential operator of the simulation.\n\n    Returns:\n    None\n\n    This function iterates over the specified number of time steps, advances the simulation using the provided method f,\n    and visualizes the progression of the simulation at milestone iterations. It also prints the total computation time.\n\n    \"\"\"\n    intermediate_solutions = []\n\n    start = default_timer()\n    u = u0.copy() \n\n    for i in range(1,2701):\n        if A is not None:\n            u = f(A,u,epsilon)\n        else:\n            u = f(u, epsilon)\n        if i % 300 == 0:\n            intermediate_solutions.append(u.copy())\n\n    end = default_timer()\n    computation_time = end - start\n    print(f\"Total computation time: {computation_time} seconds\")\n    \n    fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n    for i in range(3):\n        for j in range(3):\n            idx = i * 3 + j\n            axs[i, j].imshow(intermediate_solutions[idx], interpolation='nearest')\n            axs[i, j].set_title(f\"Iteration {idx * visualization_interval}\")\n        \n    plt.show() \n\nJust for convenience, I created a generic function for visualizing the progression of a simulation over multiple time steps.\nLet’s break it down: 1. heat_plot that takes four parameters: - f: A function representing the method used to advance the simulation by one time step. It can either take two parameters (u, epsilon) or three parameters (A, u, epsilon) depending on whether A is provided or not. - u0: The initial state of the simulation. - epsilon: A parameter representing the stability constant or time step size. - A: (Optional) The finite difference matrix representing the discretized differential operator of the simulation.\n\nintermediate_solutions = []: initializes an empty list to store intermediate states of the simulation for visualization purposes since we should not cont the time for generating plot so we should store it for ploting use with a frequency of 300.\nstart = default_timer(): records the start time of the computation.\nu = u0.copy(): creates a copy of the initial state u0 to ensure that the original state is not modified during the simulation.\nfor i in range(1,2701):: starts a loop iterating over the range from 1 to 2700 (inclusive), representing the number of time steps in the simulation.\nif A is not None:: checks if the finite difference matrix A is provided. If it is not None, it indicates that the function f takes three parameters (A, u, epsilon). Otherwise, it takes two parameters (u, epsilon). For first 2 method, we defined A and for last 2 we are not gonna define it.\nu = f(A,u,epsilon) or u = f(u, epsilon): Depending on whether A is provided or not, this line advances the simulation by one time step using the function f. If A is provided, f is called with three parameters (A, u, epsilon), otherwise with two parameters (u, epsilon).\nif i % 300 == 0:: checks if the current iteration is a multiple of 300. If it is, it indicates a milestone iteration for visualization, and the current state u is appended to the intermediate_solutions list.\nend = default_timer(): records the end time of the computation.\ncomputation_time = end - start: calculates the total computation time by subtracting the start time from the end time.\nprint(f\"Total computation time: {computation_time} seconds\"): prints the total computation time to the console.\nVisualization: This section creates a 3x3 grid of subplots using plt.subplots(3, 3, figsize=(12, 12)). It iterates over the intermediate_solutions list and plots each intermediate state on a subplot. Each subplot is titled with the corresponding iteration number.\n\nLet me show you the work!\n\nvisualization_interval = 300\n\n\nA = get_A(N)\n\n\nheat_plot(advance_time_matvecmul, u0, epsilon, A)\n\nTotal computation time: 20.457517958944663 seconds\n\n\n\n\n\n\n\n\n\nBeautiful plot but the running time is 20.457517958944663 seconds, which is pretty long. The reason why this took this much of time is that most of operations are wasted for computing zeros.\nLet’s use the data structure that exploits a lot of zeros in the matrix A: sparse matrix data structures. The JAX package holds an experimental sparse matrix support. We can use the batched coordinate (BCOO) format to only use O(N^2) space for the matrix, and only take O(N^2) time for each update.\n\n\n3.2 Sparse matrix in JAX\n\nJax\nJAX is a machine learning package putting autodiff, XLA (accelerated linear algebra), and just-in-time compilation together, created by Google.\nIn Python, we already have a couple of widely-used machine learning packages, PyTorch and TensorFlow. Why do we start with something else?\nShort answer: because it can be considerably faster. Over the years, when I was using another just-in-time compilation language for research, it was giving me significant amount of flexibility while maintaining its speed.\nPython is a slow language at its core. Its development is highly focused on flexibility. However, due to its flexibility, a lot of highly performant packages were built, often interfacing with other languages. What just-in-time (JIT) compilation adds to it is that you can write highly performant code without you directly dealing with the lower-level language (writing C or C++), even when those functionalities are not readily written in another package (numpy, scipy, tensorflow, or pytorch).\nFor example, a small neural network model (GoogleNet) on CIFAR10 can be trained in JAX 3x faster than in PyTorch with a similar setup. JAX enables this speedup by compiling functions and numerical programs for accelerators (GPU/TPU) just in time, finding the optimal utilization of the hardware.\nFrameworks with dynamic computation graphs like PyTorch cannot achieve the same efficiency, since they cannot anticipate the next operations before the user calls them. For example, in an Inception block of GoogleNet, multiple convolutional layers (which we will learn soon) are applied in parallel on the same input. JAX can optimize the execution of this layer by compiling the whole forward pass for the available accelerator and fusing operations where possible, reducing memory access and speeding up execution.\nIn contrast, when calling the first convolutional layer in PyTorch, the framework does not know that multiple convolutions on the same feature map will follow. It sends each operation one by one to the GPU, and can only adapt the execution after seeing the next Python calls. Hence, JAX can make more efficient use of the GPU than, for instance, PyTorch.\n\n\nThe downside\nEverything comes with a price. In order to efficiently compile programs just-in-time in JAX, the functions need to be written with certain constraints.\n\nFunctions are not allowed to have side effects.\n\n\nThey can’t affect any variable outside their namespaces – “pure functions” in functional programming nomenclature.\nCannot mutate input arrays.\nRandom number generation procedures are generally written in a way that mutates global states – JAX has its own way to write random number generation.\nEven having print() inside a function is a side effect!\n\nThis kind of functional programming approach is not something unique to JAX in data analytics; it has previously been used in places like Apache Spark, for large-scale parallel data analytics.\n\nJAX compiles functions based on the expected shapes of all arrays/tensors in the function.\n\n\nIt becomes an issue if the shapes or the control flow within the function depends on the values of arrays.\ny = x[x&gt;3]?\n\nStill, in a lot of numerical computations, it is straightforward to write functions within these constraints.\nMany other great JAX tutorials are there, including:\n\nJAX 101 with many subtutorials on individual parts of JAX\nJAX - The Sharp Bits discusses the constraints of JAX and how to overcome them\nJax for the Impatient for a quick intro to JAX with focus on deep learning\n\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nimport jax\n\nsparse module from the jax.experimental package: JAX is a library for numerical computing and automatic differentiation, and it provides experimental support for sparse matrix operations. The sparse module contains functions and data structures for working with sparse matrices in JAX, such as converting between dense and sparse representations, performing sparse matrix-vector multiplication, and solving linear systems with sparse matrices.\njnp: NumPy-compatible array manipulation library provided by JAX and aliases it as jnp. JAX’s NumPy (jnp) offers functions and data structures similar to NumPy for manipulating arrays, performing mathematical operations, and working with numerical data. However, JAX’s NumPy is designed to work seamlessly with JAX’s automatic differentiation capabilities, allowing users to compute gradients of functions involving array operations efficiently.\n\nfrom heat_equation import get_sparse_A\n\n# Print the source code of the 'get_A_sparse' function\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    Constructs a sparse representation of the finite difference matrix A for a 2D Laplace operator.\n\n    Parameters:\n    - N: int\n        The size of the grid in one dimension.\n\n    Returns:\n    sparse.BCOO\n        A sparse representation of the finite difference matrix A.\n\n    This function constructs the finite difference matrix A for a 2D Laplace operator based on the size of the grid N,\n    and converts it into a sparse representation using the BCOO format.\n    The Laplace operator is discretized using a five-point stencil finite difference scheme.\n    The resulting sparse matrix A_sp_matrix is a sparse matrix of size N^2 x N^2.\n\n    \"\"\"\n    A = get_A(N)\n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\n\n\nThe main high-level sparse object currently available in JAX is the BCOO, or batched coordinate sparse array, which offers a compressed storage format compatible with JAX transformations.\nFind more info about BCOO: https://jax.readthedocs.io/en/latest/jax.experimental.sparse.html\n\n# JIT-ed version of advance_time_matvecmul for better performance\nadvance_time_matvecmul_jit = jax.jit(advance_time_matvecmul)\n\nHere’s what it does:\n\nJIT Compilation: JAX’s JIT compilation feature allows for efficient execution of functions by compiling them into optimized machine code just before they are executed. This can lead to significant performance improvements, especially for functions that are called frequently or are computationally intensive.\nCompilation on Demand: When you create a JIT-compiled version of a function, JAX analyzes the function’s computation graph and generates optimized code tailored to the specific inputs it receives. This compilation process occurs on demand, the first time the function is called with a particular set of input shapes and data types.\nUsage of Compiled Function: After compilation, the resulting JIT-compiled function (advance_time_matvecmul_jit in this case) behaves like a regular Python function. However, it executes more efficiently due to the optimized machine code generated during compilation.\nBenefits: Using JIT compilation can lead to faster execution times, reduced memory usage, and improved performance, especially for functions involved in numerical computations or machine learning models.\n\n\nA = get_sparse_A(N)\nheat_plot(advance_time_matvecmul_jit, u0, epsilon, A)\n\nTotal computation time: 0.5534884999506176 seconds\n\n\n\n\n\n\n\n\n\n0.5534884999506176 seconds in Method 2 compares to 20.457517958944663 seconds in Method 1, the speed increase enomoursly by almost 40 times!\n\n\n\n3.3 Direct operation with numpy\nWe’ll implement a function called advance_time_numpy to advance the solution of the heat equation by one timestep using direct operations with NumPy. Here’s the implementation of the function:\n\nfrom heat_equation import advance_time_numpy\n\n# Print the source code of the 'advance_time_jax' function\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    # Compute the Laplacian using central differences\n    laplacian = (\n        np.roll(u, 1, axis=0) +  # Top\n        np.roll(u, -1, axis=0) +  # Bottom\n        np.roll(u, 1, axis=1) +   # Left\n        np.roll(u, -1, axis=1) -   # Right\n        4 * u  # Center\n    )\n\n    # Update the grid state using the heat equation\n    u = u + epsilon * laplacian\n\n    return u\n\n\n\nnp.roll is used to shift the elements of the array u along the specified axes (0 for rows and 1 for columns). By summing the shifted arrays, we effectively compute the Laplacian using central differences without the need for padding the array with zeros.\nYou can find more info here: https://numpy.org/doc/stable/reference/generated/numpy.roll.html\nThis function advances the solution of the heat equation by one timestep using NumPy vectorized operations. It computes the Laplacian of the temperature grid using central differences with np.roll, and updates the grid state using the heat equation.\n\nheat_plot(advance_time_numpy, u0, epsilon)\n\nTotal computation time: 0.14709462481550872 seconds\n\n\n\n\n\n\n\n\n\nNow we have:\n20.457517958944663 seconds in Method 1\n0.5534884999506176 seconds in Method 2\n0.14709462481550872 seconds in Method 3\nWe are getting faster and faster!\n\n\n3.4 Direction operation with jax\nNow, let’s use jax to do the similar using just-in-time compilation. This function advances the simulation of the heat equation by one timestep using JAX. It computes the Laplacian of the temperature grid using central differences, and updates the grid state according to the heat equation. The Laplacian is computed with boundary conditions handled by padding the temperature grid.\n\nfrom heat_equation import advance_time_jax\n\n# Print the source code of the 'advance_time_jax' function\nprint(inspect.getsource(advance_time_jax))\n\n@jax.jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the solution by one timestep using JAX.\n\n    Args:\n    u : array_like\n        N x N grid state at timestep k.\n    epsilon : float\n        Stability constant.\n\n    Returns:\n    array_like\n        N x N grid state at timestep k+1.\n    \"\"\"\n\n    laplacian = (\n        jnp.roll(u, 1, axis=0) +  # Top\n        jnp.roll(u, -1, axis=0) +  # Bottom\n        jnp.roll(u, 1, axis=1) +   # Left\n        jnp.roll(u, -1, axis=1) -   # Right\n        4 * u  # Center\n    )\n\n    # Update the grid state using the heat equation\n    u = u + epsilon * laplacian\n\n    return u\n\n\n\n\nheat_plot(advance_time_jax, u0, epsilon)\n\nTotal computation time: 0.051046375185251236 seconds\n\n\n\n\n\n\n\n\n\nWell, this is so much faster by using jit!\nNow we have:\n20.457517958944663 seconds in Method 1\n0.5534884999506176 seconds in Method 2\n0.14709462481550872 seconds in Method 3\n0.051046375185251236 seconds in Method 4\nMethod 2 is 40 times faster than Method 1 Method 3 is 200 times faster than Method 1 Method 4 is more than 2 times faster than Method 3\nCompare the implementation and performances of the four methods. Last one is the fastest and easy to write. Plus, I think it is easy to understand the logic behind it through Direct Manipulation instead of using Matrix Multiplication. Through this tutorial, we could really see the power of Linear Algebra and jit."
  },
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "Fake News Detection",
    "section": "",
    "text": "Table of Contents\n\n1. Introduction\n\n\n2. Data Acquisition\n\n\n3. Data Preparation\n\n\n4. Model Creation: Model1 (using Title), Model2 (using Text), Model3 (using Title + Text)\n\n\n5. Model Training\n\n\n6. Model Evaluation\n\n\n7. Embeding Visualization\n\n# Set the default Plotly renderer to \"iframe\"\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n\n\n\n1. Introduction\nIn today’s digital age, the spread of misinformation and fake news has become a major concern. With the vast amount of information available online, it has become increasingly challenging to distinguish between credible sources and those that peddle falsehoods. This problem has far-reaching consequences, from swaying public opinion to influencing political outcomes.\nTo combat this issue, researchers and data scientists have been exploring various techniques to automate the detection of fake news. One promising approach involves the use of deep learning, a subfield of machine learning that has proven remarkably effective in tasks such as image recognition, natural language processing, and predictive modeling.\nIn this blog post, we’ll explore the application of deep learning, specifically using the Keras library, to build models capable of identifying fake news articles. We’ll be implementing three different models, each leveraging different aspects of the news article:\n\nModel 1 (Using Title): This model will focus solely on the title of the news article, aiming to capture any potential signals or patterns that might indicate whether the content is legitimate or fabricated.\nModel 2 (Using Text): In contrast, this model will analyze the body text of the news article, taking into account the writing style, word choice, and overall content to make its prediction.\nModel 3 (Using Title + Text): The third model will combine the strengths of the previous two, incorporating both the title and the body text to make a more informed decision about the veracity of the news article.\n\nBy comparing the performance of these three models, we’ll gain insights into the relative importance of different components of a news article in determining its credibility. Additionally, we’ll explore techniques for optimizing these models, such as data preprocessing, feature engineering, and hyperparameter tuning.\nFake news detection is a complex and multifaceted challenge, but the application of deep learning techniques holds great promise in tackling this issue. Join us as we delve into the world of deep learning and explore its potential in combating the spread of misinformation.\n\n# Import pakeages\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\n\nfrom keras import layers\nfrom keras import losses\nimport keras\n\nfrom keras.layers import TextVectorization\n\nfrom matplotlib import pyplot as plt\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nprint(keras.__version__)\n\n3.0.5\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n\n\n2. Data Acquisition\nOur data for this assignment comes from the article\nAhmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n\nThe dataset hosted a training data set at the below URL. You can either read it into Python directly (via pd.read_csv()) or download it to your computer and read it from disk. This would be our training dataset.\n\ndata = pd.read_csv(train_url)\n\n\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nEach row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.\n\ndata.shape\n\n(22449, 4)\n\n\n\ndf = data.copy()\n\nAlways copy from the original dataset, in case you make some inreversible changes and do not want to load data again.\n\n\n3. Data Preparation\nAs you can see from the preview of the dataset, we need to perform some preprocessing steps on the ‘title’ and ‘text’ columns. These steps are essential for improving the performance and accuracy of our machine learning models. Let me explain the reasons behind each step:\n\nChange all characters into lowercase: This step ensures consistency in the text data, as it eliminates the possibility of treating the same word differently based on its capitalization. For example, “News” and “news” would be treated as different words without lowercasing, which is undesirable for text analysis tasks.\nRemove stopwords: Stopwords are common words in a language that carry little or no meaningful information, such as articles (e.g., “a,” “an,” “the”), prepositions (e.g., “in,” “on,” “at”), conjunctions (e.g., “and,” “but,” “or”), and pronouns (e.g., “it,” “he,” “she”). Removing these words helps to reduce noise in the text data and focuses the analysis on the more meaningful words, which can improve the model’s ability to capture the essence of the content.\nConvert into TensorFlow Dataset: TensorFlow Datasets are a convenient way to represent and handle data in TensorFlow, making it easier to load, preprocess, and iterate over the data during training and evaluation. By converting the preprocessed data into a TensorFlow Dataset, we can take advantage of various features and optimizations provided by TensorFlow, such as batching, shuffling, and efficient data loading.\n\nBy performing these preprocessing steps, we aim to clean and prepare the text data in a way that makes it more suitable for our machine learning models. Lowercasing ensures consistent treatment of words, removing stopwords reduces noise, and converting to a TensorFlow Dataset streamlines the data handling process. These steps are commonly applied in natural language processing tasks and can significantly improve the performance and accuracy of text-based machine learning models.\nTo do this, we write a function called make_dataset. You can consider it as a pipline since you might want to apply it later on the test dataset to make sure the correspondence.\n\n# Import stopwords with nltk.\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\n{'t', 'who', 'so', 'where', 'about', 'did', 'yours', 'm', 'during', 'any', 'itself', 'theirs', 'd', 'those', 'and', 'you', 'will', 'won', 'both', 'through', 'his', \"mightn't\", 'by', 'i', 'him', 'mightn', 'why', \"wasn't\", 'aren', 'been', 'her', 'is', 'yourselves', 'this', 'an', \"you're\", 'there', \"shouldn't\", 'y', 'against', 'out', 's', 'hasn', 'more', 'over', 'our', \"couldn't\", 'your', 'on', 'same', 'because', 'isn', 'while', 'mustn', 'shouldn', 'ain', 'only', 'some', 'a', 'has', 'than', \"doesn't\", 'other', 'their', 'shan', 'from', 'in', 'for', 'don', 'wouldn', 'do', \"don't\", 'does', 'which', 'not', \"didn't\", 'they', 'that', 'having', \"shan't\", \"won't\", 'too', 'of', 'below', \"wouldn't\", 'when', 'with', 'haven', 'or', 'very', 'be', 'as', 'own', 'above', 'just', 'are', 'them', 'needn', 'wasn', 'these', \"haven't\", \"aren't\", 'off', 'being', 'had', \"that'll\", 'after', \"you've\", 'it', 'down', 'if', \"it's\", 'me', 'whom', 'was', \"mustn't\", 'hadn', 'again', 'myself', 'herself', 'until', \"she's\", 'up', 'can', \"you'll\", 'the', 'few', 'should', 'ma', 'himself', 'o', \"needn't\", \"hadn't\", 'doesn', \"should've\", 'yourself', 've', 'didn', 'have', 'now', 'further', 'its', \"you'd\", 'here', 'weren', 'then', 're', 'she', 'all', 'he', \"weren't\", 'll', 'what', 'ourselves', 'but', 'most', 'couldn', 'doing', 'no', 'were', 'at', 'hers', 'themselves', 'am', \"hasn't\", 'between', 'how', 'ours', 'each', 'to', 'under', 'we', 'such', 'into', \"isn't\", 'my', 'nor', 'once', 'before'}\n\n\n\ndef make_dataset(df):\n   \"\"\"\n   Preprocesses the given DataFrame and creates a TensorFlow Dataset.\n\n   Args:\n       df (pandas.DataFrame): The input DataFrame containing the news articles and their corresponding labels.\n\n   Returns:\n       tf.data.Dataset: A TensorFlow Dataset containing the preprocessed data.\n\n   This function performs the following preprocessing steps:\n   1. Converts the 'text' and 'title' columns to lowercase.\n   2. Removes stop words from the 'title' and 'text' columns.\n   3. Creates a TensorFlow Dataset from the preprocessed data, with 'title' and 'text' as features\n      and 'fake' as the label.\n   \"\"\"\n   df['text'] = df['text'].str.lower()\n   df['title'] = df['title'].str.lower()\n   df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n   df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n   tf_df = tf.data.Dataset.from_tensor_slices(({'title': df[\"title\"], 'text': df[\"text\"]}, df[\"fake\"]))\n   return tf_df\n\nThis will construct and return a tf.data.Dataset with two inputs(title, text) and one output(fake). Now we get our data!\n\ntf_df = make_dataset(df)\n\nYou may wish to batch your Dataset prior to returning it, which can be done like this: my_data_set.batch(100). Batching causes your model to train on chunks of data rather than individual rows. This can sometimes reduce accuracy, but can also greatly increase the speed of training. Finding a balance is key. I found batches of 100 rows to work well. This time I wou’t do it.\n\ntf_df = tf_df.shuffle(buffer_size = len(tf_df), reshuffle_each_iteration=False)\n\ntrain_size = int(0.8*len(tf_df))\nval_size   = int(0.2*len(tf_df))\n\ntrain = tf_df.take(train_size)\nval   = tf_df.skip(train_size).take(val_size)\n\nlen(train), len(val)\n\n(17959, 4489)\n\n\nTo ensure that the model is exposed to a diverse set of examples during training and can be evaluated on unseen data during validation, we do the following:\ntf_df = tf_df.shuffle(buffer_size=len(tf_df), reshuffle_each_iteration=False): shuffles the tf_df TensorFlow Dataset. Shuffling the data is an important step in machine learning, as it helps to introduce randomness and prevent the model from learning any patterns specific to the order of the data. The buffer_size=len(tf_df) parameter specifies that the entire dataset should be loaded into the buffer for shuffling. reshuffle_each_iteration=False means that the data will be shuffled once, and the same shuffled order will be used for each iteration or epoch during training.\ntrain_size = int(0.8 * len(tf_df)): calculates the size of the training dataset by taking 80% of the total dataset size. The int() function is used to convert the result to an integer, as the dataset indexing requires integer values.\nval_size = int(0.2 * len(tf_df)): Similarly, this line calculates the size of the validation dataset by taking 20% of the total dataset size.\ntrain = tf_df.take(train_size): creates a new TensorFlow Dataset train by taking the first train_size elements from tf_df. This subset will be used for training the machine learning model.\nval = tf_df.skip(train_size).take(val_size): creates a new TensorFlow Dataset val by skipping the first train_size elements from tf_df and then taking the next val_size elements. This subset will be used for validation during the training process, to evaluate the model’s performance on unseen data.\n\ndf.fake.value_counts()\n\n1    11740\n0    10709\nName: fake, dtype: int64\n\n\n\n11740 / (11740+10709)\n\n0.522963160942581\n\n\nDo you remember Base rate that we mentioned last time? Base rate refers to the accuracy of a model that always makes the same guess (for example, such a model might always say “fake news!”). The base rate for this case would be 0.522 as calculated above through dividing number of most occurency (1) by number of 0s.\nVectorization is the process of converting text into numerical representations, allowing machine learning models to process and understand textual data. In this case, we use a frequency ranking approach, where each word is replaced by its rank based on its frequency of occurrence within the dataset. For example, “Poll: Penguins Best Bird” might be represented as [708, 1567, 89, 632], where the numbers correspond to the frequency ranks of “poll”, “penguins”, “best”, and “bird” respectively.\nTensorFlow provides a TextVectorization layer that handles this process efficiently. It takes the standardized text data as input and converts it into numerical vectors suitable for training and inference. This step is crucial for feeding textual data into machine learning models that operate on numerical inputs.\n\n#preparing a text vectorization layer for tf model\nmax_tokens = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\n# replace each words by its frequency rank in the data\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=max_tokens, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntexts = train.map(lambda x, y: x['title'] + ' ' + x['text'])\nvectorize_layer.adapt(texts)\n\nSince we used Embedding in the following model, we might need to expand dimision from 2D to 3D. If you want to do it in another way, here is one option:\ndata = tf.data.Dataset.from_tensor_slices(\n    (\n        {\n            \"title\" : df[[\"title\"]],\n            \"text\" : df[['text']]\n        },\n        {\n            \"fake\" : df[[\"fake\"]]\n        }\n    )\n)\n\n# increase the dim\ndef expand_dim(data, label):\n    title = tf.expand_dims(data['title'], -1)\n    text = tf.expand_dims(data['text'], -1)\n    return {'title': title, 'text': text}, [label]\n\ntrain = train.map(expand_dim)\nval = val.map(expand_dim)\n\nOk, let’s create our models!\n\n\n4. Model Creation: Model1 (using Title), Model2 (using Text), Model3 (using Title + Text)\nWe start by specifying the two kinds of keras.Input for our model. You should have one input for each qualitatively distinct “kind” of predictor data. All the parameters here are important:\n\nshape should describe the shape of a single item of data. For example, the lyrics column contains just one entry for each song, so the shape is (1,) (a tuple of length 1). On the other hand, there are len(scalars) = 22 distinct columns of scalar scores.\nthe name should be some descriptive name that you’re able to remember for later.\nThe dtype specifies the kind of data contained in each of the input tensors.\n\nBy creating these separate input layers for ‘title’ and ‘text’, we can feed the data into the model independently and allow the model to learn the relevant features from each input source. This approach is useful when dealing with different types of input data or when you want to process different parts of the input separately before combining them later in the model.\n\n# Define input layers for title and text\ntitle_input = tf.keras.Input(shape=(1, ), dtype=tf.string, name='title')\ntext_input = tf.keras.Input(shape=(1, ), dtype=tf.string, name='text')\nprint(title_input.shape)\n\n(None, 1)\n\n\nFirst, let’s write a pipeline for the lyrics.\nLast time we use Sequential API, this time we will use Functional API.\nKeras provides two different APIs for building neural network models: the Sequential API and the Functional API. The main difference between these two APIs lies in their approach to model construction and the level of flexibility they offer.\nSequential API: - The Sequential API is a linear stack of layers, where each layer is added sequentially to the model. - It is best suited for simple, feed-forward models where the data flows from the input layer through the hidden layers to the output layer. - The architecture is defined by simply stacking the layers one after the other, making it easy to construct and understand. - Example:\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=input_size))\nmodel.add(Activation('relu'))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\nFunctional API: - The Functional API is a more flexible and powerful way to build complex models with non-linear architectures, such as models with multiple inputs or outputs, models with shared layers, or models with residual connections. - It allows you to define complex computational graphs by connecting layers with tensors, enabling the creation of models with arbitrary architectures. - The model is defined by specifying the input tensors and then using these tensors to create the output tensors by applying layers and operations. - Example:\ninputs = Input(shape=(input_size,))\nx = Dense(64, activation='relu')(inputs)\nx = Dense(10, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=x)\nIn summary, the Sequential API is more beginner-friendly and suitable for simple, linear models, while the Functional API offers more flexibility and control for building complex models with non-linear architectures. The Functional API is generally preferred for advanced models, as it allows for greater customization and reusability of components within the model.\nIt’s worth noting that both APIs eventually create the same Keras model object, and the choice between them depends on the complexity of the model you want to build and your personal preference for the coding style.\n\n# passes the title_input tensor through the vectorize_layer\ntitle_features = vectorize_layer(title_input)\ntitle_features = layers.Embedding(2000, 3, name = \"title_embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\ntext_features = vectorize_layer(text_input)\ntext_features = layers.Embedding(2000, 3, name = \"text_embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\nmain1 = layers.Dense(32, activation='relu')(title_features)\noutput1 = layers.Dense(2, name = \"fake\")(main1)\nmodel1 = keras.Model(\n    inputs = title_input,\n    outputs = output1\n)\n\nmain2 = layers.Dense(32, activation='relu')(text_features)\noutput2 = layers.Dense(2, name = \"fake\")(main2)\nmodel2 = keras.Model(\n    inputs = text_input,\n    outputs = output2\n)\n\nmain3 = layers.concatenate([title_features, text_features], axis = 1)\nmain3 = layers.Dense(32, activation='relu')(main3)\noutput3 = layers.Dense(2, name = \"fake\")(main3)\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output3\n)\n\nHere we defines three separate models with difference input but same layers.\nModel1 (using Title)\nModel2 (using Text)\nModel3 (using Title + Text)\nLet’s go through each layer used in the code:\n\nTextVectorization Layer:\n\ntitle_features = vectorize_layer(title_input)\ntext_features = vectorize_layer(text_input)\nThis layer converts the input text into numerical representations, such as frequency rank vectors or one-hot encodings.\n\nEmbedding Layer:\n\ntitle_features = layers.Embedding(2000, 3, name=\"title_embedding\")(title_features)\ntext_features = layers.Embedding(2000, 3, name=\"text_embedding\")(text_features)\nThis layer maps the numerical representations of words/tokens to dense vector representations (embeddings) of a fixed size (in this case, 3).\nThe 2000 parameter specifies the vocabulary size (maximum number of unique words/tokens).\n\nDropout Layer:\n\ntitle_features = layers.Dropout(0.2)(title_features)\ntext_features = layers.Dropout(0.2)(text_features)\nThis layer randomly sets a fraction (0.2 or 20%) of the input units to 0 during training, which helps prevent overfitting by introducing noise and regularization.\n\nGlobalAveragePooling1D Layer:\n\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\nThis layer computes the average of the input vectors across the sequence dimension (1D), effectively reducing the representation to a single vector that captures the overall meaning of the input sequence.\n\nDense Layer:\n\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\nThis is a fully-connected layer that applies a linear transformation to the input and then applies a ReLU (Rectified Linear Unit) activation function.\nThe 32 parameter specifies the number of output units in the dense layer.\n\n\nBy applying these layers to the ‘title’ and ‘text’ inputs separately, the code generates separate feature representations for each input source.\nBy the following code, you can see how the data have gone through each layers.\n\nkeras.utils.plot_model(model1, \"classifier_model_with_title_only.png\", show_shapes=True,\n                       show_layer_names=True,\n                       show_layer_activations=True)\n\n\n\n\n\n\n\n\n\nkeras.utils.plot_model(model2, \"classifier_model_with_text_only.png\", show_shapes=True,\n                       show_layer_names=True,\n                       show_layer_activations=True)\n\n\n\n\n\n\n\n\n\nkeras.utils.plot_model(model3, \"classifier_model_with_title_and_text.png\", show_shapes=True,\n                       show_layer_names=True,\n                       show_layer_activations=True)\n\n\n\n\n\n\n\n\n\nmodel1.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\nmodel2.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\nmodel3.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nThis code is responsible for compiling the three models (model1, model2, and model3) and setting up an early stopping callback for the training process.\nModel Compilation: - model1.compile(optimizer=\"adam\", loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']): compiles model1 with the following settings: - optimizer=\"adam\": Specifies the Adam optimizer for updating the model’s weights during training. - loss=losses.SparseCategoricalCrossentropy(from_logits=True): Specifies the loss function to be used. The SparseCategoricalCrossentropy loss is suitable for multi-class classification problems where the target labels are integer-encoded (e.g., 0, 1, 2, …). The from_logits=True parameter indicates that the model output is not yet passed through a softmax activation function, which will be done internally by the loss function. - metrics=['accuracy']: Specifies the evaluation metric to be tracked during training and testing. In this case, the accuracy metric is used, which measures the fraction of correctly classified samples.\n\nThe same compilation steps are repeated for model2 and model3.\n\nEarly Stopping Callback: - callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5): This line creates an EarlyStopping callback object, which monitors the validation loss (val_loss) during training and stops the training process if the validation loss does not decrease for a certain number of epochs (specified by patience=5).\n\nEarly stopping is a regularization technique used to prevent overfitting by stopping the training process when the model performance on the validation set stops improving. This can help save computational resources and prevent the model from overfitting to the training data.\nThe monitor='val_loss' parameter specifies that the callback should monitor the validation loss. If you want to monitor a different metric, such as validation accuracy (val_accuracy), you can change this parameter accordingly.\nThe patience=5 parameter means that the training process will stop if the validation loss does not decrease for 5 consecutive epochs.\n\nAfter compiling the models and setting up the early stopping callback, you can proceed to train the models using methods like model.fit() or model.fit_generator(). The early stopping callback will be applied during the training process, and the training will stop automatically if the specified criteria are met (in this case, if the validation loss does not decrease for 5 consecutive epochs). We can see it shortly.\n\n\n5. Model Training\nThings are quite similar to the last time:\nepochs=20: This parameter sets the number of epochs (complete passes through the training dataset) for the training process. In this case, the training will run for 20 epochs.\ncallbacks=[callback]: This parameter specifies a list of callbacks to be applied during the training process. In this case, the list contains a single callback object callback, which was defined earlier as callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5). This callback will monitor the validation loss and stop the training if it does not decrease for 5 consecutive epochs, helping to prevent overfitting.\nverbose=True: This parameter controls the verbosity of the training output. When set to True, it will print the progress of the training process, including the loss and metric values for each epoch.\nIn summary, this line starts the training process for model1 using the specified training dataset (train), validation dataset (val), and training parameters (epochs=20, callbacks=[callback], verbose=True). The training history, containing information about the loss and metric values at each epoch, is stored in the history1 variable. During the training process, the early stopping callback will monitor the validation loss and stop the training if it does not decrease for 5 consecutive epochs, helping to prevent overfitting.\nWe will do the same thing for the other 2 models.\n\nhistory1 = model1.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 68s 4ms/step - accuracy: 0.5175 - loss: 0.6937 - val_accuracy: 0.5150 - val_loss: 0.6931\nEpoch 2/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 3ms/step - accuracy: 0.5250 - loss: 0.6923 - val_accuracy: 0.5150 - val_loss: 0.6931\nEpoch 3/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 61s 3ms/step - accuracy: 0.5250 - loss: 0.6923 - val_accuracy: 0.5150 - val_loss: 0.6931\nEpoch 4/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 61s 3ms/step - accuracy: 0.5250 - loss: 0.6923 - val_accuracy: 0.5150 - val_loss: 0.6931\nEpoch 5/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 64s 4ms/step - accuracy: 0.5250 - loss: 0.6923 - val_accuracy: 0.5150 - val_loss: 0.6931\nEpoch 6/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 62s 3ms/step - accuracy: 0.5250 - loss: 0.6923 - val_accuracy: 0.5150 - val_loss: 0.6931\n\n\n\nhistory2 = model2.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 64s 3ms/step - accuracy: 0.7399 - loss: 0.4599 - val_accuracy: 0.8908 - val_loss: 0.2152\nEpoch 2/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 4ms/step - accuracy: 0.9142 - loss: 0.2142 - val_accuracy: 0.9632 - val_loss: 0.1042\nEpoch 3/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 64s 4ms/step - accuracy: 0.9299 - loss: 0.1766 - val_accuracy: 0.9595 - val_loss: 0.1088\nEpoch 4/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 64s 4ms/step - accuracy: 0.9390 - loss: 0.1568 - val_accuracy: 0.9461 - val_loss: 0.1306\nEpoch 5/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 3ms/step - accuracy: 0.9409 - loss: 0.1508 - val_accuracy: 0.9624 - val_loss: 0.0994\nEpoch 6/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 4ms/step - accuracy: 0.9474 - loss: 0.1344 - val_accuracy: 0.9652 - val_loss: 0.0916\nEpoch 7/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 64s 4ms/step - accuracy: 0.9499 - loss: 0.1326 - val_accuracy: 0.9581 - val_loss: 0.1014\nEpoch 8/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 4ms/step - accuracy: 0.9522 - loss: 0.1207 - val_accuracy: 0.9670 - val_loss: 0.0879\nEpoch 9/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 4ms/step - accuracy: 0.9580 - loss: 0.1154 - val_accuracy: 0.9187 - val_loss: 0.2010\nEpoch 10/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 64s 4ms/step - accuracy: 0.9587 - loss: 0.1070 - val_accuracy: 0.9779 - val_loss: 0.0779\nEpoch 11/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 4ms/step - accuracy: 0.9586 - loss: 0.1088 - val_accuracy: 0.9733 - val_loss: 0.0763\nEpoch 12/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 62s 3ms/step - accuracy: 0.9617 - loss: 0.1045 - val_accuracy: 0.9804 - val_loss: 0.0723\nEpoch 13/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 64s 4ms/step - accuracy: 0.9617 - loss: 0.0997 - val_accuracy: 0.9439 - val_loss: 0.1411\nEpoch 14/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 62s 3ms/step - accuracy: 0.9618 - loss: 0.0988 - val_accuracy: 0.9710 - val_loss: 0.0836\nEpoch 15/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 4ms/step - accuracy: 0.9645 - loss: 0.0928 - val_accuracy: 0.9644 - val_loss: 0.0895\nEpoch 16/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 4ms/step - accuracy: 0.9644 - loss: 0.0942 - val_accuracy: 0.9719 - val_loss: 0.0841\nEpoch 17/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 63s 4ms/step - accuracy: 0.9670 - loss: 0.0888 - val_accuracy: 0.9755 - val_loss: 0.0734\n\n\n\nhistory3 = model3.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 86s 5ms/step - accuracy: 0.9638 - loss: 0.1010 - val_accuracy: 0.9813 - val_loss: 0.0689\nEpoch 2/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 86s 5ms/step - accuracy: 0.9652 - loss: 0.0918 - val_accuracy: 0.9681 - val_loss: 0.0971\nEpoch 3/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 86s 5ms/step - accuracy: 0.9667 - loss: 0.0890 - val_accuracy: 0.9808 - val_loss: 0.0675\nEpoch 4/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 87s 5ms/step - accuracy: 0.9672 - loss: 0.0900 - val_accuracy: 0.9782 - val_loss: 0.0722\nEpoch 5/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 84s 5ms/step - accuracy: 0.9699 - loss: 0.0821 - val_accuracy: 0.9808 - val_loss: 0.0667\nEpoch 6/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 84s 5ms/step - accuracy: 0.9695 - loss: 0.0821 - val_accuracy: 0.9791 - val_loss: 0.0684\nEpoch 7/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 85s 5ms/step - accuracy: 0.9713 - loss: 0.0810 - val_accuracy: 0.9782 - val_loss: 0.0698\nEpoch 8/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 90s 5ms/step - accuracy: 0.9694 - loss: 0.0804 - val_accuracy: 0.9820 - val_loss: 0.0671\nEpoch 9/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 89s 5ms/step - accuracy: 0.9701 - loss: 0.0820 - val_accuracy: 0.9808 - val_loss: 0.0645\nEpoch 10/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 88s 5ms/step - accuracy: 0.9692 - loss: 0.0794 - val_accuracy: 0.9833 - val_loss: 0.0645\nEpoch 11/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 89s 5ms/step - accuracy: 0.9720 - loss: 0.0769 - val_accuracy: 0.9791 - val_loss: 0.0699\nEpoch 12/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 88s 5ms/step - accuracy: 0.9744 - loss: 0.0727 - val_accuracy: 0.9804 - val_loss: 0.0652\nEpoch 13/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 88s 5ms/step - accuracy: 0.9720 - loss: 0.0750 - val_accuracy: 0.9804 - val_loss: 0.0674\nEpoch 14/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 88s 5ms/step - accuracy: 0.9747 - loss: 0.0729 - val_accuracy: 0.9815 - val_loss: 0.0643\nEpoch 15/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 91s 5ms/step - accuracy: 0.9710 - loss: 0.0753 - val_accuracy: 0.9715 - val_loss: 0.0799\nEpoch 16/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 90s 5ms/step - accuracy: 0.9740 - loss: 0.0717 - val_accuracy: 0.9815 - val_loss: 0.0644\nEpoch 17/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 91s 5ms/step - accuracy: 0.9752 - loss: 0.0684 - val_accuracy: 0.9755 - val_loss: 0.0761\nEpoch 18/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 92s 5ms/step - accuracy: 0.9745 - loss: 0.0700 - val_accuracy: 0.9802 - val_loss: 0.0674\nEpoch 19/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 87s 5ms/step - accuracy: 0.9770 - loss: 0.0674 - val_accuracy: 0.9824 - val_loss: 0.0610\nEpoch 20/20\n17959/17959 ━━━━━━━━━━━━━━━━━━━━ 88s 5ms/step - accuracy: 0.9783 - loss: 0.0658 - val_accuracy: 0.9833 - val_loss: 0.0597\n\n\nThe first two models stoped at Epoch 6/20, 17/20 indicates that it did decrease for 5 consecutive epochs, helping to prevent overfitting.\nval_accuracy range for model1 stays at 0.51 val_accuracy range for model2: 0.8908 - 0.9804 val_accuracy range for model3: 0.9681 - 0.9833\nFrom this, we could see model3, which takes both title and text performs the best!\nHere are some interesting figures that might help you understand the learning rate of each model during the whole process.\nThis is just to save weights in the model if you want to plot by yourself without runing the model, since model runing is sometimes time-consuming.\n\nplt.plot(history1.history[\"accuracy\"], label = \"training\")\nplt.plot(history1.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"model1-accuracy\")\nplt.ylim(0.5, 1)\nplt.legend()\n\n\n\n\n\n\n\n\nThere might be some obvious overfitting in model1, and since the line is constant, the model does not display a active learning tendency.\n\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"model2-accuracy\")\nplt.ylim(0.5, 1)\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history3.history[\"accuracy\"], label = \"training\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"model3-accuracy\")\nplt.ylim(0.5, 1)\nplt.legend()\n\n\n\n\n\n\n\n\nOverfitting is not as strong as the case last time since the training performance only over the validation performance for a little bit, which might caused by the usage of early stopping.\n\n\n6. Model Evaluation\nNow let’s read the test data.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ndata = pd.read_csv(test_url)\n\nMake sure you do the same things to the test data — that’s why we need a pipline.\n\ntest_data = make_dataset(data)\ntest_data = test_data.map(expand_dim)\n\n\nmodel3.evaluate(test_data)\n\n22449/22449 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9811 - loss: 0.0718\n\n\n[0.06856698542833328, 0.9815582036972046]\n\n\nSince model3 is our best model, we use model3 on test data and achieved 0.0.9815582036972046 overall, what a great model we’ve built!!!\n\n\n7. Embedding visualization\nWord embeddings are often produced as intermediate stages in many machine learning algorithms. In fact, we already made one – it’s the Embedding layer at the base of our model. Let’s take a look at the embedding layer to see how our own model represents words in a vector space.\n\nweights = model3.get_layer('title_embedding').get_weights()[0]# get the weights from the embedding layer\nvocab = vectorize_layer.get_vocabulary()# get the vocabulary from our data prep for later\n\nThe collection of weights is 3-dimensional. For plotting in 2 dimensions, we have several choices for how to reduce the data to a 2d representation. A very simple and standard approach is our friend, principal component analysis (PCA).\nPCA stands for Principal Component Analysis. It is a widely used technique in machine learning and data analysis for dimensionality reduction and feature extraction.\nPCA is an unsupervised linear transformation technique that aims to find a new set of uncorrelated features called principal components. These principal components are ordered based on their ability to capture the variance in the original data. The first principal component captures the maximum possible variance, the second principal component captures the second-highest variance, and so on.\nThe main steps involved in PCA are:\n\nStandardize the data: The input features are standardized by subtracting the mean and dividing by the standard deviation to ensure that all features are on a similar scale.\nCompute the covariance matrix: The covariance matrix is calculated to capture the relationships between the features.\nCalculate the eigenvectors and eigenvalues: The eigenvectors and corresponding eigenvalues of the covariance matrix are computed. The eigenvectors represent the principal components, and the eigenvalues represent the amount of variance captured by each principal component.\nSelect the principal components: The eigenvectors with the highest eigenvalues are chosen as the principal components, as they capture the most variance in the data.\nProject the data onto the principal components: The original data is projected onto the selected principal components, resulting in a new dataset with fewer dimensions (principal components) while retaining most of the important information.\n\nPCA is useful in various applications, including:\n\nDimensionality reduction: PCA can be used to reduce the number of features in a dataset while retaining most of the important information, making the data more manageable and computationally efficient for further analysis or modeling.\nData visualization: By projecting high-dimensional data onto a lower-dimensional subspace (e.g., 2D or 3D), PCA can aid in visualizing and exploring complex datasets.\nNoise reduction: PCA can help remove noise or redundant information from the data by keeping only the principal components that capture the most significant patterns.\nFeature extraction: The principal components obtained from PCA can be used as new, uncorrelated features for further analysis or modeling.\n\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\nembedding_df\n\n\n  \n    \n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n\n0.356733\n-0.126862\n\n\n1\n[UNK]\n34.395100\n0.346069\n\n\n2\nsaid\n2.507210\n0.529500\n\n\n3\ntrump\n2.315338\n0.045322\n\n\n4\nus\n-12.205403\n0.066065\n\n\n...\n...\n...\n...\n\n\n1995\nquestioned\n0.706504\n-0.048583\n\n\n1996\npresence\n0.127328\n-0.245068\n\n\n1997\nmnuchin\n-1.282213\n-0.226332\n\n\n1998\nfootage\n0.789734\n-0.005336\n\n\n1999\noversight\n-0.154048\n-0.094105\n\n\n\n\n2000 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 3,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nalt text\n\n\n\npronounce_close = [\"2\", \"too\"]\nmeaning_close = [\"good\", \"great\"]\nmeaning_opposite = [\"yes\", \"no\"]\n\ndef gender_mapper(x):\n    if x in pronounce_close:\n        return 1\n    elif x in pronounce_close:\n        return 4\n    elif x in meaning_opposite:\n        return 5\n    else:\n        return 0\n\nembedding_df[\"highlight\"] = embedding_df[\"word\"].apply(gender_mapper)\nembedding_df[\"size\"]      = np.array(1.0 + 50*(embedding_df[\"highlight\"] &gt; 0))\n\n\nimport plotly.express as px\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 color = \"highlight\",\n                 size = list(embedding_df[\"size\"]),\n                 size_max = 10,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nalt text\n\n\n\n“2” and “too” can sometimes be used interchangeably in informal written communication, especially in contexts like text messaging or online forums where brevity and informality are common. I think that’s why they are close located.\nThe proximity of “good” and “great” in the word embedding space indicates that these two adjectives are likely to be synonymous or have similar meanings.\nThe fact that “yes” and “no” are located far apart in the embedding space suggests that these two words have opposite meanings. ”"
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "Penguin World",
    "section": "",
    "text": "Data Visualization\nIn this post, I’ll show you how to make a nice visualization of the famous “Palmer Penguins” dataset by using the Pandas, Matplotlib, and Seaborn packages in Python.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nLoad The Palmer Penguins Data Set\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nPandas is a powerful open-source library in Python used for data manipulation and analysis. It provides easy-to-use data structures and functions to work with structured data. Pandas is commonly used in data preprocessing tasks before feeding data into machine learning models.\n\n# Drop rows with missing values in specific columns\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\n\n# Extract the first word of the \"Species\" column\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\n# Filter out rows where \"Sex\" is not specified\npenguins = penguins[penguins[\"Sex\"] != \".\"]\n\n# Selecting specific columns\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]\n\n\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\nHave a general idea of the proportion of each species and the island they inhabit.\nMatplotlib is a widely used Python library for creating static, interactive, and animated visualizations in Python. It provides a MATLAB-like interface and can generate plots, histograms, power spectra, bar charts, error charts, scatterplots, etc.\n\nimport matplotlib.pyplot as plt\n\n# Count the occurrences of each species\nspecies = penguins.Species.value_counts()\n\n# Plotting the pie chart\nspecies.plot(kind='pie', autopct=\"%.2f%%\")\nplt.title('Percentage Distribution of Penguin Species')\n\n# Display the pie chart\nplt.show()\n\nText(0.5, 1.0, 'Percentage Distribution of Penguin Species')\n\n\n\n\n\n\n\n\n\nBased on the pie chart, the Palmer Penguins data set predominantly consists of Adelie penguins, accounting for 43.84%, followed by Gentoo penguins at 35.74%, and finally, Chinstrap penguins at 20.42%.\n\n# Count the occurrences of penguins on each island\nisland = penguins.Island.value_counts()\n\n# Plotting the pie chart\nisland.plot(kind='pie', autopct=\"%.2f%%\")\nplt.title('Percentage Distribution of Penguins on Each Island')\n\n# Display the pie chart\nplt.show()\n\nText(0.5, 1.0, 'Percentage Distribution of Penguin on Each Island')\n\n\n\n\n\n\n\n\n\nBased on the pie chart, the Palmer Penguins data set indicates that the majority of penguins inhabit Biscoe Island, comprising 48.95%, followed by Dream Island at 36.94%, and finally, Torgersen Island at 14.11%.\n\nimport seaborn as sns\n\n# Set style for seaborn\nsns.set(style=\"whitegrid\")\n\n# Create a figure with three subplots in the same line\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n\n# Plot 1: Island = Biscoe\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Biscoe'], ax=axes[0])\naxes[0].set_title(\"Biscoe Island\")\n\n# Plot 2: Island = Dream\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Dream'], ax=axes[1])\naxes[1].set_title(\"Dream Island\")\n\n# Plot 3: Island = Torgersen\nsns.countplot(x=\"Species\", data=penguins[penguins['Island'] == 'Torgersen'], ax=axes[2])\naxes[2].set_title(\"Torgersen Island\")\n\n# Set common y-label\naxes[0].set_ylabel(\"Count\")\n\nplt.suptitle(\"Distribution of Penguin Species on Different Islands\", y=1.05)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nBased on the bar plot illustrating the distribution of each species on each island, Gentoo penguins are the predominant species on Biscoe Island, Chinstrap penguins dominate Dream Island, and Adelie penguins are the major species on Torgersen Island. However, it’s worth noting that Adelie penguins inhabit all three islands, with Dream Island having the highest count of Adelie, even though Torgersen Island exclusively has Adelie penguins.\n\n\nDive deep into the Culmen Length and Culmen Depth relationships\nPlotly Express is a Python library that provides an easy-to-use interface for creating interactive visualizations. It is built on top of Plotly, a JavaScript library for creating interactive plots. Plotly Express offers a wide range of chart types and customization options.\n\npx.scatter(): This function creates a scatter plot using Plotly Express. We specify the DataFrame penguins and map the columns “Culmen Length (mm)” to the x-axis and “Culmen Depth (mm)” to the y-axis.\ncolor=\"Species\": This parameter colors the points based on the species of the penguins.\nhover_name=\"Species\": This parameter determines what appears when hovering over each point. Here, we display the species name.\nhover_data=[\"Island\", \"Sex\"]: This parameter adds additional information to be displayed when hovering over each point. We include the island and sex of the penguins.\nsize=\"Body Mass (g)\": This parameter sizes the points based on the body mass of the penguins.\nsize_max=8: This parameter sets the maximum size of the points.\nwidth=500, height=300: These parameters set the width and height of the plot.\nopacity=0.5: This parameter sets the opacity of the points to 0.5, making them slightly transparent.\n\n\nimport plotly\n\nfrom plotly import express as px\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 size = \"Body Mass (g)\",\n                 size_max = 8,\n                 width = 500,\n                 height = 300,\n                 opacity = 0.5\n                )\n\n#reduce whitespace\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\n# show the plot\nfig.show()\n\n\n\n\nWe now observe a general pattern where Adelie penguins tend to exhibit shorter culmen length but longer culmen depth compared to the other two species. Chinstrap penguins typically fall in the middle for both culmen length and culmen depth when compared to the other two species. On the other hand, Gentoo penguins tend to have greater culmen length and shorter culmen depth than the other two species. In summary, across all species, culmen length tends to be greater than culmen depth.\n\n\nNow, let’s explore the factors that may contribute to the variation in culmen depth and culmen length among different species.\nFirst let’s do some data preprocessing:\nThe line of code penguins_encoded = pd.get_dummies(penguins, columns=[\"Species\", \"Island\", \"Sex\"], drop_first=True) is used to convert categorical values into numerical values through a process called one-hot encoding.\nExplanation:\n\nCategorical Variables: Categorical variables are variables that can take on a limited, and usually fixed, number of possible values. In the DataFrame penguins, the columns “Species”, “Island”, and “Sex” are categorical variables because they represent categories rather than numerical quantities.\nOne-Hot Encoding: One-hot encoding is a technique used to convert categorical variables into a numerical format that can be provided as input to machine learning algorithms. In this encoding scheme, each category is represented as a binary vector, where each element of the vector corresponds to one category. The element representing the category of a particular observation is set to 1, while all other elements are set to 0.\npd.get_dummies() Function: The pd.get_dummies() function provided by the Pandas library is used to perform one-hot encoding. It creates a new DataFrame where each categorical variable specified in the columns parameter is converted into one-hot encoded columns.\ndrop_first=True: The drop_first parameter is set to True to avoid multicollinearity in the dataset. When drop_first=True, the first category for each variable is dropped after encoding, leaving only n-1 binary columns for n categories. This prevents redundancy in the data, as the dropped category can be inferred from the other binary columns. It also helps to avoid issues such as the “dummy variable trap” in regression analysis.\nOutput: After executing this line of code, penguins_encoded will contain the original columns from penguins, along with new binary columns representing the one-hot encoded categorical variables. Each binary column corresponds to a unique category within the original categorical variable. The dataset is now in a format suitable for many machine learning algorithms that require numerical input.\n\n\n# convert categorical values into numerical values\npenguins_encoded = pd.get_dummies(penguins, columns=[\"Species\", \"Island\", \"Sex\"], drop_first=True)\n\nSince we want to find out each variable’s relation with the Culmen Length and Culmen Depth, the following code will add 2 extra columns for Species_Adelie and Island_Biscoe since they would not be included in the data frame due to get_dummies would neglact these 2 columns and replace it with other columns being False and False.\n\npenguins_encoded['Species_Adelie'] = ~penguins_encoded['Species_Chinstrap'] & ~penguins_encoded['Species_Gentoo']\n\n\npenguins_encoded['Island_Biscoe'] =  ~penguins_encoded['Island_Dream'] & ~penguins_encoded['Island_Torgersen']\n\n\npenguins_encoded\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSpecies_Chinstrap\nSpecies_Gentoo\nIsland_Dream\nIsland_Torgersen\nSex_MALE\nSpecies_Adelie\nIsland_Biscoe\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n1\n39.5\n17.4\n186.0\n3800.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n2\n40.3\n18.0\n195.0\n3250.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n36.7\n19.3\n193.0\n3450.0\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n5\n39.3\n20.6\n190.0\n3650.0\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\n47.2\n13.7\n214.0\n4925.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n340\n46.8\n14.3\n215.0\n4850.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n341\n50.4\n15.7\n222.0\n5750.0\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n342\n45.2\n14.8\n212.0\n5200.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n343\n49.9\n16.1\n213.0\n5400.0\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n333 rows × 11 columns\n\n\n\n\n# Compute the correlation matrix\ncorrelation_matrix = penguins_encoded.corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n\n# Set plot title\nplt.title(\"Correlation Heatmap\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nBased on the plot, it appears that flipper length has a significant effect on culmen length, with a correlation of 0.65. Following this, body mass shows a correlation of 0.59. This aligns with the earlier visualization, indicating that Adelie tends to have a shorter length with a negative coefficient of -0.84. In contrast, Gentoo tends to have a longer culmen length with a positive coefficient of 0.49. The same trend applies to Chinstrap, with a positive coefficient similar to Gentoo.\nConcerning culmen depth, Adelie tends to have a longer culmen depth with the greatest positive coefficient of 0.53. Dream Island positively affects culmen depth with a coefficient of 0.46, which makes sense as Adelie and Chinstrap occupy the island. Gentoo, on the other hand, exhibits the greatest negative effect on culmen depth, consistent with the fact that Gentoo inhabits Biscoe Island."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jamie’s Blog",
    "section": "",
    "text": "Final Project\n\n\n\n\n\n\nFinal\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nJamie, Tyler, Zion\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Detection\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nDoggie & Catie Classification\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nAccelerate The Heat Diffusion!\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nMessage Hub\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nMovie or TV Show Recommendation\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Warming\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nJan 28, 2023\n\n\nJamie\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin World\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nJamie\n\n\n\n\n\n\nNo matching items"
  }
]